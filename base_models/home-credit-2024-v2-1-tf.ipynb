{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":50160,"databundleVersionId":7921029},{"sourceType":"datasetVersion","sourceId":8108358,"datasetId":4680819,"databundleVersionId":8226605}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CONTENT\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"markdown","source":"# **LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import os\nimport tempfile\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom glob import glob\nfrom pathlib import Path\nfrom datetime import datetime\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score \nfrom sklearn.metrics import roc_curve, auc\nimport sklearn\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom tqdm.notebook import tqdm\nimport joblib\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nmpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:51:56.614530Z","iopub.execute_input":"2024-04-13T15:51:56.614790Z","iopub.status.idle":"2024-04-13T15:52:14.479220Z","shell.execute_reply.started":"2024-04-13T15:51:56.614765Z","shell.execute_reply":"2024-04-13T15:52:14.478358Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n2024-04-13 15:52:05.004439: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-13 15:52:05.004571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-13 15:52:05.126040: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **CONFIGURATION**\n<a id='configuration'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"code","source":"# GENERAL CONFIGURATIONS\nNUM_THREADS = 4\nDATA_DIRECTORY = \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/\"\nSUBMISSION_SUFIX = \"_model_2.1_31\"\n#MODE CONFIGURATION\nSHOW_REPORT = False\nSELECTKBEST = False\nEXPORT_DATAFRAME = False\nIMPORT_DATAFRAME = False\n# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\nGENERATE_SUBMISSION_FILES = True\nEVALUATE_VALIDATION_SET = True\nSTRATIFIED_KFOLD = True\nBALANCE_COLUMNS = False\nRANDOM_SEED = 324\nNUM_FOLDS = 4\nEARLY_STOPPING = 100\nEPOCHS = 100\nBATCH_SIZE = 2048\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.480764Z","iopub.execute_input":"2024-04-13T15:52:14.481464Z","iopub.status.idle":"2024-04-13T15:52:14.487733Z","shell.execute_reply.started":"2024-04-13T15:52:14.481437Z","shell.execute_reply":"2024-04-13T15:52:14.486594Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Set aggregations","metadata":{}},{"cell_type":"code","source":"APPLPREV1_AGG = {\n\n    'num_group1':['count'],\n    'actualdpd_943P': ['mean','last'],\n    'annuity_853A': ['max','mean','last'],\n    'approvaldate_319D':['max','mean','last'],\n    'byoccupationinc_3656910L': ['max'],\n    'cancelreason_3545846M':['max','last'],\n    'childnum_21L': ['max'],\n    'creationdate_885D':['mean','last'],\n   # 'credacc_actualbalance_314A': ['min','max','mean','sum'],\n    'credacc_credlmt_575A': ['max','mean','last'],\n #   'credacc_maxhisbal_375A': ['min','max','mean','sum'],\n  #  'credacc_minhisbal_90A': ['min','max','mean','sum'],\n    'credacc_status_367L': ['max'],\n  #  'credacc_transactions_402L': ['min','max','mean','sum'],\n    'credamount_590A': ['max','mean','last'],\n    'credtype_587L': ['max','last'],\n    'currdebt_94A': ['max','mean','last'],\n    'dateactivated_425D':['max','mean','last'],\n    'district_544M':['max'],\n    'downpmt_134A': ['max','mean','last'],\n    'dtlastpmt_581D':['max','mean'],\n    'dtlastpmtallstes_3545839D':['max','mean','last'],\n    'education_1138M':['max','last'],\n    'employedfrom_700D':['max','last'],\n    'familystate_726L': ['max','last'],\n    'firstnonzeroinstldate_307D': ['max','mean','last'],\n    'inittransactioncode_279L': ['max','last'],\n    'isbidproduct_390L': ['max','last'],\n   # 'isdebitcard_527L': ['min','max','mean','sum'],\n    'mainoccupationinc_437A': ['max','mean','last'],\n    'maxdpdtolerance_577P': ['mean','last'],\n    'outstandingdebt_522A': ['max','mean','last'],\n    'pmtnum_8L': ['max','last'],\n    'postype_4733339M':['max','last'],\n    #'profession_152M':['max'],\n    'rejectreason_755M':['max','last'],\n    'rejectreasonclient_4145042M':['max','last'],\n   # 'revolvingaccount_394A': ['min','max','mean','sum'],\n    'status_219L': ['max','last'],\n   # 'tenor_203L': ['min','max','mean','sum'],\n    \n}\nAPPLPREV2_AGG = {\n    'num_group1':['count'],\n    'num_group2':['count'],\n    'conts_type_509L':['max','last'],\n    'cacccardblochreas_147M': ['max','last'],\n    'credacc_cards_status_52L':['max'],\n    \n}\nPERSON1_AGG={\n    'num_group1':['count'],\n    'birth_259D': ['max','last'],\n    #'childnum_185L':['max','mean','min'],\n    'contaddr_district_15M':['max'],\n    'contaddr_matchlist_1032L':['max','last'],\n    'contaddr_smempladdr_334L':['max','last'],\n    'contaddr_zipcode_807M':['max'],\n    'education_927M':['max','last'],\n    'empl_employedfrom_271D':['max','mean','min'],\n    'empl_employedtotal_800L':['max'],\n    'empl_industry_691L':['max'],\n    'empladdr_district_926M' : ['max','min','mean','last'],\n    'empladdr_zipcode_114M' : ['max','min','mean','last'],\n    'familystate_447L':['max','count'],\n    #'gender_992L'\n    'housetype_905L':['max'],\n    #'housingtype_772L'\n    'incometype_1044T':['max','last'],\n    #'isreference_387L'\n    'language1_981M':['max', 'last'],\n    'mainoccupationinc_384A':['max','mean','min', 'count','last'],\n    #'maritalst_703L'\n    'personindex_1023L':['max','mean','min', 'count','sum','last'],\n    'persontype_1072L':['max','mean','min', 'count','sum','last'],\n    'persontype_792L':['max','mean','min', 'count','sum'],\n    #'registaddr_district_1083M'\n    #'registaddr_zipcode_184M'\n    'relationshiptoclient_415T':['max','count'],\n    'relationshiptoclient_642T':['max','count','last'],\n    'remitter_829L':['max'],\n    'role_1084L':['max','count','last'],\n    #'role_993L'\n    'safeguarantyflag_411L':['max','last'],\n    'sex_738L':['max','last'],\n    'type_25L':['max','last']\n    \n\n    \n    \n    \n}\nPERSON2_AGG={\n    'num_group1':['count'],\n    'num_group2':['count'],\n    #'addres_district_368M'\n    #'addres_role_871L'\n    #'addres_zip_823M'\n    'conts_role_79M':['max','last'],\n    'empls_economicalst_849M':['max','last'],\n    #'empls_employedfrom_796D'\n    'empls_employer_name_740M':['max','last'],\n    #'relatedpersons_role_762T'\n}\nOTHER_AGG={\n    'num_group1':['count'],\n    'amtdebitincoming_4809443A':['max','mean','min', 'count','sum'],\n    'amtdebitoutgoing_4809440A':['max','mean','min', 'count','sum'],\n    #'amtdepositbalance_4809441A'\n    #'amtdepositincoming_4809444A'\n    #'amtdepositoutgoing_4809442A'\n}\nDEBITCARD_AGG={\n    'num_group1':['count'],\n    #'last180dayaveragebalance_704A'\n    #'last180dayturnover_1134A'\n    #'last30dayturnover_651A'\n    'openingdate_857D':['min','max','mean']\n}\nTAX_REGISTRY_A_AGG={\n    'num_group1':['count'],\n    'amount_4527230A': ['max','mean','min','sum'],\n    'name_4527232M':['max'],\n    'recorddate_4527225D':['max','mean','min']\n    \n}\nTAX_REGISTRY_B_AGG={\n    'num_group1':['count'],\n    'amount_4917619A':['min','mean','max','sum'],\n    'deductiondate_4917603D':['max','mean','min'],\n    'name_4917606M':['max'],\n    \n    \n}\nTAX_REGISTRY_C_AGG={\n    'num_group1':['count'],\n    'employername_160M':['max'],\n    'pmtamount_36A':['min','mean','max','sum','last'],\n    'processingdate_168D':['mean','min','max','last'],\n\n}\nCREDIT_BUREAU_A_1_AGG={\n    \n    'num_group1':['count'],\n    #'annualeffectiverate_199L'\n    #'annualeffectiverate_63L'\n    'classificationofcontr_13M':['max','last'],\n    'classificationofcontr_400M':['max','last'],\n    'contractst_545M':['max','last'],\n    'contractst_964M':['max','last'],\n    #'contractsum_5085717L'\n    'credlmt_230A':['mean'],\n    'credlmt_935A':['mean','min','max'],\n    'dateofcredend_289D':['mean'],\n    'dateofcredend_353D':['mean','max'],\n    'dateofcredstart_181D':['max'],\n    'dateofcredstart_739D':['mean'],\n    'dateofrealrepmt_138D':['mean','max'],\n    'debtoutstand_525A':['max'],\n    'debtoverdue_47A':['max'],\n    'description_351M':['max','last'],\n   \n    'dpdmax_757P':['mean'],\n    'dpdmaxdatemonth_442T':['max'],\n    'dpdmaxdatemonth_89T':['max'],\n    'dpdmaxdateyear_596T':['max'],\n    'dpdmaxdateyear_896T':['max'],\n    'financialinstitution_382M':['max','last'],\n    'financialinstitution_591M':['max','last'],\n    'instlamount_768A':['mean'],\n    #'instlamount_852A'\n    #'interestrate_508L'\n    'lastupdate_1112D':['mean','max'],\n    'lastupdate_388D':['mean','max'],\n    'monthlyinstlamount_332A':['mean'],\n    'monthlyinstlamount_674A':['mean','max'],\n    'nominalrate_281L':['max'],\n    'nominalrate_498L':['max'],\n    'numberofcontrsvalue_258L':['max'],\n    'numberofcontrsvalue_358L':['max'],\n    'numberofinstls_229L':['max'],\n    'numberofinstls_320L':['max'],\n    'numberofoverdueinstls_834L':['max'],\n    'numberofoutstandinstls_520L':['max'],\n    'numberofoutstandinstls_59L':['max'],\n    'numberofoverdueinstlmax_1039L':['max'],\n    'numberofoverdueinstlmax_1151L' : ['max'],\n    'numberofoverdueinstlmaxdat_148D': ['max'],\n    'numberofoverdueinstlmaxdat_641D':['mean'],\n    'numberofoverdueinstls_725L':['max'],\n    'outstandingamount_354A': ['mean'],\n    'outstandingamount_362A': ['mean'],\n    'overdueamountmaxdatemonth_284T': ['max'],\n    'overdueamountmaxdatemonth_365T': ['max'],\n    'overdueamountmaxdateyear_2T': ['max'],\n    'overdueamountmaxdateyear_994T': ['max'],\n    'overdueamount_31A': [ 'mean'],\n    'overdueamountmax_35A' : [ 'mean'],\n    'overdueamountmax2_398A': [ 'mean'],\n    'overdueamountmax2date_1002D'  : [ 'mean'],\n    'overdueamountmax2date_1142D': ['max'],\n    'overdueamount_659A': [ 'mean'],\n    'overdueamountmax2_14A' :['mean'],\n    'periodicityofpmts_1102L': ['max'],\n    'periodicityofpmts_837L': ['max'],\n    'prolongationcount_1120L': [ 'max'],\n    'prolongationcount_599L': [ 'max'],\n    'purposeofcred_426M': ['max','last'],\n    'purposeofcred_874M': ['max','last'],\n    'refreshdate_3813885D': ['mean', 'max','last'],\n    'residualamount_488A': [ 'max'],\n    'residualamount_856A': ['mean'],\n    'subjectrole_182M' : [ 'max', 'last'],\n    'subjectrole_93M' : [ 'max', 'last'],\n    'totalamount_6A': ['mean', 'max'],\n    'totalamount_996A': [ 'mean'],\n    \n    'totaldebtoverduevalue_718A': [ 'mean'],\n    'totaloutstanddebtvalue_39A': ['mean'],\n    'totaloutstanddebtvalue_668A': [ 'mean']\n   \n}\nCREDIT_BUREAU_B_1_AGG={\n    'num_group1':['count'],\n    \n}\nCREDIT_BUREAU_A_2_AGG={\n \n    'collater_typofvalofguarant_298M' : [ 'max', 'last'],\n    'collater_typofvalofguarant_407M': [ 'max', 'last'],\n    'collaterals_typeofguarante_359M': ['max', 'last'],\n    'collaterals_typeofguarante_669M': ['max', 'last'],\n    'collater_valueofguarantee_1124L': ['max'],\n    'collater_valueofguarantee_876L': ['max'],\n    'pmts_dpd_1073P': ['mean'],\n    'pmts_dpd_303P': ['mean', 'max'],\n    'pmts_month_158T': [ 'max','last'],\n    'pmts_month_706T': ['max','last'],\n    'pmts_overdue_1140A': ['mean'],\n    'pmts_overdue_1152A': [ 'mean'],\n    'pmts_year_1139T': [ 'max', 'last'],\n    'pmts_year_507T': ['max','last'],\n    'subjectroles_name_541M': [ 'max','last'],\n    'subjectroles_name_838M': ['min', 'mean', 'max','count','last'],\n    \n    \n    'num_group1':['count'],\n    'num_group2':['count']\n}\nCREDIT_BUREAU_B_2_AGG={\n    'num_group1':['count'],\n    'num_group2':['count'],\n    'pmts_date_1107D':['min', 'mean', 'max'],\n    'pmts_dpdvalue_108P':['min','mean','max'],\n    'pmts_pmtsoverdue_635A':['min','mean','max'],\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.489343Z","iopub.execute_input":"2024-04-13T15:52:14.489724Z","iopub.status.idle":"2024-04-13T15:52:14.526980Z","shell.execute_reply.started":"2024-04-13T15:52:14.489693Z","shell.execute_reply":"2024-04-13T15:52:14.526222Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **MAIN FUNCTION**\n<a id='main_function'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"code","source":"def main(debug= False):\n    num_rows = 11111 if debug else None\n    print(\"Notebook started:\")\n    df=pd.DataFrame()\n    if not IMPORT_DATAFRAME:\n        df=pl.DataFrame()\n        with timer(\"base\"):\n\n            df = get_base(DATA_DIRECTORY, num_rows=num_rows)\n\n            print(\"base dataframe shape:\", df.shape)\n\n\n\n        with timer(\"static\"):\n\n            df_static = get_static(DATA_DIRECTORY, num_rows=num_rows)\n            df_static = df_static.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_static, on='case_id', how='left', suffix='_static')\n            print(\"static dataframe shape:\", df_static.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n\n            del df_static\n            gc.collect()\n        \n        with timer(\"static_cb\"):\n\n            df_static_cb = get_static_cb(DATA_DIRECTORY, num_rows=num_rows)\n            df_static_cb = df_static_cb.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_static_cb, on='case_id', how='left', suffix='_static_cb')\n            print(\"static cb dataframe shape:\", df_static_cb.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_static_cb\n            gc.collect()\n\n        with timer(\"Previous applications depth 1 test\"):\n\n            df_applprev1 = get_applprev1(DATA_DIRECTORY, num_rows=num_rows)\n            df_applprev1 = df_applprev1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n            print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_applprev1\n            gc.collect()\n\n        with timer(\"Previous applications depth 2 test\"):\n\n            df_applprev2 = get_applprev2(DATA_DIRECTORY, num_rows=num_rows)\n            df_applprev2 = df_applprev2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_applprev2, on='case_id', how='left', suffix='_applprev2')\n            print(\"Previous applications depth 2 test dataframe shape:\", df_applprev2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_applprev2\n            gc.collect()\n\n        with timer(\"Person depth 1 test\"):\n\n            df_person1 = get_person1(DATA_DIRECTORY, num_rows=num_rows)\n            df_person1 = df_person1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_person1, on='case_id', how='left', suffix='_person1')\n            print(\"Person depth 1 test dataframe shape:\", df_person1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_person1\n            gc.collect()\n\n        with timer(\"Person depth 2 test\"):\n\n            df_person2 = get_person2(DATA_DIRECTORY, num_rows=num_rows)\n            df_person2 = df_person2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_person2, on='case_id', how='left', suffix='_person2')\n            print(\"Person depth 2 test dataframe shape:\", df_person2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_person2\n            gc.collect()\n\n        with timer(\"Other test\"):\n\n            df_other = get_other(DATA_DIRECTORY, num_rows=num_rows)\n            df_other = df_other.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_other, on='case_id', how='left', suffix='_other')\n            print(\"Other test dataframe shape:\", df_other.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_other\n            gc.collect()\n\n        with timer(\"Debit card test\"):\n\n            df_debitcard = get_debitcard(DATA_DIRECTORY, num_rows=num_rows)\n            df_debitcard = df_debitcard.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_debitcard, on='case_id', how='left', suffix='_debitcard')\n            print(\"Debit card test dataframe shape:\", df_debitcard.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_debitcard\n            gc.collect()\n\n        with timer(\"Tax registry a test\"):\n\n            df_tax_registry_a = get_tax_registry_a(DATA_DIRECTORY, num_rows=num_rows)\n            df_tax_registry_a = df_tax_registry_a.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_tax_registry_a, on='case_id', how='left', suffix='_tax_registry_a')\n            print(\"Tax registry a test dataframe shape:\", df_tax_registry_a.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_tax_registry_a\n            gc.collect()\n\n        with timer(\"Tax registry b test\"):\n\n            df_tax_registry_b = get_tax_registry_b(DATA_DIRECTORY, num_rows=num_rows)\n            df_tax_registry_b = df_tax_registry_b.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_tax_registry_b, on='case_id', how='left', suffix='_tax_registry_b')\n            print(\"Tax registry b test dataframe shape:\", df_tax_registry_b.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_tax_registry_b\n            gc.collect()\n\n        with timer(\"Tax registry c test\"):\n\n            df_tax_registry_c = get_tax_registry_c(DATA_DIRECTORY, num_rows=num_rows)\n            df_tax_registry_c = df_tax_registry_c.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_tax_registry_c, on='case_id', how='left', suffix='_tax_registry_c')\n            print(\"Tax registry c test dataframe shape:\", df_tax_registry_c.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_tax_registry_c\n            gc.collect()\n\n\n        with timer(\"Credit bureau a 1 test\"):\n\n            df_credit_bureau_a_1 = get_credit_bureau_a_1(DATA_DIRECTORY, num_rows=num_rows)\n          \n     \n            df_credit_bureau_a_1 = df_credit_bureau_a_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n         \n            df = df.join(df_credit_bureau_a_1, on='case_id', how='left', suffix='_cb_a_1')\n        \n            print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n       \n            del df_credit_bureau_a_1\n            gc.collect()\n       \n        with timer(\"Credit bureau b 1 test\"):\n\n            df_credit_bureau_b_1 = get_credit_bureau_b_1(DATA_DIRECTORY, num_rows=num_rows)\n            df_credit_bureau_b_1 = df_credit_bureau_b_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_credit_bureau_b_1, on='case_id', how='left', suffix='_cb_b_1')\n            print(\"Credit bureau b 1 test dataframe shape:\", df_credit_bureau_b_1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_credit_bureau_b_1\n            gc.collect()\n\n\n\n\n        with timer(\"Credit bureau a 2 test\"):\n\n            df_credit_bureau_a_2 = get_credit_bureau_a_2(DATA_DIRECTORY, num_rows=num_rows)\n            df_credit_bureau_a_2 = df_credit_bureau_a_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_credit_bureau_a_2, on='case_id', how='left', suffix='_cb_a_2')\n            print(\"Credit bureau a 2 test dataframe shape:\", df_credit_bureau_a_2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_credit_bureau_a_2\n            gc.collect()\n\n        with timer(\"Credit bureau b 2 test\"):\n\n            df_credit_bureau_b_2 = get_credit_bureau_b_2(DATA_DIRECTORY, num_rows=num_rows)\n            df_credit_bureau_b_2 = df_credit_bureau_b_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_credit_bureau_b_2, on='case_id', how='left', suffix='_cb_b_2')\n            print(\"Credit bureau b 2 test dataframe shape:\", df_credit_bureau_b_2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_credit_bureau_b_2\n            gc.collect()\n\n        with timer(\"Feature engineering / preprocessing\"): \n\n            df=feature_engineering(df)\n            get_info(df)\n            df, cat_cols = to_pandas(df)\n            \n            df=reduce_mem_usage(df)\n            print(\"DATAFRAME shape:\", df.shape)\n    \n    #else:\n        #with timer(\"Importing processed dataframe\"):\n            \n            \n            #df = pd.read_parquet(\"/kaggle/input/home-credit-2024-additional-dataset/processed_003_std.parquet\")\n            \n         \n            #for col in df.select_dtypes(exclude=['number']).columns:\n             #   df[col] = df[col].astype('category')\n            \n            #print(df.dtypes.value_counts())\n            #df=reduce_mem_usage(df)\n           \n            #print(\"DATAFRAME shape:\", df.shape)\n    \n    \n    with timer(\"Data preprocessing for Tensorflow\"): \n        \n        df = preprocessingX(df)\n        \n        for col in df.select_dtypes(include=['float16']).columns:\n            df[col] = df[col].astype(float)\n        \n        \n           \n        print(\"shape after, \", df.shape)\n    #with timer(\"Data transforming for Tensorflow\"): \n     #   df=data_transformation(df)\n        \n    with timer(\"PCA\"):\n        df.to_parquet(\"transformed_data.parquet\")\n        del df; gc.collect()\n        if num_rows is None:\n            df=principal_component_analysis(pd.read_parquet(\"/kaggle/working/transformed_data.parquet\"),300,'pandas')\n        else:\n            df=principal_component_analysis(pd.read_parquet(\"/kaggle/working/transformed_data.parquet\"),10,'pandas')\n        df.to_parquet(\"transformed_data.parquet\")\n        del df; gc.collect()\n    \n    if EXPORT_DATAFRAME:\n        with timer(\"Export dataframe\"):\n            df.to_parquet(\"/kaggle/working/processed.parquet\", index=False)\n            \n            \n            print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n            return\n    \n    if(SELECTKBEST):\n        with timer(\"SelectKBest feature research\"):\n            \n            selectkbestX(df)\n            print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n            return\n        \n    \n\n    with timer(\"Model training\"):\n       \n        \n        #del_features = ['target', 'case_id','WEEK_NUM']\n        #predictors = list(filter(lambda v: v not in del_features, df.columns))\n        #cat_cols = list(df.select_dtypes(\"object\").columns)\n        model = kfold_tensorflow()\n       \n        \n\n    \n    \n    #with timer(\"Feature importance assesment\"):\n        \n     #   get_features_importances(predictors, model)\n        \n        \n    \n        \n    with timer(\"Submission\"):\n\n        if  GENERATE_SUBMISSION_FILES:\n            \n            if generate_submission_file(model):\n\n\n                print(\"Submission file has been created.\")\n            \n    \n    print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n    \n    return 1\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.529383Z","iopub.execute_input":"2024-04-13T15:52:14.529686Z","iopub.status.idle":"2024-04-13T15:52:14.570631Z","shell.execute_reply.started":"2024-04-13T15:52:14.529658Z","shell.execute_reply":"2024-04-13T15:52:14.569677Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY FUNCTIONS**","metadata":{}},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"class Pipeline:\n    @staticmethod\n    \n    \n    # Sets datatypes accordingly\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))            \n\n        return df\n    \n    \n    # Changes the values of all date columns. The result will not be a date but number of days since date_decision.\n    @staticmethod\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                \n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n                df = df.with_columns(pl.col(col).dt.total_days())\n                \n        df = df.with_columns((pl.col(\"date_decision\").dt.month()).alias(\"month_decision\"))\n        df = df.with_columns((pl.col(\"date_decision\").dt.weekday()).alias(\"weekday_decision\"))\n                \n        df = df.drop(\"date_decision\", \"MONTH\")\n\n        return df\n    \n    # It drops columns with a lot of NaN values.\n    @staticmethod\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n\n                if isnull > 0.95:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.571774Z","iopub.execute_input":"2024-04-13T15:52:14.572125Z","iopub.status.idle":"2024-04-13T15:52:14.585810Z","shell.execute_reply.started":"2024-04-13T15:52:14.572092Z","shell.execute_reply":"2024-04-13T15:52:14.584848Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_info(dataframe):\n    \"\"\"\n    View data types, shape, and calculate the percentage of NaN (missing) values in each column\n    of a Polars DataFrame simultaneously.\n    \n    Parameters:\n    dataframe (polars.DataFrame): The DataFrame to analyze.\n    \n    Returns:\n    None\n    \"\"\"\n    # Print DataFrame shape\n    print(\"DataFrame Shape:\", dataframe.shape)\n    print(\"-\" * 60)\n    \n    # Print column information\n    print(\"{:<50} {:<30} {:<20}\".format(\"Column Name\", \"Data Type\", \"NaN Percentage\"))\n    print(\"-\" * 60)\n    \n    # Total number of rows in the DataFrame\n    total_rows = len(dataframe)\n    \n    # Iterate over each column\n    for column in dataframe.columns:\n        # Get the data type of the column\n        dtype = str(dataframe[column].dtype)\n        \n        # Count the number of NaN values in the column\n        nan_count = dataframe[column].null_count()\n        \n        # Calculate the percentage of NaN values\n        nan_percentage = (nan_count / total_rows) * 100\n        \n        # Print the information\n        print(\"{:<50} {:<30} {:.2f}%\".format(column, dtype, nan_percentage))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.587188Z","iopub.execute_input":"2024-04-13T15:52:14.587953Z","iopub.status.idle":"2024-04-13T15:52:14.597644Z","shell.execute_reply.started":"2024-04-13T15:52:14.587928Z","shell.execute_reply":"2024-04-13T15:52:14.596871Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def principal_component_analysis(data, num_output_features, framework='pandas'):\n    print(\"shape before PCA \", data.shape)\n    if framework == 'pandas':\n        del_features = ['target', 'case_id', 'WEEK_NUM']\n        predictors = [col for col in data.columns if col not in del_features]\n        df1 = data[del_features]\n        data.drop(columns=del_features, inplace=True)\n        \n        # Convert DataFrame to numpy array\n        #data_np = data.to_numpy()\n        \n        # Step 1: Standardize the data\n        # Step 2: Apply PCA\n        pca = IncrementalPCA(n_components=num_output_features)\n        \n        chunk_size = len(data) // 30\n        num_chunks = len(data) // chunk_size \n        \n        # Iterate through chunks\n        with tqdm(total=num_chunks, desc=\"Fitting PCA\") as pbar_fit:\n            for chunk_idx in range(num_chunks):\n                start_idx = chunk_idx * chunk_size\n                end_idx = min((chunk_idx + 1) * chunk_size, len(data))\n                if (end_idx+num_output_features) > (len(data)):\n                    end_idx=len(data)\n                chunk = data[start_idx:end_idx].to_numpy()\n                #print(\"chunk number \", start_idx, \" \", end_idx, \" values \", chunk[0], \" \", chunk[-1])\n                pca.partial_fit(chunk)\n                del chunk;gc.collect()\n                pbar_fit.update(1)\n        \n        # Transform data in chunks\n        X_transformed = None\n        with tqdm(total=num_chunks, desc=\"Transforming data\") as pbar_transform:\n            for chunk_idx in range(num_chunks):\n                start_idx = 0\n                end_idx = chunk_size\n                if (end_idx+num_output_features) > (len(data)):\n                    end_idx=len(data)\n                chunk = data[start_idx:end_idx].to_numpy()\n                #print(\"chunk number \", start_idx, \" \", end_idx, \" values \", chunk[0], \" \", chunk[-1])\n                chunk_transformed = pca.transform(chunk)\n                if X_transformed is None:\n                    X_transformed = chunk_transformed\n                else:\n                    X_transformed = np.vstack((X_transformed, chunk_transformed))\n                del chunk;gc.collect()\n                data.drop(data.index[0:chunk_size], inplace=True)\n                pbar_transform.update(1)\n        \n        del data; gc.collect()\n        # Convert transformed data back to DataFrame\n        data = pd.DataFrame(data=X_transformed, columns=[f\"PC{i}\" for i in range(1, num_output_features + 1)])\n        print(\"check shape \", data.shape)\n        \n        # Concatenate with other features\n        data = pd.concat([data, df1], axis=1)\n        print(\"shape after PCA \", data.shape)\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.598796Z","iopub.execute_input":"2024-04-13T15:52:14.599089Z","iopub.status.idle":"2024-04-13T15:52:14.614002Z","shell.execute_reply.started":"2024-04-13T15:52:14.599066Z","shell.execute_reply":"2024-04-13T15:52:14.613194Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    \n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    \n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    \n    return df_data, cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.615032Z","iopub.execute_input":"2024-04-13T15:52:14.615281Z","iopub.status.idle":"2024-04-13T15:52:14.625806Z","shell.execute_reply.started":"2024-04-13T15:52:14.615259Z","shell.execute_reply":"2024-04-13T15:52:14.624995Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.626776Z","iopub.execute_input":"2024-04-13T15:52:14.627067Z","iopub.status.idle":"2024-04-13T15:52:14.639693Z","shell.execute_reply.started":"2024-04-13T15:52:14.627040Z","shell.execute_reply":"2024-04-13T15:52:14.638815Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.643344Z","iopub.execute_input":"2024-04-13T15:52:14.643636Z","iopub.status.idle":"2024-04-13T15:52:14.648716Z","shell.execute_reply.started":"2024-04-13T15:52:14.643613Z","shell.execute_reply":"2024-04-13T15:52:14.648013Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    \n\n    temp=base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\").mean()\n   \n    week_nums_to_drop = temp[(temp[\"target\"] == 0) | (temp[\"target\"] == 1)].index.tolist()\n\n    base_filtered = base[~base[\"WEEK_NUM\"].isin(week_nums_to_drop)]\n\n    # Apply the aggregator\n    gini_in_time = base_filtered.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]] \\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n\n    \n\n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a * x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.nanmean(gini_in_time)  # Use np.nanmean to handle NaN values\n    \n    if SHOW_REPORT:\n        # Display the plot of x on y\n        plt.figure(figsize=(8, 6))\n        plt.plot(x, y, 'o', label='Gini in Time')\n        plt.plot(x, y_hat, '-', label='Fitted line (slope={:.2f}, intercept={:.2f})'.format(a, b))\n        plt.xlabel('Week')\n        plt.ylabel('Gini in Time')\n        plt.title('Gini Stability Over Time')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    \n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.649777Z","iopub.execute_input":"2024-04-13T15:52:14.650074Z","iopub.status.idle":"2024-04-13T15:52:14.661412Z","shell.execute_reply.started":"2024-04-13T15:52:14.650051Z","shell.execute_reply":"2024-04-13T15:52:14.660633Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Report function","metadata":{}},{"cell_type":"code","source":"def group(df_to_agg, prefix, aggregations, aggregate_by='case_id', datatype='polars'):\n    # Create a dictionary mapping aggregation functions to their string representations\n    \n    if datatype=='polars':\n        func_mapping = {\n        'min': pl.min,\n        'max': pl.max,\n        'mean': pl.mean,\n        'sum': pl.sum,\n        'count': pl.count,\n         'median': pl.median,\n              \"last\" : pl.last\n        }\n\n    # Perform the aggregation\n        agg_df = df_to_agg.group_by(aggregate_by).agg(**{\n            f\"{func}_{col}\": func_mapping[func](col) for col, funcs in aggregations.items() for func in funcs\n        })\n        '''\n        # Rename columns\n        for col, funcs in aggregations.items():\n            for func in funcs:\n                old_name = f\"{col}_{func}\"\n                new_name = f\"{prefix}{col}_{func.upper()}\"\n                agg_df = agg_df.select(pl.col(old_name).alias(new_name))\n        '''\n        return agg_df\n    \n    if datatype=='pandas':\n            # Create a dictionary mapping aggregation functions to their string representations\n        func_mapping = {\n            'min': 'min',\n            'max': 'max',\n            'mean': 'mean',\n            'sum': 'sum',\n            'count': 'count',\n            \n        }\n\n        # Perform the aggregation\n        agg_df = df_to_agg.groupby(aggregate_by).agg(**{\n            f\"{prefix}{col}_{func.upper()}\": (col, func_mapping[func]) for col, funcs in aggregations.items() for func in funcs\n        }).reset_index()\n        \n        return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.662570Z","iopub.execute_input":"2024-04-13T15:52:14.663415Z","iopub.status.idle":"2024-04-13T15:52:14.674129Z","shell.execute_reply.started":"2024-04-13T15:52:14.663384Z","shell.execute_reply":"2024-04-13T15:52:14.673265Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# **SELECTKBEST METHOD**","metadata":{}},{"cell_type":"code","source":"def preprocessingX(data):\n    \n        \n\n        def one_hot_encode(data):\n            \n            \n            original_columns = list(data.columns)\n            categories = [cat for cat in data.columns if data[cat].dtype == 'category']\n            data = pd.get_dummies(data, columns= categories, dummy_na= True) #one_hot_encode the categorical features\n            categorical_columns = [cat for cat in data.columns if cat not in original_columns]\n            return data, categorical_columns\n        \n        \n        data,categorical_columns=one_hot_encode(data)\n        \n        \n        total_nan_count = data.isna().sum().sum()\n\n        print(\"Total count of NaN values in the DataFrame:\", total_nan_count)\n        for column in (set(data.columns)-{'target'}):\n            # Calculate the mean value of the column excluding NaNs\n           \n            \n            \n            data[column]=data[column].fillna(0)\n\n        total_nan_count = data.isna().sum().sum()\n\n        print(\"Total count of NaN values in the DataFrame:\", total_nan_count)\n\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.675046Z","iopub.execute_input":"2024-04-13T15:52:14.675303Z","iopub.status.idle":"2024-04-13T15:52:14.688162Z","shell.execute_reply.started":"2024-04-13T15:52:14.675266Z","shell.execute_reply":"2024-04-13T15:52:14.687282Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def selectkbestX(data):\n    #########################################################################################\n    \n    #########################################################################################\n    def selectkbest_base(X_train, y_train):\n        \n        # Define SelectKBest with desired parameters\n        k = 500  # Number of top features to select\n        S = SelectKBest(score_func=f_classif, k=k)\n\n        # Fit SelectKBest on training data and transform features\n        X_train_k_best = S.fit_transform(X_train, y_train)\n\n        # Get scores assigned to each feature\n        feature_scores = S.scores_\n        \n        # Create a DataFrame to store feature names and their scores\n        feature_scores_df = pd.DataFrame({'Feature': X_train.columns, 'Score': feature_scores})\n\n        # Sort DataFrame by scores in descending order\n        #feature_scores_df_sorted = feature_scores_df.sort_values(by='Score', ascending=False)\n\n        # Print the table of top features and their scores\n      \n        # Return DataFrame with feature names and their scores\n        return feature_scores_df\n    #########################################################################################\n    \n    \n    df=preprocessingX(data)\n    del data;gc.collect()\n    \n    \n    \n    \n   \n    N_CHUNKS=5\n    df.drop(df[df['target'].isnull()].index, inplace=True)\n    \n   \n    del_features = ['target', 'case_id']\n    predictors = [col for col in df.columns if col not in del_features]\n    \n    feats_df = pd.DataFrame({'feature': predictors}, columns=['feature'])\n    \n    results=[]\n    \n    with tqdm(total=N_CHUNKS) as pbar:\n        for i in range(N_CHUNKS):\n\n            sub_df = df[df.index % N_CHUNKS == i]\n            df.drop(df.index[df.index % N_CHUNKS == i], inplace=True)\n            X_train=sub_df[predictors]\n            y_train=sub_df['target']\n\n\n            result_df=selectkbest_base(X_train, y_train)\n            \n            del sub_df\n            gc.collect()\n\n            results.append(result_df)\n            pbar.update(1)\n            \n    del df; gc.collect()\n    merged_df = results[0]\n\n# Merge the remaining dataframes horizontally on the 'Feature' column\n    for df_index in range(1, len(results)):\n        suffix = '_' + str(df_index)  # Add a suffix to distinguish overlapping column names\n        merged_df = pd.merge(merged_df, results[df_index], on='Feature', suffixes=('', suffix))\n\n    merged_df.rename(columns={'Score': 'Score_0'}, inplace=True)\n    merged_df['mean_score'] = 0\n    \n    for i in range(N_CHUNKS):\n        merged_df['mean_score']+=merged_df[\"Score_\"+str(i)]\n    \n    \n    final_df=merged_df[['Feature', 'mean_score']]\n    final_df = final_df.sort_values(by='mean_score', ascending=False)\n    pd.set_option('display.max_rows', None)  # Show all rows\n# Display the DataFrame\n    print(final_df)\n   \n\n    final_df.to_csv(\"/kaggle/working/SelectKBest.csv\")\n    \n    return merged_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.689643Z","iopub.execute_input":"2024-04-13T15:52:14.689997Z","iopub.status.idle":"2024-04-13T15:52:14.704519Z","shell.execute_reply.started":"2024-04-13T15:52:14.689931Z","shell.execute_reply":"2024-04-13T15:52:14.703684Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#  **MODEL** <a id='model'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        y_preds = []\n        # Use tqdm to create a progress bar during the prediction\n        with tqdm(total=len(self.estimators), desc=\"Predicting\", unit=\" models\") as pbar:\n            for i, estimator in enumerate(self.estimators):\n                \n                \n                \n                \n                X = np.asarray(X).astype('float32')\n                \n         \n                # Obtain probabilities for each class\n                probabilities = estimator.predict(X)\n                # For binary classification, we are interested in the probability of the positive class\n                y_preds.append(probabilities.flatten())\n                \n                \n                \n                \n                \n            \n                pbar.update(1)  # Update the progress bar\n        return np.mean(y_preds, axis=0)\n\n    \n    def get_splits(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for x in self.estimators:\n            feature_importances_list.append(x.feature_importance(importance_type='split'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None   \n        return combined_importances\n    \n    \n    def get_gains(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for model in self.estimators:\n            feature_importances_list.append(x.feature_importance(importance_type='gain'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None\n              \n        return combined_importances\n    \n    def get_features_importances_df(self, predictors):\n        \n        \n        importance_df = pd.DataFrame()\n        eval_results = dict()\n        for model in self.estimators:\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = predictors\n            fold_importance[\"gain\"] = model.feature_importance(importance_type='gain')\n            fold_importance[\"split\"] = model.feature_importance(importance_type='split')\n            importance_df = pd.concat([importance_df, fold_importance], axis=0)\n            importance_df= importance_df.groupby('feature').mean().reset_index()\n        return importance_df\n    \n    \n    def add_predictions(self, predictions):\n        self.predictions=predictions\n        \n    def get_predictions(self):\n        return self.predictions\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.705520Z","iopub.execute_input":"2024-04-13T15:52:14.705805Z","iopub.status.idle":"2024-04-13T15:52:14.722045Z","shell.execute_reply.started":"2024-04-13T15:52:14.705782Z","shell.execute_reply":"2024-04-13T15:52:14.721309Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def kfold_tensorflow(data=None, categorical_feature = None):\n    \n    \n    #data = pd.read_parquet(\"/kaggle/input/home-credit-2024-additional-dataset/processed_004_PCA300.parquet\")\n    data = pd.read_parquet(\"/kaggle/working/transformed_data.parquet\")        \n         \n    for col in data.select_dtypes(exclude=['number']).columns:\n        data[col] = data[col].astype('category')\n\n    print(data.dtypes.value_counts())\n    #df=reduce_mem_usage(df)\n\n    categories = [cat for cat in data.columns if data[cat].dtype == 'category']\n    categories=set(categories) - set(categories[::3])\n    data.drop(columns=list(categories), inplace=True)\n    \n    print(\"DATAFRAME shape:\", data.shape)\n    print(data.dtypes.value_counts())\n    #time.sleep(30)\n    start_time = time.time()\n    \n    \n    \n    numerical_columns = data.select_dtypes(include=['number']).columns\n\n    for column in numerical_columns:\n        min_value = data[column].min()\n        max_value = data[column].max()\n        print(f\"Column: {column}, Min Value: {min_value}, Max Value: {max_value}\")\n        \n    \n    \n    data.drop(data[data['target'].isnull()].index, inplace=True)\n \n    \n    \n  \n\n \n    \n   \n    \n  \n   \n    print(\"Train/valid shape: {}, \".format(data.shape))\n    del_features = ['target', 'case_id', 'WEEK_NUM']\n    predictors = list(filter(lambda v: v not in del_features, data.columns))\n\n    if not STRATIFIED_KFOLD:\n        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    else:\n        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    \n        # Hold oof predictions, test predictions, feature importance and training/valid auc\n    oof_preds = np.zeros(data.shape[0])\n    \n    importance_df = pd.DataFrame()\n    eval_results = dict()\n    \n    fitted_models = []\n    \n    METRICS = [\n      keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n      keras.metrics.MeanSquaredError(name='Brier score'),\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\n    def make_model(metrics=METRICS, output_bias=None):\n        if output_bias is not None:\n            output_bias = tf.keras.initializers.Constant(output_bias)\n        model = keras.Sequential([\n          keras.layers.Dense(\n              16, activation='relu',\n              input_shape=(len(predictors),)),\n          keras.layers.Dropout(0.5),\n          keras.layers.Dense(1, activation='sigmoid',\n                             bias_initializer=output_bias),\n        ])\n\n        model.compile(\n          optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n          loss=keras.losses.BinaryCrossentropy(),\n          metrics=metrics)\n\n        return model\n    \n    \n    def plot_metrics(history):\n                metrics = ['loss', 'prc', 'precision', 'recall']\n                for n, metric in enumerate(metrics):\n                    name = metric.replace(\"_\",\" \").capitalize()\n                    plt.subplot(2,2,n+1)\n                    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n                    plt.plot(history.epoch, history.history['val_'+metric],\n                             color=colors[0], linestyle=\"--\", label='Val')\n                    plt.xlabel('Epoch')\n                    plt.ylabel(name)\n                if metric == 'loss':\n                    plt.ylim([0, plt.ylim()[1]])\n                elif metric == 'auc':\n                    plt.ylim([0.8,1])\n                else:\n                    plt.ylim([0,1])\n\n                plt.legend()\n                plt.show()\n    \n    \n    def plot_loss(history, label, n):\n                  # Use a log scale on y-axis to show the wide range of values.\n                plt.semilogy(history.epoch, history.history['loss'],\n                               color=colors[n], label='Train ' + label)\n                plt.semilogy(history.epoch, history.history['val_loss'],\n                               color=colors[n], label='Val ' + label,\n                               linestyle=\"--\")\n                plt.xlabel('Epoch')\n                plt.ylabel('Loss')\n                plt.legend()\n                plt.show()\n    \n    \n    def plot_cm(labels, predictions, threshold=0.5):\n                cm = confusion_matrix(labels, predictions > threshold)\n                plt.figure(figsize=(5,5))\n                sns.heatmap(cm, annot=True, fmt=\"d\")\n                plt.title('Confusion matrix @{:.2f}'.format(threshold))\n                plt.ylabel('Actual label')\n                plt.xlabel('Predicted label')\n                plt.show()\n\n                print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n                print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n                print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n                print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n                print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n                \n    def plot_roc(name, labels, predictions, **kwargs):\n                fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n\n                plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n                plt.xlabel('False positives [%]')\n                plt.ylabel('True positives [%]')\n                plt.xlim([-0.5,20])\n                plt.ylim([80,100.5])\n                plt.grid(True)\n                ax = plt.gca()\n                ax.set_aspect('equal')\n                plt.show()\n                \n                \n    data_target=data[['target']].copy()           \n  \n    \n    print(\"first for loop\")\n    with tqdm(total=NUM_FOLDS) as pbar:\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(np.zeros(len(data_target)), data_target['target'])):\n            #data[predictors].iloc[train_idx].to_parquet(\"/kaggle/working/train_x\"+str(n_fold)+\".parquet\")\n            #data[['target']].iloc[train_idx].to_parquet(\"/kaggle/working/train_y\"+str(n_fold)+\".parquet\")\n            data[predictors].iloc[valid_idx].to_parquet(\"/kaggle/working/valid_x\"+str(n_fold)+\".parquet\")\n            data[['target']].iloc[valid_idx].to_parquet(\"/kaggle/working/valid_y\"+str(n_fold)+\".parquet\")\n            #print('valid_idx[:5] ', valid_idx[:5])\n            print(len(valid_idx))\n            \n            pbar.update(1)\n    del data;gc.collect()\n    print(\"second for loop\")\n    with tqdm(total=NUM_FOLDS) as pbar:\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(np.zeros(len(data_target)), data_target['target'])):\n            print(\"FOLD \", n_fold)\n            train_x= np.empty((0,))\n            train_y= np.empty((0,))\n            for fold in range(NUM_FOLDS):\n                if fold!=n_fold:\n                    if np.array_equal(train_x, np.empty((0,))):\n                        print(\"here \",fold)\n                        train_x=np.asarray(pd.read_parquet(\"/kaggle/working/valid_x\"+str(fold)+\".parquet\").values).astype('float32')\n                        train_y=np.asarray(pd.read_parquet(\"/kaggle/working/valid_y\"+str(fold)+\".parquet\")['target'].values).astype('float32').reshape(-1, 1)\n                    else:\n                        print(\"here2 \", fold)\n                        train_x=np.vstack((train_x, np.asarray(pd.read_parquet(\"/kaggle/working/valid_x\"+str(fold)+\".parquet\").values).astype('float32')))\n                        train_y=np.vstack((train_y, np.asarray(pd.read_parquet(\"/kaggle/working/valid_y\"+str(fold)+\".parquet\")['target'].values).astype('float32').reshape(-1, 1)))\n                    print(\"length \", train_x.shape, train_y.shape)\n                \n            print(\"train_x shape: \", train_x.shape)\n            print(\"train_y shape: \", train_y.shape)\n                        \n            #train_x, train_y = np.asarray(pd.read_parquet(\"/kaggle/working/train_x\"+str(n_fold)+\".parquet\").values).astype('float32'), np.asarray(pd.read_parquet(\"/kaggle/working/train_y\"+str(n_fold)+\".parquet\")['target'].values).astype('float32').reshape(-1, 1)\n            valid_x, valid_y = np.asarray(pd.read_parquet(\"/kaggle/working/valid_x\"+str(n_fold)+\".parquet\").values).astype('float32'), np.asarray(pd.read_parquet(\"/kaggle/working/valid_y\"+str(n_fold)+\".parquet\")['target'].values).astype('float32').reshape(-1, 1)\n            \n            \n            print(\"valid_x shape: \", valid_x.shape)\n            print(\"valid_y shape: \", valid_y.shape)\n           \n            \n            \n            \n           \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            print(\"Now create and train your model using the function that was defined earlier. Notice that the model is fit using a larger than default batch size of 2048, this is important to ensure that each batch has a decent chance of containing a few positive samples. If the batch size was too small, they would likely have no fraudulent transactions to learn from.\")\n            \n            \n\n            early_stopping = tf.keras.callbacks.EarlyStopping(\n                monitor='val_prc', \n                verbose=1,\n                patience=10,\n                mode='max',\n                restore_best_weights=True)\n            \n            print()\n            print()\n            print()\n            print()\n            print()\n            print()\n            print(\"MODEL 1\")\n            model = make_model()\n            print(model.summary())\n            \n            model.predict(train_x[:10])\n           \n        \n        \n        \n            \n            print(\"Optional: Set the correct initial bias.These initial guesses are not great. You know the dataset is imbalanced. Set the output layer's bias to reflect that, see A Recipe for Training Neural Networks: init well. This can help with initial convergence.With the default bias initialization the loss should be about math.log(2) = 0.69314\")\n            results = model.evaluate(train_x, train_y, batch_size=BATCH_SIZE, verbose=0)\n            print(\"Loss: {:0.4f}\".format(results[0]))\n            del model;gc.collect\n            print()\n            print()\n            print()\n            print()\n            print()\n            print()\n            print(\"MODEL 2\")\n            initial_bias = np.log([(1/31)])\n            \n            model = make_model(output_bias=initial_bias)\n            model.predict(train_x[:10])\n            \n            results = model.evaluate(train_x, train_y, batch_size=BATCH_SIZE, verbose=0)\n            print(\"Loss: {:0.4f}\".format(results[0]))\n            \n            \n            \n            initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights.weights.h5')\n            model.save_weights(initial_weights)\n            \n            \n            del model;gc.collect\n            \n            \n            print()\n            print()\n            print()\n            print()\n            print()\n            print()\n            \n            \n            '''\n            print(\"MODEL 3\")\n            \n            model = make_model()\n            model.load_weights(initial_weights)\n            model.layers[-1].bias.assign([0.0])\n            zero_bias_history = model.fit(\n                train_x,\n                train_y,\n                batch_size=BATCH_SIZE,\n                epochs=20,\n                validation_data=(valid_x, valid_y), \n                verbose=0)\n            \n            \n            del model;gc.collect\n            print()\n            print()\n            print()\n            print()\n            print()\n            print()\n            print(\"MODEL 4\")\n            \n            model = make_model()\n            model.load_weights(initial_weights)\n            careful_bias_history = model.fit(\n                train_x,\n                train_y,\n                batch_size=BATCH_SIZE,\n                epochs=20,\n                validation_data=(valid_x, valid_y), \n                verbose=0)\n            \n           \n            \n            \n            \n            plot_loss(zero_bias_history, \"Zero Bias\", 0)\n            plot_loss(careful_bias_history, \"Careful Bias\", 1)\n            \n            del model;gc.collect\n            \n            '''\n            print()\n            print()\n            print()\n            print()\n            print()\n            print()\n            print(\"MODEL 5\")\n            \n            model = make_model()\n            model.load_weights(initial_weights)\n            baseline_history = model.fit(\n                train_x,\n                train_y,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                callbacks=[early_stopping],\n                validation_data=(valid_x, valid_y))\n            \n            \n            \n            train_auc = baseline_history.history['auc']\n            val_auc = baseline_history.history['val_auc']\n\n            # Plot AUC curves\n            epochs = range(1, len(train_auc) + 1)\n            plt.plot(epochs, train_auc, 'b', label='Training AUC')\n            plt.plot(epochs, val_auc, 'r', label='Validation AUC')\n            plt.title('Training and Validation AUC')\n            plt.xlabel('Epochs')\n            plt.ylabel('AUC')\n            plt.legend()\n            plt.show()\n            \n            plot_metrics(baseline_history)\n            \n            train_predictions_baseline = model.predict(train_x, batch_size=BATCH_SIZE)\n            test_predictions_baseline = model.predict(valid_x, batch_size=BATCH_SIZE)\n            \n            \n            \n            baseline_results = model.evaluate(valid_x, valid_y,\n                                  batch_size=BATCH_SIZE, verbose=0)\n            print(\"HERE1\")\n            for name, value in zip(model.metrics_names, baseline_results):\n                print(name, ': ', value)\n            print(\"HERE2\")\n            print()\n\n            plot_cm(valid_y, test_predictions_baseline)\n            \n            \n            plot_cm(valid_y, test_predictions_baseline, threshold=0.1)\n            plot_cm(valid_y, test_predictions_baseline, threshold=0.01)\n            \n            \n                \n            plot_roc(\"Train Baseline\", train_y, train_predictions_baseline, color=colors[0])\n            plot_roc(\"Test Baseline\", valid_y, test_predictions_baseline, color=colors[0], linestyle='--')\n            plt.legend(loc='lower right');\n            \n            \n            \n            '''\n          \n            print()\n            print()\n            print()\n            print()\n            print()\n            print()\n            print(\"MODEL 6\")\n            \n            # Scaling by total/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\n            neg=31\n            pos=1\n            total=32\n            \n            weight_for_0 = (1 / neg) * (total / 2.0)\n            weight_for_1 = (1 / pos) * (total / 2.0)\n\n            class_weight = {0: weight_for_0, 1: weight_for_1}\n\n            print('Weight for class 0: {:.2f}'.format(weight_for_0))\n            print('Weight for class 1: {:.2f}'.format(weight_for_1))\n         \n            weighted_model = make_model()\n            weighted_model.load_weights(initial_weights)\n\n            weighted_history = weighted_model.fit(\n                train_x,\n                train_y,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                callbacks=[early_stopping],\n                validation_data=(valid_x, valid_y),\n                # The class weights go here\n                class_weight=class_weight)\n            \n            \n            train_auc = weighted_history.history['auc']\n            val_auc = weighted_history.history['val_auc']\n\n            # Plot AUC curves\n            epochs = range(1, len(train_auc) + 1)\n            plt.plot(epochs, train_auc, 'b', label='Training AUC')\n            plt.plot(epochs, val_auc, 'r', label='Validation AUC')\n            plt.title('Training and Validation AUC')\n            plt.xlabel('Epochs')\n            plt.ylabel('AUC')\n            plt.legend()\n            plt.show()\n            \n            \n            \n            \n            plot_metrics(weighted_history)\n            train_predictions_weighted = weighted_model.predict(train_x, batch_size=BATCH_SIZE)\n            test_predictions_weighted = weighted_model.predict(valid_x, batch_size=BATCH_SIZE)\n                         \n            weighted_results = weighted_model.evaluate(valid_x, valid_y,\n                                           batch_size=BATCH_SIZE, verbose=0)\n            for name, value in zip(weighted_model.metrics_names, weighted_results):\n                print(name, ': ', value)\n            print()\n\n            plot_cm(valid_y, test_predictions_weighted)\n            '''\n            print()\n            print()\n            print()\n            print()\n            print(\"FINISHED\")\n            print()\n            print()\n            print()\n            print()\n            \n            clf=model\n            fitted_models.append(clf)\n\n            if EVALUATE_VALIDATION_SET:\n                # Obtain probabilities for each class\n                probabilities = clf.predict(valid_x)\n                print('print(probabilities[0:5])',probabilities[0:10])\n                # For binary classification, we are interested in the probability of the positive class\n                oof_preds[valid_idx] = probabilities.flatten() \n                print('oof', probabilities[0:10].flatten() )\n\n\n                # Feature importance by GAIN and SPLIT\n\n            #eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n            #eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n\n            elapsed_time = time.time() - start_time\n            remaining_time = elapsed_time * (NUM_FOLDS - n_fold - 1) / (n_fold + 1)\n            print('Fold %2d AUC : %.6f. Elapsed time: %.2f seconds. Remaining time: %.2f seconds.'\n                  % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx]), elapsed_time, remaining_time))\n            del clf, train_x, train_y, valid_x, valid_y\n            gc.collect()\n            \n          \n            \n            pbar.update(1)\n   \n    print('Full AUC score %.6f' % roc_auc_score(data_target['target'], oof_preds))\n    # Get the average feature importance between folds\n    \n    \n    \n    if len(data_target)>0:\n        base=get_base(DATA_DIRECTORY, len(data_target))\n        base, cat_cols = to_pandas(base)\n        base=base[base['target'].notnull()]\n        base['score']= oof_preds\n        gini_score = gini_stability(base)\n        print(\"Gini Score of the valid set:\", gini_score)\n    \n    \n    \n    \n    # Save feature importance, test predictions and oof predictions as csv\n    \n        \n        \n  \n        \n        \n    model = VotingModel(fitted_models)\n    \n        \n\n\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.723502Z","iopub.execute_input":"2024-04-13T15:52:14.724152Z","iopub.status.idle":"2024-04-13T15:52:14.787819Z","shell.execute_reply.started":"2024-04-13T15:52:14.724120Z","shell.execute_reply":"2024-04-13T15:52:14.787030Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **SUBMISSION**","metadata":{}},{"cell_type":"code","source":"def generate_submission_file(model):\n    \n    \n    #data = pd.read_parquet(\"/kaggle/input/home-credit-2024-additional-dataset/processed_004_PCA300.parquet\")\n    data = pd.read_parquet(\"/kaggle/working/transformed_data.parquet\")   \n    data.drop(data[data['target'].notnull()].index, inplace=True)\n    \n    '''\n    length=len(test)\n    y_pred = pd.Series([0.5] * length,index=test['case_id'])\n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n    df_subm.to_csv(\"submission.csv\")\n    '''\n    #test=preprocessingX(test)\n    del_features = ['target', 'case_id','WEEK_NUM']\n    predictors = list(filter(lambda v: v not in del_features, data.columns))\n   \n    y_pred = model.predict_proba(data[predictors])\n    \n    \n   \n\n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n\n    df_subm.to_csv(\"submission.csv\")\n    \n    return True\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.789047Z","iopub.execute_input":"2024-04-13T15:52:14.789361Z","iopub.status.idle":"2024-04-13T15:52:14.801422Z","shell.execute_reply.started":"2024-04-13T15:52:14.789323Z","shell.execute_reply":"2024-04-13T15:52:14.800592Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **EVALUATE FEATURES IMPORTANCES**","metadata":{}},{"cell_type":"code","source":"def get_features_importances(predictors, model):\n    importance_df = model.get_features_importances_df(predictors)\n    mean_importance = importance_df.groupby('feature').mean().reset_index()\n    mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n    mean_importance.to_csv('feature_importance_gain{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    mean_importance.sort_values(by= 'split', ascending=False, inplace=True)\n    mean_importance.to_csv('feature_importance_split{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.802566Z","iopub.execute_input":"2024-04-13T15:52:14.803017Z","iopub.status.idle":"2024-04-13T15:52:14.811586Z","shell.execute_reply.started":"2024-04-13T15:52:14.802986Z","shell.execute_reply":"2024-04-13T15:52:14.810747Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# **DENOISE AUTOENCODER**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nMETRICS = [\n      keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n      keras.metrics.MeanSquaredError(name='Brier score'),\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\ndef denoise_autoencoder():\n    class AnomalyDetector(Model):\n        def __init__(self):\n            super(AnomalyDetector, self).__init__()\n            self.encoder = tf.keras.Sequential([\n                layers.Dense(512, activation=\"relu\"),\n                layers.Dense(256, activation=\"relu\"),\n                layers.Dense(128, activation=\"relu\"),\n                layers.Dense(64, activation=\"relu\"),\n                layers.Dense(32, activation=\"relu\"),\n                layers.Dense(16, activation=\"relu\"),\n                layers.Dense(8, activation=\"relu\"),\n                layers.Dense(1000, activation=\"relu\")])  # Reduce to 1000 features\n\n            self.decoder = tf.keras.Sequential([\n                layers.Dense(8, activation=\"relu\"),\n                layers.Dense(16, activation=\"relu\"),\n                layers.Dense(32, activation=\"relu\"),\n                layers.Dense(64, activation=\"relu\"),\n                layers.Dense(128, activation=\"relu\"),\n                layers.Dense(256, activation=\"relu\"),\n                layers.Dense(512, activation=\"relu\"),\n                layers.Dense(2592, activation=\"sigmoid\")])\n\n        def call(self, x):\n            encoded = self.encoder(x)\n            decoded = self.decoder(encoded)\n            return decoded\n\n    autoencoder = AnomalyDetector()\n\n    autoencoder.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=METRICS\n        )\n\n    data = pd.read_parquet(\"/kaggle/input/home-credit-2024-additional-dataset/processed_003_std.parquet\")\n    print(\"data shape \", data.shape)\n    del_features = ['target', 'case_id', 'WEEK_NUM']\n    predictors = list(filter(lambda v: v not in del_features, data.columns))\n    train_x, train_y = np.asarray(data[predictors][0:1200000].values).astype('float32'), np.asarray(\n        data['target'][0:1200000].values).astype('float32').reshape(-1, 1)\n    valid_x, valid_y = np.asarray(data[predictors][1200000:1500000].values).astype('float32'), np.asarray(\n        data['target'][1200000:1500000].values).astype('float32').reshape(-1, 1)\n\n    early_stopping = EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=5,           # Number of epochs with no improvement after which training will be stopped\n    restore_best_weights=True  # Restore the weights from the epoch with the best validation loss\n)\n\n# Train the autoencoder with early stopping\n    history = autoencoder.fit(\n    train_x,\n    train_x,  # Input and output are the same for autoencoder\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(valid_x, valid_x),  # Input and output are the same for autoencoder\n    callbacks=[early_stopping]  # Pass early stopping callback\n    )\n\n    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n    plt.legend()\n    encoded_data = autoencoder.encoder.predict(train_x)\n\n    encoded_df = pd.DataFrame(encoded_data, columns=[f'encoded_feature_{i}' for i in range(encoded_data.shape[1])])\n\n    print(encoded_df.head())\n    print(encoded_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:14.812759Z","iopub.execute_input":"2024-04-13T15:52:14.813123Z","iopub.status.idle":"2024-04-13T15:52:15.471677Z","shell.execute_reply.started":"2024-04-13T15:52:14.813099Z","shell.execute_reply":"2024-04-13T15:52:15.470908Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# **FEATURE ENGINEERING FUNCTION**","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    \n    \n    \n    df=df.pipe(Pipeline.handle_dates) \n    #df=df.pipe(Pipeline.filter_cols)\n    \n    '''\n     data['CREDIT_INCOME_PERCENT'] = data['AMT_CREDIT'] / data['AMT_INCOME_TOTAL']\n                                    final credit amount on the previous application./Income of the client\n    \n    \n    credamount_590A/byoccupationinc_3656910L\n    '''\n    \n    \n    columns_to_add = [\n        (pl.col(\"days30_165L\")/ pl.col(\"days360_512L\")).alias(\"ratio_queries_30\"),\n        ((pl.col(\"days90_310L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_90\")),\n        ((pl.col(\"days120_123L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_120\")),\n        ((pl.col(\"days180_256L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_180\")),\n        \n        \n        ((pl.col(\"credamount_770A\") / pl.col(\"max_mainoccupationinc_437A\")).alias(\"CREDIT_INCOME_PERCENT\")),\n        ((pl.col(\"annuity_780A\") / pl.col(\"max_mainoccupationinc_437A\")).alias(\"ANNUITY_INCOME_PERCENT\")),\n        ((pl.col(\"credamount_770A\") / pl.col(\"annuity_780A\")).alias(\"CREDIT_ANNUITY_PERCENT\")),\n        \n        ((pl.col(\"annuity_780A\") / pl.col(\"credamount_770A\")).alias(\"CREDIT_TERM\")),\n        ((pl.col(\"max_mainoccupationinc_437A\") / pl.col(\"childnum_21L\")).alias(\"CHILDREN_CNT_INCOME_PERCENT\")),\n        \n        #data['ANNUITY_LENGTH_EMPLOYED_PERCENT'] = data['CREDIT_TERM']/ data['DAYS_EMPLOYED']\n        ((pl.col(\"CREDIT_TERM\") / pl.col(\"max_empl_employedfrom_271D\")).alias(\"ANNUITY_LENGTH_EMPLOYED_PERCENT\")),\n        \n        #data['PHONE_CHANGE_EMP_PERCENT'] = data['DAYS_LAST_PHONE_CHANGE']/data['DAYS_EMPLOYED']\n        \n        \n        \n        #((pl.col(\"credamount_590A\") / pl.col(\"byoccupationinc_3656910L\")).alias(\"credit_income_percent\")),\n        \n        \n        \n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"collater_typofvalofguarant_407M\")).alias(\"sum_collater\")),\n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_active\")),\n        #((pl.col(\"collater_typofvalofguarant_407M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_close\")),\n        #((pl.col(\"overdueamount_31A\") + pl.col(\"overdueamount_659A\")).alias(\"sum_overdue_amount\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"totalamount\")).alias(\"sum_totalcredit_contract\")),\n        #((pl.col(\"totalamount_503A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_active\")),\n        #((pl.col(\"totalamount_6A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_close\")),\n        #((pl.col(\"totaldebtoverduevalue_178A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_active\")),\n        #((pl.col(\"totaldebtoverduevalue_718A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_close\")),\n        #((pl.col(\"numberofinstls_229L\") + pl.col(\"numberofinstls_320L\")).alias(\"sum_instalments\")),\n        #((pl.col(\"numberofinstls_320L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_active\")),\n        #((pl.col(\"numberofinstls_229L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_close\"))\n    ]\n\n# Add the calculated columns to the DataFrame\n    '''\n        for column in columns_to_add:\n        df = df.with_columns([column])\n        new_cols=[\"ratio_queries_30\",\"ratio_queries_90\",\"ratio_queries_120\",\"ratio_queries_180\",\"CREDIT_INCOME_PERCENT\",\n                               \"ANNUITY_INCOME_PERCENT\",\"CREDIT_ANNUITY_PERCENT\",\"CREDIT_TERM\",\"CHILDREN_CNT_INCOME_PERCENT\",\n                                \"ANNUITY_LENGTH_EMPLOYED_PERCENT\"]\n        for column_name in new_cols:\n            df=df.with_columns(\n            pl.when(pl.col(column_name).is_infinite())\n            .then(None)\n            .otherwise(pl.col(column_name))\n            .keep_name()\n        )\n\n        '''\n        \n    '''\n    df = df.with_columns(\n    'ratio_queries_30',\n    df['days30_165L'] / df['days360_512L']\n)\n    df['ratio_queries_90'] = df['days90_310L'] / df['days360_512L']\n    df['ratio_queries_120'] = df['days120_123L'] / df['days360_512L']\n    df['ratio_queries_180'] = df['days180_256L'] / df['days360_512L']\n    df['sum_collater'] = df['collater_typofvalofguarant_298M'] +    df['collater_typofvalofguarant_407M']\n    df['ratio_collater_active'] = df['collater_typofvalofguarant_298M'] +    df['sum_collater']\n    df['ratio_collater_close'] = df['collater_typofvalofguarant_407M'] +    df['sum_collater']\n    df['sum_overdue_amount'] = df['overdueamount_31A'] +    df['overdueamount_659A']\n    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['sum_overdue_amount']\n    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['sum_overdue_amount']\n    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['total_overdue_amount']\n    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['total_overdue_amount']\n    df['sum_totalcredit_contract'] = df['totalamount']\n    df['ratio_totalcredit_contract_active'] = df['totalamount_503A'] /    df['sum_totalcredit_contract']\n    df['ratio_totalcredit_contract_close'] = df['totalamount_6A'] /    df['sum_totalcredit_contract']\n    df['ratio_overdue_debt_active'] = df['totaldebtoverduevalue_178A'] /    df['totaldebt_9A']\n    df['ratio_overdue_debt_close'] = df['totaldebtoverduevalue_718A'] /    df['totaldebt_9A']\n    df['sum_instalments'] = df['numberofinstls_229L'] +    df['numberofinstls_320L']\n    df['ratio_instalments_active'] = df['numberofinstls_320L'] /    df['sum_instalments']\n    df['ratio_instalments_close'] = df['numberofinstls_229L'] /    df['sum_instalments']\n    '''\n    df=df.pipe(Pipeline.filter_cols)\n    \n    columns_to_drop=[\n     \n   'min_pmts_year_1139T',              \n'mean_pmts_year_1139T',                            \n'max_pmts_year_1139T',                             \n'min_pmts_year_507T' ,                               \n'mean_pmts_year_507T',                              \n'max_pmts_year_507T',\n        \n        'min_overdueamountmaxdateyear_2T'  ,               \n'mean_overdueamountmaxdateyear_2T'   ,              \n'max_overdueamountmaxdateyear_2T'  ,              \n'min_overdueamountmaxdateyear_994T'  ,              \n'mean_overdueamountmaxdateyear_994T' ,             \n'max_overdueamountmaxdateyear_994T'\n]\n    columns_to_drop_existing = [col for col in columns_to_drop if col in df.columns]\n\n    #df=df.drop(columns_to_drop_existing)\n    \n    \n    \n    features400=[\n            #0\n        'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n        \n        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n        \n         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n        'pmtscount_423L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n        'numinsttopaygr_769L','sum_credacc_actualbalance_314A','numinstunpaidmax_3546851L','max_empl_employedtotal_800L','price_1097A',\n        'ratio_queries_30','max_postype_4733339M','mean_creationdate_885D','maxdebt4_972A',\n        'mean_amount_4527230A','thirdquarter_1082L','sum_mainoccupationinc_437A','sum_pmtamount_36A','maxdpdinstldate_3546855D',\n        'min_purposeofcred_874M','max_description_351M','description_5085714M','min_dtlastpmt_581D',\n        'totaldebt_9A','max_contractst_964M','currdebt_22A','min_firstnonzeroinstldate_307D',\n        \n         'max_contractst_545M','max_purposeofcred_426M','max_pmts_dpd_303P',\n        'max_classificationofcontr_13M','max_financialinstitution_591M','max_classificationofcontr_400M','max_financialinstitution_382M','count_num_group1',\n        'eir_270L','interestrate_311L','min_purposeofcred_426M','maxdbddpdlast1m_3658939P',\n        'sum_revolvingaccount_394A','max_firstnonzeroinstldate_307D','min_dateactivated_425D','firstdatedue_489D','min_approvaldate_319D',\n        'min_refreshdate_3813885D','totalsettled_863A','count_persontype_1072L','max_credacc_cards_status_52L','min_creationdate_885D',\n        'max_pmtamount_36A','min_pmtnum_8L','min_tenor_203L','count_num_group1_tax_registry_c','max_credacc_status_367L',\n        'sumoutstandtotalest_4493215A','datelastunpaid_3546854D','sum_persontype_1072L','sumoutstandtotal_3546847A',\n        'numinstls_657L','mean_dtlastpmtallstes_3545839D','max_familystate_447L','mean_pmtamount_36A',\n        'max_purposeofcred_874M','maxdpdlast3m_392P','mean_credacc_actualbalance_314A','currdebtcredtyperange_828A',\n        'sum_persontype_792L','lastapplicationdate_877D','max_creationdate_885D','max_outstandingdebt_522A','max_credacc_actualbalance_314A',\n        \n        'max_currdebt_94A','max_pmts_dpd_1073P','mean_credamount_590A','sum_personindex_1023L','min_credacc_actualbalance_314A',\n        'sum_outstandingdebt_522A','min_amount_4527230A','mean_persontype_792L','mean_credacc_credlmt_575A','lastactivateddate_801D',\n        'sum_currdebt_94A','count_personindex_1023L','count_relationshiptoclient_642T','max_dateactivated_425D','max_housetype_905L',\n        'min_currdebt_94A','max_relationshiptoclient_415T','min_lastupdate_388D','min_processingdate_168D',\n        'min_dateofrealrepmt_138D','mean_lastupdate_388D','max_persontype_1072L','max_persontype_792L','min_pmtamount_36A',\n        'lastapprcommoditycat_1041M','pmtaverage_4527227A','annuity_780A','mean_dateofrealrepmt_138D',\n        'sum_annuity_853A','mean_pmts_dpd_1073P','min_credacc_credlmt_575A','lastapprdate_640D','mean_credacc_minhisbal_90A',\n        'min_dateofcredend_353D','mean_dtlastpmt_581D','max_credacc_minhisbal_90A','avgoutstandbalancel6m_4187114A','max_approvaldate_319D',\n        'maxannuity_159A','min_credacc_minhisbal_90A','sum_amount_4917619A','cntincpaycont9m_3716944L','min_dateofcredstart_181D',\n        'mean_refreshdate_3813885D','max_remitter_829L','pmtaverage_3A','avglnamtstart24m_4525187A','education_1103M',\n       'mean_dpdmax_139P','mean_numberofoverdueinstlmax_1039L','min_recorddate_4527225D','min_annuity_853A',\n        'max_dpdmax_139P','sum_dpdmax_139P','lastdelinqdate_224D','mean_persontype_1072L',\n        'count_num_group1_cb_a_2','count_num_group2_cb_a_2','twobodfilling_608L','sum_numberofoverdueinstlmax_1039L','homephncnt_628L',\n        'count_num_group1_tax_registry_b','datefirstoffer_1144D','max_numberofoverdueinstlmax_1039L','min_numberofoverdueinstlmax_1039L',\n        'mean_downpmt_134A','max_empls_economicalst_849M','min_revolvingaccount_394A','responsedate_4527233D',\n        'sum_isbidproduct_390L','max_mainoccupationinc_437A','count_familystate_447L','min_dateofcredstart_739D','max_amount_4917619A',\n        'mean_dateofcredend_353D','min_dpdmax_139P','mean_revolvingaccount_394A','maininc_215A','lastrejectcredamount_222A',\n        'max_processingdate_168D','min_totaldebtoverduevalue_178A','inittransactioncode_186L','max_deductiondate_4917603D',\n        'min_deductiondate_4917603D','mean_processingdate_168D','contractssum_5085716L','mean_dateofcredstart_181D','applicationscnt_867L',\n        'mean_amount_4917619A','max_revolvingaccount_394A','mean_isbidproduct_390L','mean_dateofcredstart_739D','min_pmts_dpd_1073P',\n        \n        'sum_credacc_credlmt_575A','min_pmts_dpd_303P','lastapprcredamount_781A','max_empl_industry_691L',\n        'min_amount_4917619A','mean_annuity_853A',\n       'max_overdueamountmaxdatemonth_365T','max_downpmt_134A','disbursementtype_67L',\n        'min_overdueamountmaxdatemonth_284T','sum_numberofcontrsvalue_358L','count_num_group1_person2','sum_byoccupationinc_3656910L','mean_deductiondate_4917603D',\n        'sellerplacescnt_216L','mean_overdueamountmaxdatemonth_365T','max_overdueamountmaxdatemonth_284T','mean_approvaldate_319D','credtype_322L',\n        'max_numberofcontrsvalue_358L','mean_numberofcontrsvalue_358L','min_downpmt_134A','min_credacc_maxhisbal_375A','mean_isdebitcard_527L',\n        'min_mainoccupationinc_384A','bankacctype_710L','mean_pmts_overdue_1152A','min_numberofcontrsvalue_358L','min_mainoccupationinc_437A',\n        'min_residualamount_856A','mean_byoccupationinc_3656910L','downpmt_116A','isbidproduct_1095L','clientscnt12m_3712952L',\n        'mean_credacc_maxhisbal_375A','max_nominalrate_281L','mean_dateactivated_425D','sum_downpmt_134A','mean_totaldebtoverduevalue_178A',\n        \n         'mean_numberofinstls_320L','max_numberofinstls_320L','max_isdebitcard_527L','mean_nominalrate_281L','dtlastpmtallstes_4499206D',\n        'max_lastupdate_388D','responsedate_4917613D','sum_credacc_minhisbal_90A','max_byoccupationinc_3656910L',\n        'min_credacc_transactions_402L','min_instlamount_768A','inittransactionamount_650A','max_totaldebtoverduevalue_178A','min_isdebitcard_527L',\n        'clientscnt6m_3712949L','mean_credacc_transactions_402L','max_credacc_maxhisbal_375A','min_numberofinstls_320L','mean_totalamount_996A',\n        'validfrom_1069D','count_num_group1_cb_a_1','mean_residualamount_856A','max_debtoverdue_47A',\n        'max_dateofrealrepmt_138D','mean_totalamount_6A','min_pmts_overdue_1152A',\"max_cancelreason_3545846M\",\n    ]\n    \n    features125=[\n        \n        'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n        \n        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n        'pmtscount_423L','max_empl_industry_691L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n        \n    ]\n    \n    features200=[\n         'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n        \n        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n        \n         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n        'pmtscount_423L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n        'numinsttopaygr_769L','sum_credacc_actualbalance_314A','numinstunpaidmax_3546851L','max_empl_employedtotal_800L','price_1097A',\n        'ratio_queries_30','max_postype_4733339M','mean_creationdate_885D','maxdebt4_972A',\n        'mean_amount_4527230A','thirdquarter_1082L','sum_mainoccupationinc_437A','sum_pmtamount_36A','maxdpdinstldate_3546855D',\n        'min_purposeofcred_874M','max_description_351M','description_5085714M','min_dtlastpmt_581D',\n        'totaldebt_9A','max_contractst_964M','currdebt_22A','min_firstnonzeroinstldate_307D',\n        \n         'max_contractst_545M','max_purposeofcred_426M','max_pmts_dpd_303P',\n        'max_classificationofcontr_13M','max_financialinstitution_591M','max_classificationofcontr_400M','max_financialinstitution_382M','count_num_group1',\n        'eir_270L','interestrate_311L','min_purposeofcred_426M','maxdbddpdlast1m_3658939P',\n        'sum_revolvingaccount_394A','max_firstnonzeroinstldate_307D','min_dateactivated_425D','firstdatedue_489D','min_approvaldate_319D',\n        'min_refreshdate_3813885D','totalsettled_863A','count_persontype_1072L','max_credacc_cards_status_52L','min_creationdate_885D',\n        'max_pmtamount_36A','min_pmtnum_8L','min_tenor_203L','count_num_group1_tax_registry_c','max_credacc_status_367L',\n        'sumoutstandtotalest_4493215A','datelastunpaid_3546854D','sum_persontype_1072L','sumoutstandtotal_3546847A',\n        'numinstls_657L','mean_dtlastpmtallstes_3545839D','max_familystate_447L','mean_pmtamount_36A',\n        'max_purposeofcred_874M','maxdpdlast3m_392P','mean_credacc_actualbalance_314A','currdebtcredtyperange_828A',\n        'sum_persontype_792L','lastapplicationdate_877D','max_creationdate_885D','max_outstandingdebt_522A','max_credacc_actualbalance_314A',\n        \n        'max_currdebt_94A','max_pmts_dpd_1073P','mean_credamount_590A','sum_personindex_1023L','min_credacc_actualbalance_314A',\n        'sum_outstandingdebt_522A','min_amount_4527230A','mean_persontype_792L','mean_credacc_credlmt_575A','lastactivateddate_801D',\n        'sum_currdebt_94A','count_personindex_1023L','count_relationshiptoclient_642T','max_dateactivated_425D','max_housetype_905L',\n        'min_currdebt_94A','max_relationshiptoclient_415T','min_lastupdate_388D','min_processingdate_168D',\n        'min_dateofrealrepmt_138D','mean_lastupdate_388D','max_persontype_1072L','max_persontype_792L','min_pmtamount_36A',\n        'lastapprcommoditycat_1041M','pmtaverage_4527227A','annuity_780A','mean_dateofrealrepmt_138D'\n    ]\n    \n    features575=[\"count_num_group1\",\"count_num_group2\",\"count_num_group1_applprev2\",\"count_num_group1_person1\",\n                 \"count_num_group1_person2\",\"count_num_group2_person2\", \"count_num_group1_debitcard\",\n                 \"count_num_group1_tax_registry_a\",\"count_num_group1_tax_registry_b\",\"count_num_group1_tax_registry_c\",\n                 \"count_num_group1_cb_a_1\", \"count_num_group1_cb_a_2\", \"count_num_group2_cb_a_2\",\n        \"month_decision\", \"weekday_decision\", \"credamount_770A\", \"applicationcnt_361L\", \"applications30d_658L\", \"applicationscnt_1086L\", \"applicationscnt_464L\", \"applicationscnt_867L\", \"clientscnt_1022L\", \"clientscnt_100L\", \"clientscnt_1071L\", \"clientscnt_1130L\", \"clientscnt_157L\",\n                 \"clientscnt_257L\", \"clientscnt_304L\", \"clientscnt_360L\", \"clientscnt_493L\", \"clientscnt_533L\", \"clientscnt_887L\", \"clientscnt_946L\", \"deferredmnthsnum_166L\", \"disbursedcredamount_1113A\", \"downpmt_116A\", \"homephncnt_628L\", \"isbidproduct_1095L\", \"mobilephncnt_593L\", \"numactivecreds_622L\", \"numactivecredschannel_414L\", \"numactiverelcontr_750L\", \"numcontrs3months_479L\", \"numnotactivated_1143L\", \"numpmtchanneldd_318L\", \"numrejects9m_859L\", \"sellerplacecnt_915L\", \"max_mainoccupationinc_384A\", \"max_birth_259D\", \"max_num_group1_9\", \"birthdate_574D\", \"dateofbirth_337D\", \"days180_256L\", \"days30_165L\", \"days360_512L\", \"firstquarter_103L\", \"fourthquarter_440L\", \"secondquarter_766L\", \"thirdquarter_1082L\", \"max_debtoutstand_525A\", \"max_debtoverdue_47A\", \"max_refreshdate_3813885D\", \"mean_refreshdate_3813885D\", \"pmtscount_423L\", \"pmtssum_45A\", \"responsedate_1012D\", \"responsedate_4527233D\", \"actualdpdtolerance_344P\", \"amtinstpaidbefduel24m_4187115A\", \"numinstlswithdpd5_4187116L\", \"annuitynextmonth_57A\", \"currdebt_22A\", \"currdebtcredtyperange_828A\", \"numinstls_657L\", \"totalsettled_863A\", \"mindbddpdlast24m_3658935P\", \"avgdbddpdlast3m_4187120P\", \"mindbdtollast24m_4525191P\", \"avgdpdtolclosure24_3658938P\", \"avginstallast24m_3658937A\", \"maxinstallast24m_3658928A\", \"avgmaxdpdlast9m_3716943P\", \"avgoutstandbalancel6m_4187114A\", \"avgpmtlast12m_4525200A\", \"cntincpaycont9m_3716944L\", \"cntpmts24_3658933L\", \"commnoinclast6m_3546845L\", \"maxdpdfrom6mto36m_3546853P\", \"datefirstoffer_1144D\", \"datelastunpaid_3546854D\", \"daysoverduetolerancedd_3976961L\", \"numinsttopaygr_769L\", \"dtlastpmtallstes_4499206D\", \"eir_270L\", \"firstclxcampaign_1125D\", \"firstdatedue_489D\", \"lastactivateddate_801D\", \"lastapplicationdate_877D\", \"mean_creationdate_885D\", \"max_num_group1\", \"last_num_group1\", \"max_num_group2_14\", \"last_num_group2_14\", \"lastapprcredamount_781A\", \"lastapprdate_640D\", \"lastdelinqdate_224D\", \"lastrejectcredamount_222A\", \"lastrejectdate_50D\", \"maininc_215A\", \"mastercontrelectronic_519L\", \"mastercontrexist_109L\", \"maxannuity_159A\", \"maxdebt4_972A\", \"maxdpdlast24m_143P\", \"maxdpdlast3m_392P\", \"maxdpdtolerance_374P\", \"maxdbddpdlast1m_3658939P\", \"maxdbddpdtollast12m_3658940P\", \"maxdbddpdtollast6m_4187119P\", \"maxdpdinstldate_3546855D\", \"maxdpdinstlnum_3546846P\", \"maxlnamtstart6m_4525199A\", \"maxoutstandbalancel12m_4187113A\", \"numinstpaidearly_338L\", \"numinstpaidearly5d_1087L\", \"numinstpaidlate1d_3546852L\", \"numincomingpmts_3546848L\", \"numinstlsallpaid_934L\", \"numinstlswithdpd10_728L\", \"numinstlswithoutdpd_562L\", \"numinstpaid_4499208L\", \"numinstpaidearly3d_3546850L\", \"numinstregularpaidest_4493210L\", \"numinstpaidearly5dest_4493211L\", \"sumoutstandtotalest_4493215A\", \"numinstpaidlastcontr_4325080L\", \"numinstregularpaid_973L\", \"pctinstlsallpaidearl3d_427L\", \"pctinstlsallpaidlate1d_3546856L\", \"pctinstlsallpaidlat10d_839L\", \"pctinstlsallpaidlate4d_3546849L\", \"pctinstlsallpaidlate6d_3546844L\", \"pmtnum_254L\", \"posfpd10lastmonth_333P\", \"posfpd30lastmonth_3976960P\", \"posfstqpd30lastmonth_3976962P\", \"price_1097A\", \"sumoutstandtotal_3546847A\", \"totaldebt_9A\", \"mean_actualdpd_943P\", \"max_annuity_853A\", \"mean_annuity_853A\", \"max_credacc_credlmt_575A\", \"max_credamount_590A\", \"max_downpmt_134A\", \"mean_credacc_credlmt_575A\", \"mean_credamount_590A\", \"mean_downpmt_134A\", \"max_currdebt_94A\", \"mean_currdebt_94A\", \"max_mainoccupationinc_437A\", \"mean_mainoccupationinc_437A\", \"mean_maxdpdtolerance_577P\", \"max_outstandingdebt_522A\", \"mean_outstandingdebt_522A\", \"last_actualdpd_943P\", \"last_annuity_853A\", \"last_credacc_credlmt_575A\", \"last_credamount_590A\", \"last_downpmt_134A\", \"last_currdebt_94A\", \"last_mainoccupationinc_437A\", \"last_maxdpdtolerance_577P\", \"last_outstandingdebt_522A\", \"max_approvaldate_319D\", \"mean_approvaldate_319D\", \"max_dateactivated_425D\", \"mean_dateactivated_425D\", \"max_dtlastpmt_581D\", \"mean_dtlastpmt_581D\", \"max_dtlastpmtallstes_3545839D\", \"mean_dtlastpmtallstes_3545839D\", \"max_employedfrom_700D\", \"max_firstnonzeroinstldate_307D\", \"mean_firstnonzeroinstldate_307D\", \"last_approvaldate_319D\", \"last_creationdate_885D\", \"last_dateactivated_425D\", \"last_dtlastpmtallstes_3545839D\", \"last_employedfrom_700D\", \"last_firstnonzeroinstldate_307D\", \"max_byoccupationinc_3656910L\", \"max_childnum_21L\", \"max_pmtnum_8L\", \"last_pmtnum_8L\", \"max_pmtamount_36A\", \"last_pmtamount_36A\", \"max_processingdate_168D\", \"last_processingdate_168D\", \"max_num_group1_5\", \"mean_credlmt_230A\", \"mean_credlmt_935A\", \"mean_pmts_dpd_1073P\", \"max_dpdmaxdatemonth_89T\", \"max_dpdmaxdateyear_596T\", \"max_pmts_dpd_303P\", \"mean_dpdmax_757P\", \"max_dpdmaxdatemonth_442T\", \"max_dpdmaxdateyear_896T\", \"mean_pmts_dpd_303P\", \"mean_instlamount_768A\", \"mean_monthlyinstlamount_332A\", \"max_monthlyinstlamount_674A\", \"mean_monthlyinstlamount_674A\", \"mean_outstandingamount_354A\", \"mean_outstandingamount_362A\", \"mean_overdueamount_31A\", \"mean_overdueamount_659A\", \"max_numberofoverdueinstls_725L\", \"mean_overdueamountmax2_14A\", \"mean_totaloutstanddebtvalue_39A\", \"mean_dateofcredend_289D\", \"mean_dateofcredstart_739D\", \"max_lastupdate_1112D\", \"mean_lastupdate_1112D\", \"max_numberofcontrsvalue_258L\", \"max_numberofoverdueinstlmax_1039L\", \"max_overdueamountmaxdatemonth_365T\", \"max_overdueamountmaxdateyear_2T\", \"mean_pmts_overdue_1140A\", \"max_pmts_month_158T\", \"max_pmts_year_1139T\", \"mean_overdueamountmax2_398A\", \"max_dateofcredend_353D\", \"max_dateofcredstart_181D\", \"mean_dateofcredend_353D\", \"max_numberofoverdueinstlmax_1151L\", \"mean_overdueamountmax_35A\", \"max_overdueamountmaxdatemonth_284T\", \"max_overdueamountmaxdateyear_994T\", \"mean_pmts_overdue_1152A\", \"max_residualamount_488A\", \"mean_residualamount_856A\", \"max_totalamount_6A\", \"mean_totalamount_6A\", \"mean_totalamount_996A\", \"mean_totaldebtoverduevalue_718A\", \"mean_totaloutstanddebtvalue_668A\", \"max_numberofcontrsvalue_358L\", \"max_dateofrealrepmt_138D\", \"mean_dateofrealrepmt_138D\", \"max_lastupdate_388D\", \"mean_lastupdate_388D\", \"max_numberofoverdueinstlmaxdat_148D\", \"mean_numberofoverdueinstlmaxdat_641D\", \"mean_overdueamountmax2date_1002D\", \"max_overdueamountmax2date_1142D\", \"last_refreshdate_3813885D\", \"max_nominalrate_281L\", \"max_nominalrate_498L\", \"max_numberofinstls_229L\", \"max_numberofinstls_320L\", \"max_numberofoutstandinstls_520L\", \"max_numberofoutstandinstls_59L\", \"max_numberofoverdueinstls_834L\", \"max_periodicityofpmts_1102L\", \"max_periodicityofpmts_837L\", \"last_num_group1_6\", \"last_mainoccupationinc_384A\", \"last_birth_259D\", \"max_empl_employedfrom_271D\", \"last_personindex_1023L\", \"last_persontype_1072L\", \"max_collater_valueofguarantee_1124L\", \"max_collater_valueofguarantee_876L\", \"max_pmts_month_706T\", \"max_pmts_year_507T\", \"last_pmts_month_158T\", \"last_pmts_year_1139T\", \"last_pmts_month_706T\", \"last_pmts_year_507T\", \"max_num_group1_13\", \"max_num_group2_13\", \"last_num_group2_13\", \"max_num_group1_15\", \"max_num_group2_15\", \"description_5085714M\", \"education_1103M\", \"education_88M\", \"maritalst_385M\", \"maritalst_893M\", \"requesttype_4525192L\", \"credtype_322L\", \"disbursementtype_67L\", \"inittransactioncode_186L\", \"lastapprcommoditycat_1041M\", \"lastcancelreason_561M\", \"lastrejectcommoditycat_161M\", \"lastrejectcommodtypec_5251769M\", \"lastrejectreason_759M\", \"lastrejectreasonclient_4145040M\", \"lastst_736L\", \"opencred_647L\", \"paytype1st_925L\", \"paytype_783L\", \"twobodfilling_608L\", \"max_cancelreason_3545846M\", \"max_education_1138M\", \"max_postype_4733339M\", \"max_rejectreason_755M\", \"max_rejectreasonclient_4145042M\", \"last_cancelreason_3545846M\", \"last_education_1138M\", \"last_postype_4733339M\", \"last_rejectreason_755M\", \"last_rejectreasonclient_4145042M\", \"max_credtype_587L\", \"max_familystate_726L\", \"max_inittransactioncode_279L\", \"max_isbidproduct_390L\", \"max_status_219L\", \"last_credtype_587L\", \"last_familystate_726L\", \"last_inittransactioncode_279L\", \"last_isbidproduct_390L\", \"last_status_219L\", \"max_classificationofcontr_13M\", \"max_classificationofcontr_400M\", \"max_contractst_545M\", \"max_contractst_964M\", \"max_description_351M\", \"max_financialinstitution_382M\", \"max_financialinstitution_591M\", \"max_purposeofcred_426M\", \"max_purposeofcred_874M\", \"max_subjectrole_182M\", \"max_subjectrole_93M\", \"last_classificationofcontr_13M\", \"last_classificationofcontr_400M\", \"last_contractst_545M\", \"last_contractst_964M\", \"last_description_351M\", \"last_financialinstitution_382M\", \"last_financialinstitution_591M\", \"last_purposeofcred_426M\", \"last_purposeofcred_874M\", \"last_subjectrole_182M\", \"last_subjectrole_93M\", \"max_education_927M\", \"max_empladdr_district_926M\", \"max_empladdr_zipcode_114M\", \"max_language1_981M\", \"last_education_927M\", \"last_empladdr_district_926M\", \"last_empladdr_zipcode_114M\", \"last_language1_981M\", \"max_contaddr_matchlist_1032L\", \"max_contaddr_smempladdr_334L\", \"max_empl_employedtotal_800L\", \"max_empl_industry_691L\", \"max_familystate_447L\", \"max_incometype_1044T\", \"max_relationshiptoclient_415T\", \"max_relationshiptoclient_642T\", \"max_remitter_829L\", \"max_role_1084L\", \"max_safeguarantyflag_411L\", \"max_sex_738L\", \"max_type_25L\", \"last_contaddr_matchlist_1032L\", \"last_contaddr_smempladdr_334L\", \"last_incometype_1044T\", \"last_relationshiptoclient_642T\", \"last_role_1084L\", \"last_safeguarantyflag_411L\", \"last_sex_738L\", \"last_type_25L\", \"max_collater_typofvalofguarant_298M\", \"max_collater_typofvalofguarant_407M\", \"max_collaterals_typeofguarante_359M\", \"max_collaterals_typeofguarante_669M\", \"max_subjectroles_name_541M\", \"max_subjectroles_name_838M\", \"last_collater_typofvalofguarant_298M\", \"last_collater_typofvalofguarant_407M\", \"last_collaterals_typeofguarante_359M\", \"last_collaterals_typeofguarante_669M\", \"last_subjectroles_name_541M\", \"last_subjectroles_name_838M\", \"max_cacccardblochreas_147M\", \"last_cacccardblochreas_147M\", \"max_conts_type_509L\", \"last_conts_type_509L\", \"max_conts_role_79M\", \n                 \"max_empls_economicalst_849M\", \"max_empls_employer_name_740M\", \"last_conts_role_79M\", \"last_empls_economicalst_849M\", \"last_empls_employer_name_740M\"]\n    \n    \n    \n    \n    seen = set()\n    duplicates = []\n    for item in features400:\n        if item in seen:\n            duplicates.append(item)\n        else:\n            seen.add(item)\n    print(\"duplicates:\")\n    print(duplicates)\n    print()\n    \n    print(\"Length of features400: \", len(features400))\n    print(\"Length of features200: \", len(features200))\n    print(\"Length of features125: \", len(features125))\n    missing_columns = [col for col in features575 if col not in df.columns]\n\n    # Print missing columns, if any\n    if missing_columns:\n        print(\"The following columns are missing in the DataFrame:\")\n        for col in missing_columns:\n            print(col)\n    else:\n        print(\"All columns from features400 are present in the DataFrame.\")\n    \n    # Columns to preserve\n    preserved_columns = ['target', 'case_id', 'WEEK_NUM']\n\n    # Identify columns to drop excluding the preserved columns\n    columns_to_drop = [col for col in df.columns if col not in preserved_columns + features575]\n\n    # Drop columns that are not in features_selected and not preserved\n    \n    for col in columns_to_drop:\n        df.drop_in_place(col)\n    \n    \n    if BALANCE_COLUMNS:\n\n        # the test set might be different, so lets drop the columns that have many nan values in test set\n        train = df.filter(df['target'].is_not_null())\n        test = df.filter(df['target'].is_null())\n\n        valid_percentage_train = []\n        valid_percentage_test=[]\n        for col in df.columns:\n            valid_percentage_train.append(train[col].count())\n            valid_percentage_test.append(test[col].count())\n\n        valid_percentage_train = pd.Series(valid_percentage_train)\n        valid_percentage_test= pd.Series(valid_percentage_test)\n\n        print(train.count())\n        print(test.count())\n        print(\"length of train\",len(valid_percentage_train))\n        print(\"length of test\",len(valid_percentage_test))\n        print(\"df columns\",len(df.columns))\n\n        info_df = pd.DataFrame({'column': df.columns, 'valid_train': valid_percentage_train, 'valid_test': valid_percentage_test})\n        irrelevant_columns = info_df[info_df['valid_test'] < 0.5 * info_df['valid_train']]['column'].to_list()\n        columns_to_drop = [col for col in df.columns if (col not in preserved_columns) and (col in irrelevant_columns) ]\n        df = df.drop(columns=columns_to_drop)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.473351Z","iopub.execute_input":"2024-04-13T15:52:15.473625Z","iopub.status.idle":"2024-04-13T15:52:15.545446Z","shell.execute_reply.started":"2024-04-13T15:52:15.473602Z","shell.execute_reply":"2024-04-13T15:52:15.544620Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# **DATA TRANSFORMATION**","metadata":{}},{"cell_type":"code","source":"def data_transformation(data):\n    #print(\"shape before preprocessing\", data.shape)\n    #data = preprocessingX(data)\n    #print(\"shape after preprocessing\", data.shape)\n    \n    print(\"Column names:\", data.columns)  # Print out the column names for debugging\n    \n    print(\"shape before normalizing\", data.shape)\n    cols_to_keep = ['target', 'case_id', 'MONTH', \"WEEK_NUM\", \"date_decision\"]\n    for col in data.select_dtypes(include='category').columns:\n        cols_to_keep.append(col)\n        \n    cols_to_standardize = [col for col in data.columns if col not in cols_to_keep]\n    transformed_cols_dfs=[]\n    with tqdm(total=len(cols_to_standardize), desc=\"Processing columns\") as pbar:\n        # Iterate through columns and update the progress bar\n        for col in cols_to_standardize:\n            try:\n                col_series = data[col].copy().values.reshape(-1, 1)\n                data.drop(columns=[col], inplace=True)  # Using df.drop() instead of df.pop() to drop the column\n                scaler = StandardScaler()\n                scaled_col_series = scaler.fit_transform(col_series)\n                clipped_scaled_col_series=np.clip(scaled_col_series, -5, 5)\n                \n                \n                col_df = pd.DataFrame(clipped_scaled_col_series.flatten(), columns=[col])\n                transformed_cols_dfs.append(col_df)\n                del scaler\n                gc.collect()\n            except KeyError:\n                print(f\"Column '{col}' not found in the DataFrame.\")\n            \n            # Update the progress bar\n            pbar.update(1)\n    \n    transformed_cols_dfs.append(data)\n    data = pd.concat(transformed_cols_dfs, axis=1)\n    \n    \n    print(\"shape after normalizing\", data.shape)\n    print()\n    \n   \n    return data","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.546484Z","iopub.execute_input":"2024-04-13T15:52:15.546747Z","iopub.status.idle":"2024-04-13T15:52:15.561372Z","shell.execute_reply.started":"2024-04-13T15:52:15.546724Z","shell.execute_reply":"2024-04-13T15:52:15.560525Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **GET FUNCTIONS**","metadata":{}},{"cell_type":"markdown","source":"### get_base()","metadata":{}},{"cell_type":"code","source":"def get_base(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    train={}\n    test={}\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet'))\n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet')).limit(num_rows) \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_base.parquet'))    \n    length=len(test)\n    nan_series=pl.Series([None] * length)\n    test = test.select(pl.col(\"*\"), nan_series.alias(\"target\"))\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    \n    \n    df = df.with_columns(pl.col('date_decision').cast(pl.Date))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.562445Z","iopub.execute_input":"2024-04-13T15:52:15.562777Z","iopub.status.idle":"2024-04-13T15:52:15.574943Z","shell.execute_reply.started":"2024-04-13T15:52:15.562751Z","shell.execute_reply":"2024-04-13T15:52:15.574087Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### get_static()","metadata":{}},{"cell_type":"code","source":"def get_static(path, num_rows = None):\n# Read the Parquet file using scan() method\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path,low_memory=True).pipe(Pipeline.set_table_dtypes) )\n    train = (pl.concat(chunks, how=\"vertical_relaxed\")).pipe(Pipeline.filter_cols)\n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n        \n        train=df1\n        del df2\n        gc.collect()\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path,low_memory=True).pipe(Pipeline.set_table_dtypes) )\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    \n    \n    columns_to_keep = train.columns\n\n# Find columns in 'test' that are not in 'train'\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n\n# Drop columns from 'test' that are not in 'train'\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.576289Z","iopub.execute_input":"2024-04-13T15:52:15.576606Z","iopub.status.idle":"2024-04-13T15:52:15.588623Z","shell.execute_reply.started":"2024-04-13T15:52:15.576560Z","shell.execute_reply":"2024-04-13T15:52:15.587872Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### get_static_cb()","metadata":{}},{"cell_type":"code","source":"def get_static_cb(path, num_rows = None):\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet'),low_memory=True).limit(num_rows).pipe(Pipeline.set_table_dtypes) \n       \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_static_cb_0.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.589550Z","iopub.execute_input":"2024-04-13T15:52:15.589836Z","iopub.status.idle":"2024-04-13T15:52:15.598105Z","shell.execute_reply.started":"2024-04-13T15:52:15.589813Z","shell.execute_reply":"2024-04-13T15:52:15.597326Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev1(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev1(path, num_rows = None):\n    \n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    train = pl.concat(chunks, how=\"vertical_relaxed\")#.pipe(Pipeline.filter_cols)\n    \n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n\n        train=df1\n        del df2   \n        gc.collect()\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    agg_df = group(df, '', APPLPREV1_AGG)\n    del df;gc.collect()\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.599131Z","iopub.execute_input":"2024-04-13T15:52:15.599436Z","iopub.status.idle":"2024-04-13T15:52:15.611288Z","shell.execute_reply.started":"2024-04-13T15:52:15.599413Z","shell.execute_reply":"2024-04-13T15:52:15.610435Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev2(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev2(path, num_rows = None):\n    train={}\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n       \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', APPLPREV2_AGG)\n    del df ;gc.collect()\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.612178Z","iopub.execute_input":"2024-04-13T15:52:15.612447Z","iopub.status.idle":"2024-04-13T15:52:15.622169Z","shell.execute_reply.started":"2024-04-13T15:52:15.612424Z","shell.execute_reply":"2024-04-13T15:52:15.621344Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### get_person1","metadata":{}},{"cell_type":"code","source":"def get_person1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n      \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', PERSON1_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.623146Z","iopub.execute_input":"2024-04-13T15:52:15.623410Z","iopub.status.idle":"2024-04-13T15:52:15.635461Z","shell.execute_reply.started":"2024-04-13T15:52:15.623388Z","shell.execute_reply":"2024-04-13T15:52:15.634681Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### get_person2","metadata":{}},{"cell_type":"code","source":"def get_person2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes)\n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', PERSON2_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.640281Z","iopub.execute_input":"2024-04-13T15:52:15.640596Z","iopub.status.idle":"2024-04-13T15:52:15.648419Z","shell.execute_reply.started":"2024-04-13T15:52:15.640572Z","shell.execute_reply":"2024-04-13T15:52:15.647512Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### other","metadata":{}},{"cell_type":"code","source":"def get_other(path, num_rows = None):\n     # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n         \n    test = pl.read_parquet(os.path.join(path, 'test/test_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', OTHER_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.649485Z","iopub.execute_input":"2024-04-13T15:52:15.649815Z","iopub.status.idle":"2024-04-13T15:52:15.661216Z","shell.execute_reply.started":"2024-04-13T15:52:15.649783Z","shell.execute_reply":"2024-04-13T15:52:15.660370Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## get_debitcard","metadata":{}},{"cell_type":"code","source":"def get_debitcard(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n      \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', DEBITCARD_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.662190Z","iopub.execute_input":"2024-04-13T15:52:15.662447Z","iopub.status.idle":"2024-04-13T15:52:15.671634Z","shell.execute_reply.started":"2024-04-13T15:52:15.662425Z","shell.execute_reply":"2024-04-13T15:52:15.670818Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_a","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_a(path, num_rows = None):\n    \n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n  \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', TAX_REGISTRY_A_AGG)    \n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.672638Z","iopub.execute_input":"2024-04-13T15:52:15.672951Z","iopub.status.idle":"2024-04-13T15:52:15.684854Z","shell.execute_reply.started":"2024-04-13T15:52:15.672921Z","shell.execute_reply":"2024-04-13T15:52:15.684094Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_b","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_b(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes)\n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', TAX_REGISTRY_B_AGG) \n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.685931Z","iopub.execute_input":"2024-04-13T15:52:15.686269Z","iopub.status.idle":"2024-04-13T15:52:15.694612Z","shell.execute_reply.started":"2024-04-13T15:52:15.686246Z","shell.execute_reply":"2024-04-13T15:52:15.693843Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_c","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_c(path, num_rows = None):\n     # Read the Parquet file using scan() method\n# Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', TAX_REGISTRY_C_AGG)    \n    del df; gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.695632Z","iopub.execute_input":"2024-04-13T15:52:15.695908Z","iopub.status.idle":"2024-04-13T15:52:15.706567Z","shell.execute_reply.started":"2024-04-13T15:52:15.695876Z","shell.execute_reply":"2024-04-13T15:52:15.705775Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_1(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n        \n        agg_chunks.append(agg_file_df)\n        del file_df; gc.collect()\n    \n    \n    train_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        train_agg_df=train_agg_df.vstack(agg_chunk)\n    train_agg_df.rechunk()\n        \n        \n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n       \n        agg_chunks.append(agg_file_df)\n        del file_df; gc.collect()\n        \n        \n    test_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        test_agg=test_agg_df.vstack(agg_chunk)\n    test_agg_df.rechunk()\n    \n    \n   \n\n    \n    agg_df=train_agg_df\n    agg_df=agg_df.extend(test_agg_df)\n    \n\n \n    \n    print(\"agg df \", agg_df.shape)\n   \n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.707662Z","iopub.execute_input":"2024-04-13T15:52:15.707940Z","iopub.status.idle":"2024-04-13T15:52:15.717519Z","shell.execute_reply.started":"2024-04-13T15:52:15.707916Z","shell.execute_reply":"2024-04-13T15:52:15.716758Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test ; gc.collect()\n    agg_df = group(df, '', CREDIT_BUREAU_B_1_AGG) \n    \n    \n    del df; gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.718495Z","iopub.execute_input":"2024-04-13T15:52:15.718743Z","iopub.status.idle":"2024-04-13T15:52:15.731117Z","shell.execute_reply.started":"2024-04-13T15:52:15.718722Z","shell.execute_reply":"2024-04-13T15:52:15.730332Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_2(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df;gc.collect()\n    \n    train_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        train_agg_df=train_agg_df.vstack(agg_chunk)\n    train_agg_df.rechunk()\n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df;gc.collect()\n    \n    test_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        test_agg_df=test_agg_df.vstack(agg_chunk)\n    test_agg_df.rechunk()\n    \n    agg_df=train_agg_df\n    agg_df=agg_df.extend(test_agg_df)\n    \n    print(\"agg df \", agg_df.shape)\n   \n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.732154Z","iopub.execute_input":"2024-04-13T15:52:15.732407Z","iopub.status.idle":"2024-04-13T15:52:15.744646Z","shell.execute_reply.started":"2024-04-13T15:52:15.732385Z","shell.execute_reply":"2024-04-13T15:52:15.743789Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n\n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    \n    df=pl.concat([train, test])\n    del train;del test; gc.collect()\n    agg_df = group(df, '', CREDIT_BUREAU_B_2_AGG) \n    \n    del df; gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.745642Z","iopub.execute_input":"2024-04-13T15:52:15.745863Z","iopub.status.idle":"2024-04-13T15:52:15.758499Z","shell.execute_reply.started":"2024-04-13T15:52:15.745844Z","shell.execute_reply":"2024-04-13T15:52:15.757619Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# **EXECUTION** <a id='execution'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    pd.set_option('display.max_rows', 60)\n    pd.set_option('display.max_columns', 100)\n    with timer(\"Pipeline total time\"):\n        main(debug= False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:52:15.759605Z","iopub.execute_input":"2024-04-13T15:52:15.759973Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Notebook started:\nbase dataframe shape: (1526669, 5)\nbase - done in 1s\nstatic dataframe shape: (1526669, 156)\nDATAFRAME shape: (1526669, 160)\nstatic - done in 10s\nstatic cb dataframe shape: (1500486, 31)\nDATAFRAME shape: (1526669, 190)\nstatic_cb - done in 4s\nPrevious applications depth 1 test dataframe shape: (1221524, 71)\nDATAFRAME shape: (1526669, 260)\nPrevious applications depth 1 test - done in 21s\nPrevious applications depth 2 test dataframe shape: (1221523, 8)\nDATAFRAME shape: (1526669, 267)\nPrevious applications depth 2 test - done in 4s\nPerson depth 1 test dataframe shape: (1526665, 69)\nDATAFRAME shape: (1526669, 335)\nPerson depth 1 test - done in 9s\nPerson depth 2 test dataframe shape: (1435108, 9)\nDATAFRAME shape: (1526669, 343)\nPerson depth 2 test - done in 2s\nOther test dataframe shape: (51111, 12)\nDATAFRAME shape: (1526669, 354)\nOther test - done in 1s\nDebit card test dataframe shape: (111772, 5)\nDATAFRAME shape: (1526669, 358)\nDebit card test - done in 1s\nTax registry a test dataframe shape: (457934, 10)\nDATAFRAME shape: (1526669, 367)\nTax registry a test - done in 2s\nTax registry b test dataframe shape: (150734, 10)\nDATAFRAME shape: (1526669, 376)\nTax registry b test - done in 1s\nTax registry c test dataframe shape: (482265, 12)\nDATAFRAME shape: (1526669, 387)\nTax registry c test - done in 2s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}