{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":50160,"databundleVersionId":7921029},{"sourceType":"datasetVersion","sourceId":8000644,"datasetId":4680819,"databundleVersionId":8112172}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom glob import glob\nfrom pathlib import Path\nfrom datetime import datetime\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score \nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom tqdm.notebook import tqdm\nimport joblib\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:55.669230Z","iopub.execute_input":"2024-04-01T17:51:55.669708Z","iopub.status.idle":"2024-04-01T17:51:58.496949Z","shell.execute_reply.started":"2024-04-01T17:51:55.669664Z","shell.execute_reply":"2024-04-01T17:51:58.495889Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **CONFIGURATION**\n<a id='configuration'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"code","source":"# GENERAL CONFIGURATIONS\nNUM_THREADS = 4\nDATA_DIRECTORY = \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/\"\nSUBMISSION_SUFIX = \"_model_2.1_31\"\n#MODE CONFIGURATION\nBALANCE_COLUMNS=True # set True for submission, set False for debug\nSHOW_REPORT = False\nSELECTKBEST = False\nEXPORT_DATAFRAME = False\nIMPORT_DATAFRAME = True\n# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\nGENERATE_SUBMISSION_FILES = True\nEVALUATE_VALIDATION_SET = True\nSTRATIFIED_KFOLD = True\nRANDOM_SEED = 324\nNUM_FOLDS = 10\nEARLY_STOPPING = 100\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\n\nLIGHTGBM_PARAMS = {\n     \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 5,\n    \"learning_rate\": 0.03,\n    \"n_estimators\": 1000,\n    \"colsample_bytree\": 0.5, \n    \"colsample_bynode\": 0.5,\n    \"verbose\": -1,\n    \"random_state\": 42,\n    'reg_alpha': 1,\n    'reg_lambda': 1,\n    \n  \n   \n    \"device\": \"cpu\",\n    \"is_unbalance\":False, \n   \n    \n    \n \n    \n\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.498849Z","iopub.execute_input":"2024-04-01T17:51:58.499390Z","iopub.status.idle":"2024-04-01T17:51:58.509087Z","shell.execute_reply.started":"2024-04-01T17:51:58.499357Z","shell.execute_reply":"2024-04-01T17:51:58.507354Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Set aggregations","metadata":{}},{"cell_type":"code","source":"# AGGREGATIONS\n\n# D: max, min, mean\n# M: \n# A : max, min, mean, sum\n# L : max, min, mean, sum\n\n\nAPPLPREV1_AGG = {\n\n    'num_group1':['count'],\n    'actualdpd_943P': ['min','max','mean','sum'],\n    'annuity_853A': ['min','max','mean','sum'],\n    'approvaldate_319D':['max','min','mean'],\n    'byoccupationinc_3656910L': ['min','max','mean','sum'],\n    'cancelreason_3545846M':['max'],\n    'childnum_21L': ['min','max','mean','sum'],\n    'creationdate_885D':['min','max','mean'],\n    'credacc_actualbalance_314A': ['min','max','mean','sum'],\n    'credacc_credlmt_575A': ['min','max','mean','sum'],\n    'credacc_maxhisbal_375A': ['min','max','mean','sum'],\n    'credacc_minhisbal_90A': ['min','max','mean','sum'],\n    'credacc_status_367L': ['max'],\n    'credacc_transactions_402L': ['min','max','mean','sum'],\n    'credamount_590A': ['min','max','mean','sum'],\n    'credtype_587L': ['max'],\n    'currdebt_94A': ['min','max','mean','sum'],\n    'dateactivated_425D':['min','max','mean'],\n    'district_544M':['max'],\n    'downpmt_134A': ['min','max','mean','sum'],\n    'dtlastpmt_581D':['min','max','mean'],\n    'dtlastpmtallstes_3545839D':['min','max','mean'],\n    'education_1138M':['max'],\n    'employedfrom_700D':['min','max','mean'],\n    'familystate_726L': ['max'],\n    'firstnonzeroinstldate_307D': ['min','max','mean'],\n    'inittransactioncode_279L': ['max'],\n    'isbidproduct_390L': ['min','max','mean','sum'],\n    'isdebitcard_527L': ['min','max','mean','sum'],\n    'mainoccupationinc_437A': ['min','max','mean','sum','median'],\n    'maxdpdtolerance_577P': ['min','max','mean','sum'],\n    'outstandingdebt_522A': ['min','max','mean','sum'],\n    'pmtnum_8L': ['min','max','mean','sum'],\n    'postype_4733339M':['max'],\n    #'profession_152M':['max'],\n    'rejectreason_755M':['max'],\n    'rejectreasonclient_4145042M':['max'],\n    'revolvingaccount_394A': ['min','max','mean','sum'],\n    'status_219L': ['max'],\n    'tenor_203L': ['min','max','mean','sum'],\n    \n}\nAPPLPREV2_AGG = {\n    'num_group1':['count'],\n    'num_group2':['count'],\n    'conts_type_509L':['max'],\n    #'cacccardblochreas_147M'\n    'credacc_cards_status_52L':['max']\n    \n}\nPERSON1_AGG={\n    'num_group1':['count'],\n    'birth_259D': ['max'],\n    #'childnum_185L':['max','mean','min'],\n    'contaddr_district_15M':['max'],\n    'contaddr_matchlist_1032L':['max'],\n    'contaddr_smempladdr_334L':['max'],\n    'contaddr_zipcode_807M':['max'],\n    'education_927M':['max'],\n    'empl_employedfrom_271D':['max','mean','min'],\n    'empl_employedtotal_800L':['max'],\n    'empl_industry_691L':['max'],\n    #'empladdr_district_926M'\n    #'empladdr_zipcode_114M'\n    'familystate_447L':['max','count'],\n    #'gender_992L'\n    'housetype_905L':['max'],\n    #'housingtype_772L'\n    'incometype_1044T':['max'],\n    #'isreference_387L'\n    'language1_981M':['max'],\n    'mainoccupationinc_384A':['max','mean','min', 'count'],\n    #'maritalst_703L'\n    'personindex_1023L':['max','mean','min', 'count','sum'],\n    'persontype_1072L':['max','mean','min', 'count','sum'],\n    'persontype_792L':['max','mean','min', 'count','sum'],\n    #'registaddr_district_1083M'\n    #'registaddr_zipcode_184M'\n    'relationshiptoclient_415T':['max','count'],\n    'relationshiptoclient_642T':['max','count'],\n    'remitter_829L':['max'],\n    'role_1084L':['max','count'],\n    #'role_993L'\n    'safeguarantyflag_411L':['max'],\n    'sex_738L':['max'],\n    'type_25L':['max']\n    \n\n    \n    \n    \n}\nPERSON2_AGG={\n    'num_group1':['count'],\n    'num_group2':['count'],\n    #'addres_district_368M'\n    #'addres_role_871L'\n    #'addres_zip_823M'\n    #'conts_role_79M'\n    'empls_economicalst_849M':['max'],\n    #'empls_employedfrom_796D'\n    #'empls_employer_name_740M'\n    #'relatedpersons_role_762T'\n}\nOTHER_AGG={\n    'num_group1':['count'],\n    'amtdebitincoming_4809443A':['max','mean','min', 'count','sum'],\n    'amtdebitoutgoing_4809440A':['max','mean','min', 'count','sum'],\n    #'amtdepositbalance_4809441A'\n    #'amtdepositincoming_4809444A'\n    #'amtdepositoutgoing_4809442A'\n}\nDEBITCARD_AGG={\n    'num_group1':['count'],\n    #'last180dayaveragebalance_704A'\n    #'last180dayturnover_1134A'\n    #'last30dayturnover_651A'\n    'openingdate_857D':['min','max','mean']\n}\nTAX_REGISTRY_A_AGG={\n    'num_group1':['count'],\n    'amount_4527230A': ['max','mean','min','sum'],\n    'name_4527232M':['max'],\n    'recorddate_4527225D':['max','mean','min']\n    \n}\nTAX_REGISTRY_B_AGG={\n    'num_group1':['count'],\n    'amount_4917619A':['min','mean','max','sum'],\n    'deductiondate_4917603D':['max','mean','min'],\n    'name_4917606M':['max'],\n    \n    \n}\nTAX_REGISTRY_C_AGG={\n    'num_group1':['count'],\n    'employername_160M':['max'],\n    'pmtamount_36A':['min','mean','max','sum'],\n    'processingdate_168D':['mean','min','max'],\n\n}\nCREDIT_BUREAU_A_1_AGG={\n    \n    'num_group1':['count'],\n    #'annualeffectiverate_199L'\n    #'annualeffectiverate_63L'\n    'classificationofcontr_13M':['max'],\n    'classificationofcontr_400M':['max'],\n    'contractst_545M':['max'],\n    'contractst_964M':['max'],\n    #'contractsum_5085717L'\n    #'credlmt_230A'\n    'credlmt_935A':['max'],\n    'dateofcredend_289D':['mean','min','max'],\n    'dateofcredend_353D':['mean','min','max'],\n    'dateofcredstart_181D':['mean','min','max'],\n    'dateofcredstart_739D':['mean','min','max'],\n    'dateofrealrepmt_138D':['mean','min','max'],\n    'debtoutstand_525A':['min','mean','max','sum'],\n    'debtoverdue_47A':['min','mean','max','sum'],\n    'description_351M':['max'],\n    'dpdmax_139P':['min','mean','max','sum'],\n    #'dpdmax_757P'\n    #'dpdmaxdatemonth_442T':['max'],\n    #'dpdmaxdatemonth_89T':['max'],\n    #'dpdmaxdateyear_596T'\n    #'dpdmaxdateyear_896T'\n    'financialinstitution_382M':['max'],\n    'financialinstitution_591M':['max'],\n    'instlamount_768A':['min','mean','max','sum'],\n    #'instlamount_852A'\n    #'interestrate_508L'\n    'lastupdate_1112D':['mean','min','max'],\n    'lastupdate_388D':['mean','min','max'],\n    'monthlyinstlamount_332A':['min','mean','max','sum'],\n    #'monthlyinstlamount_674A'\n    'nominalrate_281L':['mean','min','max'],\n    #'nominalrate_498L'\n    'numberofcontrsvalue_258L':['min','mean','max','sum'],\n    'numberofcontrsvalue_358L':['min','mean','max','sum'],\n    #'numberofinstls_229L':\n    'numberofinstls_320L':['min','mean','max','sum'],\n    #'numberofoutstandinstls_520L'\n    'numberofoutstandinstls_59L':['min','mean','max','sum'],\n    'numberofoverdueinstlmax_1039L':['min','mean','max','sum'],\n    #'numberofoverdueinstlmax_1151L'\n    #'numberofoverdueinstlmaxdat_148D'\n    'numberofoverdueinstlmaxdat_641D':['mean','min','max'],\n    \n    \n      \n    'overdueamountmaxdatemonth_284T': ['min', 'mean', 'max'],\n    'overdueamountmaxdatemonth_365T': ['min', 'mean', 'max'],\n    'overdueamountmaxdateyear_2T': ['min', 'mean', 'max'],\n    'overdueamountmaxdateyear_994T': ['min', 'mean', 'max'],\n    'periodicityofpmts_1102L': ['min', 'mean', 'max'],\n    'periodicityofpmts_837L': ['min', 'mean', 'max'],\n    'prolongationcount_1120L': ['min', 'mean', 'max'],\n    'prolongationcount_599L': ['min', 'mean', 'max'],\n    'purposeofcred_426M': ['min', 'mean', 'max'],\n    'purposeofcred_874M': ['min', 'mean', 'max'],\n    'refreshdate_3813885D': ['min', 'mean', 'max'],\n    'residualamount_488A': ['min', 'mean', 'max'],\n    'residualamount_856A': ['min', 'mean', 'max'],\n   \n    'totalamount_6A': ['min', 'mean', 'max'],\n    'totalamount_996A': ['min', 'mean', 'max'],\n    'totaldebtoverduevalue_178A': ['min', 'mean', 'max'],\n    'totaldebtoverduevalue_718A': ['min', 'mean', 'max'],\n    'totaloutstanddebtvalue_39A': ['min', 'mean', 'max'],\n    'totaloutstanddebtvalue_668A': ['min', 'mean', 'max']\n   \n}\nCREDIT_BUREAU_B_1_AGG={\n    'num_group1':['count'],\n    \n}\nCREDIT_BUREAU_A_2_AGG={\n \n   \n    'pmts_dpd_1073P': ['min', 'mean', 'max'],\n    'pmts_dpd_303P': ['min', 'mean', 'max'],\n    'pmts_month_158T': ['min', 'mean', 'max'],\n    'pmts_month_706T': ['min', 'mean', 'max'],\n    'pmts_overdue_1140A': ['min', 'mean', 'max'],\n    'pmts_overdue_1152A': ['min', 'mean', 'max'],\n    'pmts_year_1139T': ['min', 'mean', 'max'],\n    'pmts_year_507T': ['min', 'mean', 'max'],\n    'subjectroles_name_541M': ['min', 'mean', 'max'],\n    'subjectroles_name_838M': ['min', 'mean', 'max'],\n    \n    \n    'num_group1':['count'],\n    'num_group2':['count']\n}\nCREDIT_BUREAU_B_2_AGG={\n    'num_group1':['count'],\n    'num_group2':['count'],\n    'pmts_date_1107D':['min', 'mean', 'max'],\n    'pmts_dpdvalue_108P':['min','mean','max'],\n    'pmts_pmtsoverdue_635A':['min','mean','max'],\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.510613Z","iopub.execute_input":"2024-04-01T17:51:58.511001Z","iopub.status.idle":"2024-04-01T17:51:58.667279Z","shell.execute_reply.started":"2024-04-01T17:51:58.510971Z","shell.execute_reply":"2024-04-01T17:51:58.665901Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **MAIN FUNCTION**\n<a id='main_function'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"code","source":"def main(debug= False):\n    num_rows = 11111 if debug else None\n    print(\"Notebook started:\")\n    if not IMPORT_DATAFRAME:\n    \n        with timer(\"base\"):\n\n            df = get_base(DATA_DIRECTORY, num_rows=num_rows)\n\n            print(\"base dataframe shape:\", df.shape)\n\n\n\n        with timer(\"static\"):\n\n            df_static = get_static(DATA_DIRECTORY, num_rows=num_rows)\n            df_static = df_static.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_static, on='case_id', how='left', suffix='_static')\n            print(\"static dataframe shape:\", df_static.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n\n            del df_static\n            gc.collect()\n\n        with timer(\"static_cb\"):\n\n            df_static_cb = get_static_cb(DATA_DIRECTORY, num_rows=num_rows)\n            df_static_cb = df_static_cb.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_static_cb, on='case_id', how='left', suffix='_static_cb')\n            print(\"static cb dataframe shape:\", df_static_cb.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_static_cb\n            gc.collect()\n\n        with timer(\"Previous applications depth 1 test\"):\n\n            df_applprev1 = get_applprev1(DATA_DIRECTORY, num_rows=num_rows)\n            df_applprev1 = df_applprev1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n            print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_applprev1\n            gc.collect()\n\n        with timer(\"Previous applications depth 2 test\"):\n\n            df_applprev2 = get_applprev2(DATA_DIRECTORY, num_rows=num_rows)\n            df_applprev2 = df_applprev2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_applprev2, on='case_id', how='left', suffix='_applprev2')\n            print(\"Previous applications depth 2 test dataframe shape:\", df_applprev2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_applprev2\n            gc.collect()\n\n        with timer(\"Person depth 1 test\"):\n\n            df_person1 = get_person1(DATA_DIRECTORY, num_rows=num_rows)\n            df_person1 = df_person1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_person1, on='case_id', how='left', suffix='_person1')\n            print(\"Person depth 1 test dataframe shape:\", df_person1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_person1\n            gc.collect()\n\n        with timer(\"Person depth 2 test\"):\n\n            df_person2 = get_person2(DATA_DIRECTORY, num_rows=num_rows)\n            df_person2 = df_person2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_person2, on='case_id', how='left', suffix='_person2')\n            print(\"Person depth 2 test dataframe shape:\", df_person2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_person2\n            gc.collect()\n\n        with timer(\"Other test\"):\n\n            df_other = get_other(DATA_DIRECTORY, num_rows=num_rows)\n            df_other = df_other.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_other, on='case_id', how='left', suffix='_other')\n            print(\"Other test dataframe shape:\", df_other.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_other\n            gc.collect()\n\n        with timer(\"Debit card test\"):\n\n            df_debitcard = get_debitcard(DATA_DIRECTORY, num_rows=num_rows)\n            df_debitcard = df_debitcard.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_debitcard, on='case_id', how='left', suffix='_debitcard')\n            print(\"Debit card test dataframe shape:\", df_debitcard.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_debitcard\n            gc.collect()\n\n        with timer(\"Tax registry a test\"):\n\n            df_tax_registry_a = get_tax_registry_a(DATA_DIRECTORY, num_rows=num_rows)\n            df_tax_registry_a = df_tax_registry_a.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_tax_registry_a, on='case_id', how='left', suffix='_tax_registry_a')\n            print(\"Tax registry a test dataframe shape:\", df_tax_registry_a.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_tax_registry_a\n            gc.collect()\n\n        with timer(\"Tax registry b test\"):\n\n            df_tax_registry_b = get_tax_registry_b(DATA_DIRECTORY, num_rows=num_rows)\n            df_tax_registry_b = df_tax_registry_b.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_tax_registry_b, on='case_id', how='left', suffix='_tax_registry_b')\n            print(\"Tax registry b test dataframe shape:\", df_tax_registry_b.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_tax_registry_b\n            gc.collect()\n\n        with timer(\"Tax registry c test\"):\n\n            df_tax_registry_c = get_tax_registry_c(DATA_DIRECTORY, num_rows=num_rows)\n            df_tax_registry_c = df_tax_registry_c.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_tax_registry_c, on='case_id', how='left', suffix='_tax_registry_c')\n            print(\"Tax registry c test dataframe shape:\", df_tax_registry_c.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_tax_registry_c\n            gc.collect()\n\n\n\n        with timer(\"Credit bureau a 1 test\"):\n\n            df_credit_bureau_a_1 = get_credit_bureau_a_1(DATA_DIRECTORY, num_rows=num_rows)\n            df_credit_bureau_a_1 = df_credit_bureau_a_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_credit_bureau_a_1, on='case_id', how='left', suffix='_cb_a_1')\n            print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_credit_bureau_a_1\n            gc.collect()\n        with timer(\"Credit bureau b 1 test\"):\n\n            df_credit_bureau_b_1 = get_credit_bureau_b_1(DATA_DIRECTORY, num_rows=num_rows)\n            df_credit_bureau_b_1 = df_credit_bureau_b_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_credit_bureau_b_1, on='case_id', how='left', suffix='_cb_b_1')\n            print(\"Credit bureau b 1 test dataframe shape:\", df_credit_bureau_b_1.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_credit_bureau_b_1\n            gc.collect()\n\n\n\n\n        with timer(\"Credit bureau a 2 test\"):\n\n            df_credit_bureau_a_2 = get_credit_bureau_a_2(DATA_DIRECTORY, num_rows=num_rows)\n            df_credit_bureau_a_2 = df_credit_bureau_a_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_credit_bureau_a_2, on='case_id', how='left', suffix='_cb_a_2')\n            print(\"Credit bureau a 2 test dataframe shape:\", df_credit_bureau_a_2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_credit_bureau_a_2\n            gc.collect()\n\n        with timer(\"Credit bureau b 2 test\"):\n\n            df_credit_bureau_b_2 = get_credit_bureau_b_2(DATA_DIRECTORY, num_rows=num_rows)\n            df_credit_bureau_b_2 = df_credit_bureau_b_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n            df = df.join(df_credit_bureau_b_2, on='case_id', how='left', suffix='_cb_b_2')\n            print(\"Credit bureau b 2 test dataframe shape:\", df_credit_bureau_b_2.shape)\n            print(\"DATAFRAME shape:\", df.shape)\n            del df_credit_bureau_b_2\n            gc.collect()\n\n        with timer(\"Feature engineering / preprocessing\"): \n\n            df=feature_engineering(df)\n            get_info(df)\n            df_pandas, cat_cols = to_pandas(df)\n            del df;gc.collect()\n            df=df_pandas\n            df=reduce_mem_usage(df)\n            print(\"DATAFRAME shape:\", df.shape)\n    else:\n        with timer(\"Importing processed dataframe\"):\n            \n            \n            df = pd.read_parquet(\"/kaggle/input/home-credit-2024-additional-dataset/processed_debug.parquet\")\n\n         \n            for col in df.select_dtypes(exclude=['number']).columns:\n                df[col] = df[col].astype('category')\n            print(df.dtypes.value_counts())\n            #df=reduce_mem_usage(df)\n           \n            print(\"DATAFRAME shape:\", df.shape)\n    \n    if EXPORT_DATAFRAME:\n        with timer(\"Export dataframe\"):\n            df.to_parquet(\"/kaggle/working/processed_debug.parquet\", index=False)\n            \n            \n            print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n            return\n    \n    if(SELECTKBEST):\n        with timer(\"SelectKBest feature research\"):\n            \n            selectkbestX(df)\n            print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n            return\n\n    with timer(\"Model training\"):\n       \n        \n        del_features = ['target', 'case_id','WEEK_NUM']\n        predictors = list(filter(lambda v: v not in del_features, df.columns))\n        cat_cols = list(df.select_dtypes(\"object\").columns)\n        model = kfold_lightgbm_sklearn(df, cat_cols)\n       \n        \n\n    \n    \n    with timer(\"Feature importance assesment\"):\n        \n        get_features_importances(predictors, model)\n        \n        \n    \n        \n    with timer(\"Submission\"):\n\n        if GENERATE_SUBMISSION_FILES:\n            \n            if generate_submission_file(df, model):\n\n\n                print(\"Submission file has been created.\")\n            \n    \n    print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n    \n    return df, model\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.669683Z","iopub.execute_input":"2024-04-01T17:51:58.670112Z","iopub.status.idle":"2024-04-01T17:51:58.719487Z","shell.execute_reply.started":"2024-04-01T17:51:58.670067Z","shell.execute_reply":"2024-04-01T17:51:58.718492Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY FUNCTIONS**","metadata":{}},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"class Pipeline:\n    @staticmethod\n    \n    \n    # Sets datatypes accordingly\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))            \n\n        return df\n    \n    \n    # Changes the values of all date columns. The result will not be a date but number of days since date_decision.\n    @staticmethod\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                \n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n                df = df.with_columns(pl.col(col).dt.total_days())\n                \n        df = df.drop(\"date_decision\", \"MONTH\")\n\n        return df\n    \n    # It drops columns with a lot of NaN values.\n    @staticmethod\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n\n                if isnull > 0.95:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.720760Z","iopub.execute_input":"2024-04-01T17:51:58.721152Z","iopub.status.idle":"2024-04-01T17:51:58.736848Z","shell.execute_reply.started":"2024-04-01T17:51:58.721122Z","shell.execute_reply":"2024-04-01T17:51:58.735551Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_info(dataframe):\n    \"\"\"\n    View data types, shape, and calculate the percentage of NaN (missing) values in each column\n    of a Polars DataFrame simultaneously.\n    \n    Parameters:\n    dataframe (polars.DataFrame): The DataFrame to analyze.\n    \n    Returns:\n    None\n    \"\"\"\n    # Print DataFrame shape\n    print(\"DataFrame Shape:\", dataframe.shape)\n    print(\"-\" * 60)\n    \n    # Print column information\n    print(\"{:<50} {:<30} {:<20}\".format(\"Column Name\", \"Data Type\", \"NaN Percentage\"))\n    print(\"-\" * 60)\n    \n    # Total number of rows in the DataFrame\n    total_rows = len(dataframe)\n    \n    # Iterate over each column\n    for column in dataframe.columns:\n        # Get the data type of the column\n        dtype = str(dataframe[column].dtype)\n        \n        # Count the number of NaN values in the column\n        nan_count = dataframe[column].null_count()\n        \n        # Calculate the percentage of NaN values\n        nan_percentage = (nan_count / total_rows) * 100\n        \n        # Print the information\n        print(\"{:<50} {:<30} {:.2f}%\".format(column, dtype, nan_percentage))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.738242Z","iopub.execute_input":"2024-04-01T17:51:58.739512Z","iopub.status.idle":"2024-04-01T17:51:58.753672Z","shell.execute_reply.started":"2024-04-01T17:51:58.739477Z","shell.execute_reply":"2024-04-01T17:51:58.752438Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    \n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    \n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    \n    return df_data, cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.755379Z","iopub.execute_input":"2024-04-01T17:51:58.755764Z","iopub.status.idle":"2024-04-01T17:51:58.767445Z","shell.execute_reply.started":"2024-04-01T17:51:58.755734Z","shell.execute_reply":"2024-04-01T17:51:58.766362Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.769095Z","iopub.execute_input":"2024-04-01T17:51:58.769620Z","iopub.status.idle":"2024-04-01T17:51:58.784777Z","shell.execute_reply.started":"2024-04-01T17:51:58.769565Z","shell.execute_reply":"2024-04-01T17:51:58.783786Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.786609Z","iopub.execute_input":"2024-04-01T17:51:58.787351Z","iopub.status.idle":"2024-04-01T17:51:58.800930Z","shell.execute_reply.started":"2024-04-01T17:51:58.787311Z","shell.execute_reply":"2024-04-01T17:51:58.799817Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    \n\n    temp=base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\").mean()\n   \n    week_nums_to_drop = temp[(temp[\"target\"] == 0) | (temp[\"target\"] == 1)].index.tolist()\n\n    base_filtered = base[~base[\"WEEK_NUM\"].isin(week_nums_to_drop)]\n\n    # Apply the aggregator\n    gini_in_time = base_filtered.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]] \\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n\n    \n\n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a * x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.nanmean(gini_in_time)  # Use np.nanmean to handle NaN values\n    \n    if SHOW_REPORT:\n        # Display the plot of x on y\n        plt.figure(figsize=(8, 6))\n        plt.plot(x, y, 'o', label='Gini in Time')\n        plt.plot(x, y_hat, '-', label='Fitted line (slope={:.2f}, intercept={:.2f})'.format(a, b))\n        plt.xlabel('Week')\n        plt.ylabel('Gini in Time')\n        plt.title('Gini Stability Over Time')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    \n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.805176Z","iopub.execute_input":"2024-04-01T17:51:58.806295Z","iopub.status.idle":"2024-04-01T17:51:58.819287Z","shell.execute_reply.started":"2024-04-01T17:51:58.806260Z","shell.execute_reply":"2024-04-01T17:51:58.818248Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Report function","metadata":{}},{"cell_type":"code","source":"'''\ndef make_report(num_rows, predictors, model):\n    # 1. time\n    current_time = datetime.now()\n    # Print the current time\n    print(\"Current Time:\", current_time)\n    \n    # 2. specification\n    if not num_rows:\n        print(\"The notebook was run in full mode.\")\n    else:\n        print(\"The notebook was run in debug mode. Number of rows: \" + str(num_rows))\n    \n    # 3. features\n    feat_importances_df = model.get_features_importances_df(predictors)\n    feat_importances_df['gain'] = feat_importances_df['gain'].round(0)\n    print(feat_importances_df.shape)\n    \n    predictions = pd.Series(model.get_predictions())\n   \n    numerical_columns = data.select_dtypes(include=['int', 'float']).columns\n\n    # Compute correlations of each numerical column with 'PREDICTIONS'\n    correlations = {}\n    \n    # Compute correlations of each numerical column with 'feat'\n    for column in numerical_columns:\n        correlations[column] = predictions.corr(data[column])\n\n    # Create a new DataFrame with 'features' and 'correlation' columns\n    correlation_df = pd.DataFrame(list(correlations.items()), columns=['features', 'correlation'])\n\n    # Round the correlation numbers to three decimal places\n    correlation_df['correlation'] = correlation_df['correlation'].round(3)\n\n    # Merge feat_importances_df and correlation_df on 'feature'\n    combined_df = pd.merge(feat_importances_df, correlation_df, left_on=\"feature\", right_on='features', how='left')\n\n    # Handle categorical features with no correlation\n    combined_df['correlation'] = combined_df['correlation'].fillna(value=np.nan)\n    \n\n    # Compute and add valid percentage for each feature\n    valid_percentage = (data[0:-10].count() / len(data[0:-10]))\n    valid_percentage = valid_percentage.round(3)\n    combined_df['valid_percentage'] = combined_df['feature'].map(valid_percentage)\n\n    # Print the combined_df DataFrame\n    print(combined_df.to_string(index=False))\n    print()\n    roc_score=roc_auc_score(data['target'][0:-10],predictions)\n    print(\"ROC score: \",roc_score)\n\n    # Compute false positive rate, true positive rate, and thresholds for ROC curve\n    fpr, tpr, thresholds = roc_curve(data['target'][0:-10], predictions)\n\n    # Plot ROC curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_score)\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    plt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.820896Z","iopub.execute_input":"2024-04-01T17:51:58.821347Z","iopub.status.idle":"2024-04-01T17:51:58.841676Z","shell.execute_reply.started":"2024-04-01T17:51:58.821308Z","shell.execute_reply":"2024-04-01T17:51:58.840500Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\ndef make_report(num_rows, predictors, model):\\n    # 1. time\\n    current_time = datetime.now()\\n    # Print the current time\\n    print(\"Current Time:\", current_time)\\n    \\n    # 2. specification\\n    if not num_rows:\\n        print(\"The notebook was run in full mode.\")\\n    else:\\n        print(\"The notebook was run in debug mode. Number of rows: \" + str(num_rows))\\n    \\n    # 3. features\\n    feat_importances_df = model.get_features_importances_df(predictors)\\n    feat_importances_df[\\'gain\\'] = feat_importances_df[\\'gain\\'].round(0)\\n    print(feat_importances_df.shape)\\n    \\n    predictions = pd.Series(model.get_predictions())\\n   \\n    numerical_columns = data.select_dtypes(include=[\\'int\\', \\'float\\']).columns\\n\\n    # Compute correlations of each numerical column with \\'PREDICTIONS\\'\\n    correlations = {}\\n    \\n    # Compute correlations of each numerical column with \\'feat\\'\\n    for column in numerical_columns:\\n        correlations[column] = predictions.corr(data[column])\\n\\n    # Create a new DataFrame with \\'features\\' and \\'correlation\\' columns\\n    correlation_df = pd.DataFrame(list(correlations.items()), columns=[\\'features\\', \\'correlation\\'])\\n\\n    # Round the correlation numbers to three decimal places\\n    correlation_df[\\'correlation\\'] = correlation_df[\\'correlation\\'].round(3)\\n\\n    # Merge feat_importances_df and correlation_df on \\'feature\\'\\n    combined_df = pd.merge(feat_importances_df, correlation_df, left_on=\"feature\", right_on=\\'features\\', how=\\'left\\')\\n\\n    # Handle categorical features with no correlation\\n    combined_df[\\'correlation\\'] = combined_df[\\'correlation\\'].fillna(value=np.nan)\\n    \\n\\n    # Compute and add valid percentage for each feature\\n    valid_percentage = (data[0:-10].count() / len(data[0:-10]))\\n    valid_percentage = valid_percentage.round(3)\\n    combined_df[\\'valid_percentage\\'] = combined_df[\\'feature\\'].map(valid_percentage)\\n\\n    # Print the combined_df DataFrame\\n    print(combined_df.to_string(index=False))\\n    print()\\n    roc_score=roc_auc_score(data[\\'target\\'][0:-10],predictions)\\n    print(\"ROC score: \",roc_score)\\n\\n    # Compute false positive rate, true positive rate, and thresholds for ROC curve\\n    fpr, tpr, thresholds = roc_curve(data[\\'target\\'][0:-10], predictions)\\n\\n    # Plot ROC curve\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(fpr, tpr, color=\\'blue\\', lw=2, label=\\'ROC curve (AUC = %0.2f)\\' % roc_score)\\n    plt.plot([0, 1], [0, 1], color=\\'gray\\', linestyle=\\'--\\')\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel(\\'False Positive Rate\\')\\n    plt.ylabel(\\'True Positive Rate\\')\\n    plt.title(\\'Receiver Operating Characteristic (ROC) Curve\\')\\n    plt.legend(loc=\"lower right\")\\n    plt.grid(True)\\n    plt.show()\\n'"},"metadata":{}}]},{"cell_type":"code","source":"def group(df_to_agg, prefix, aggregations, aggregate_by='case_id', datatype='polars'):\n    # Create a dictionary mapping aggregation functions to their string representations\n    \n    if datatype=='polars':\n        func_mapping = {\n        'min': pl.min,\n        'max': pl.max,\n        'mean': pl.mean,\n        'sum': pl.sum,\n        'count': pl.count,\n         'median': pl.median\n        }\n\n    # Perform the aggregation\n        agg_df = df_to_agg.group_by(aggregate_by).agg(**{\n            f\"{func}_{col}\": func_mapping[func](col) for col, funcs in aggregations.items() for func in funcs\n        })\n        '''\n        # Rename columns\n        for col, funcs in aggregations.items():\n            for func in funcs:\n                old_name = f\"{col}_{func}\"\n                new_name = f\"{prefix}{col}_{func.upper()}\"\n                agg_df = agg_df.select(pl.col(old_name).alias(new_name))\n        '''\n        return agg_df\n    \n    if datatype=='pandas':\n            # Create a dictionary mapping aggregation functions to their string representations\n        func_mapping = {\n            'min': 'min',\n            'max': 'max',\n            'mean': 'mean',\n            'sum': 'sum',\n            'count': 'count',\n            \n        }\n\n        # Perform the aggregation\n        agg_df = df_to_agg.groupby(aggregate_by).agg(**{\n            f\"{prefix}{col}_{func.upper()}\": (col, func_mapping[func]) for col, funcs in aggregations.items() for func in funcs\n        }).reset_index()\n        \n        return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.843718Z","iopub.execute_input":"2024-04-01T17:51:58.844487Z","iopub.status.idle":"2024-04-01T17:51:58.856644Z","shell.execute_reply.started":"2024-04-01T17:51:58.844448Z","shell.execute_reply":"2024-04-01T17:51:58.855557Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# **SELECTKBEST METHOD**","metadata":{}},{"cell_type":"code","source":"def selectkbestX(data):\n    #########################################################################################\n    def preprocessingX(data):\n    \n        \n\n        def one_hot_encode(data):\n            \n            \n            original_columns = list(data.columns)\n            categories = [cat for cat in data.columns if data[cat].dtype == 'category']\n            df = pd.get_dummies(data, columns= categories, dummy_na= True) #one_hot_encode the categorical features\n            categorical_columns = [cat for cat in df.columns if cat not in original_columns]\n            return df, categorical_columns\n        \n        \n        df,categorical_columns=one_hot_encode(data)\n        del data;gc.collect()\n        \n        for column in df.columns:\n            # Calculate the mean value of the column excluding NaNs\n            #column_mean = df[column].mean()\n            # Replace NaN values in the column with the mean value\n            df[column]=df[column].fillna(0)\n\n       \n\n        return df\n    #########################################################################################\n    def selectkbest_base(X_train, y_train):\n        \n        # Define SelectKBest with desired parameters\n        k = 500  # Number of top features to select\n        S = SelectKBest(score_func=f_classif, k=k)\n\n        # Fit SelectKBest on training data and transform features\n        X_train_k_best = S.fit_transform(X_train, y_train)\n\n        # Get scores assigned to each feature\n        feature_scores = S.scores_\n        \n        # Create a DataFrame to store feature names and their scores\n        feature_scores_df = pd.DataFrame({'Feature': X_train.columns, 'Score': feature_scores})\n\n        # Sort DataFrame by scores in descending order\n        #feature_scores_df_sorted = feature_scores_df.sort_values(by='Score', ascending=False)\n\n        # Print the table of top features and their scores\n      \n        # Return DataFrame with feature names and their scores\n        return feature_scores_df\n    #########################################################################################\n    \n    \n    df=preprocessingX(data)\n    del data;gc.collect()\n    \n    \n    \n    \n   \n    N_CHUNKS=5\n    df.drop(df[df['target'].isnull()].index, inplace=True)\n    \n   \n    del_features = ['target', 'case_id']\n    predictors = [col for col in df.columns if col not in del_features]\n    \n    feats_df = pd.DataFrame({'feature': predictors}, columns=['feature'])\n    \n    results=[]\n    \n    with tqdm(total=N_CHUNKS) as pbar:\n        for i in range(N_CHUNKS):\n\n            sub_df = df[df.index % N_CHUNKS == i]\n            df.drop(df.index[df.index % N_CHUNKS == i], inplace=True)\n            X_train=sub_df[predictors]\n            y_train=sub_df['target']\n\n\n            result_df=selectkbest_base(X_train, y_train)\n            \n            del sub_df\n            gc.collect()\n\n            results.append(result_df)\n            pbar.update(1)\n            \n    del df; gc.collect()\n    merged_df = results[0]\n\n# Merge the remaining dataframes horizontally on the 'Feature' column\n    for df_index in range(1, len(results)):\n        suffix = '_' + str(df_index)  # Add a suffix to distinguish overlapping column names\n        merged_df = pd.merge(merged_df, results[df_index], on='Feature', suffixes=('', suffix))\n\n    merged_df.rename(columns={'Score': 'Score_0'}, inplace=True)\n    merged_df['mean_score'] = 0\n    \n    for i in range(N_CHUNKS):\n        merged_df['mean_score']+=merged_df[\"Score_\"+str(i)]\n    \n    \n    final_df=merged_df[['Feature', 'mean_score']]\n    final_df = final_df.sort_values(by='mean_score', ascending=False)\n    pd.set_option('display.max_rows', None)  # Show all rows\n# Display the DataFrame\n    print(final_df)\n   \n\n    final_df.to_csv(\"/kaggle/working/SelectKBest.csv\")\n    \n    return merged_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.858159Z","iopub.execute_input":"2024-04-01T17:51:58.858557Z","iopub.status.idle":"2024-04-01T17:51:58.879917Z","shell.execute_reply.started":"2024-04-01T17:51:58.858488Z","shell.execute_reply":"2024-04-01T17:51:58.879001Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#  **MODEL** <a id='model'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n        # Use tqdm to create a progress bar during the prediction\n        with tqdm(total=len(self.estimators), desc=\"Predicting\", unit=\" models\") as pbar:\n            for i, estimator in enumerate(self.estimators):\n                y_preds[i] = estimator.predict_proba(X)\n                print(estimator.predict_proba(X))\n                pbar.update(1)  # Update the progress bar\n        return np.mean(y_preds, axis=0)\n\n    \n    def get_splits(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for x in self.estimators:\n            feature_importances_list.append(x.booster_.feature_importance(importance_type='split'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None   \n        return combined_importances\n    \n    \n    def get_gains(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for model in self.estimators:\n            feature_importances_list.append(x.booster_.feature_importance(importance_type='gain'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None\n              \n        return combined_importances\n    \n    def get_features_importances_df(self, predictors):\n        \n        \n        importance_df = pd.DataFrame()\n        eval_results = dict()\n        for model in self.estimators:\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = predictors\n            fold_importance[\"gain\"] = model.booster_.feature_importance(importance_type='gain')\n            fold_importance[\"split\"] = model.booster_.feature_importance(importance_type='split')\n            importance_df = pd.concat([importance_df, fold_importance], axis=0)\n            importance_df= importance_df.groupby('feature').mean().reset_index()\n        return importance_df\n    \n    \n    def add_predictions(self, predictions):\n        self.predictions=predictions\n        \n    def get_predictions(self):\n        return self.predictions\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.881198Z","iopub.execute_input":"2024-04-01T17:51:58.882192Z","iopub.status.idle":"2024-04-01T17:51:58.900475Z","shell.execute_reply.started":"2024-04-01T17:51:58.882162Z","shell.execute_reply":"2024-04-01T17:51:58.899555Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def kfold_lightgbm_sklearn(data, categorical_feature = None):\n    \n    \n    \n   \n    #time.sleep(30)\n    start_time = time.time()\n    \n    df=data.copy()\n    df.drop(df[df['target'].isnull()].index, inplace=True)\n    #test=data.copy()\n    #test.drop(test[test['target'].notnull()].index, inplace=True)\n    del data; gc.collect()\n    \n    df=reduce_mem_usage(df)\n    #test=reduce_mem_usage(test)\n    \n  \n    #time.sleep(30)\n    #print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n    print(\"Train/valid shape: {}, \".format(df.shape))\n    del_features = ['target', 'case_id', 'WEEK_NUM']\n    predictors = list(filter(lambda v: v not in del_features, df.columns))\n\n    if not STRATIFIED_KFOLD:\n        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    else:\n        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    \n        # Hold oof predictions, test predictions, feature importance and training/valid auc\n    oof_preds = np.zeros(df.shape[0])\n    \n    importance_df = pd.DataFrame()\n    eval_results = dict()\n    \n    fitted_models = []\n    with tqdm(total=NUM_FOLDS) as pbar:\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):\n           \n          \n            train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]\n            valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]\n            \n            \n        \n        \n            #time.sleep(30)\n            params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n            clf = lgb.LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n\n\n            if not categorical_feature:\n                    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n                            callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)]\n                           )\n            else:\n                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n                        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)],\n                        feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n\n\n            fitted_models.append(clf)\n\n            if EVALUATE_VALIDATION_SET:\n                oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n\n\n\n                # Feature importance by GAIN and SPLIT\n\n            eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n            eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n\n            elapsed_time = time.time() - start_time\n            remaining_time = elapsed_time * (NUM_FOLDS - n_fold - 1) / (n_fold + 1)\n            print('Fold %2d AUC : %.6f. Elapsed time: %.2f seconds. Remaining time: %.2f seconds.'\n                  % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx]), elapsed_time, remaining_time))\n            del clf, train_x, train_y, valid_x, valid_y\n            gc.collect()\n            pbar.update(1)\n            \n    print('Full AUC score %.6f' % roc_auc_score(df['target'], oof_preds))\n    # Get the average feature importance between folds\n    \n    \n    \n    if len(df)>0:\n        base=get_base(DATA_DIRECTORY, len(df))\n        base, cat_cols = to_pandas(base)\n        base=base[base['target'].notnull()]\n        base['score']= oof_preds\n        gini_score = gini_stability(base)\n        print(\"Gini Score of the valid set:\", gini_score)\n    \n    \n    \n    \n    # Save feature importance, test predictions and oof predictions as csv\n    \n        \n        \n  \n        \n        \n    model = VotingModel(fitted_models)\n    if GENERATE_SUBMISSION_FILES:\n        \n\n\n            # Generate oof csv\n            oof = pd.DataFrame()\n            oof['case_id'] = df['case_id'].copy()\n            df['PREDICTIONS'] = oof_preds.copy()\n            df['target'] = df['target'].copy()\n            df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    model.add_predictions(oof_preds.copy())\n    del df; gc.collect()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.901916Z","iopub.execute_input":"2024-04-01T17:51:58.902534Z","iopub.status.idle":"2024-04-01T17:51:58.926697Z","shell.execute_reply.started":"2024-04-01T17:51:58.902489Z","shell.execute_reply":"2024-04-01T17:51:58.925558Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **SUBMISSION**","metadata":{}},{"cell_type":"code","source":"def generate_submission_file(data, model):\n    test=data.copy()\n    test.drop(test[test['target'].notnull()].index, inplace=True)\n    del data;gc.collect()\n\n    '''\n    length=len(test)\n    y_pred = pd.Series([0.5] * length,index=test['case_id'])\n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n    df_subm.to_csv(\"submission.csv\")\n    '''\n\n    del_features = ['target', 'case_id','WEEK_NUM']\n    predictors = list(filter(lambda v: v not in del_features, test.columns))\n    y_pred = pd.Series(model.predict_proba(test[predictors])[:, 1], index=test['case_id']) \n\n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n\n    df_subm.to_csv(\"submission.csv\")\n    \n    return True\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.928533Z","iopub.execute_input":"2024-04-01T17:51:58.929728Z","iopub.status.idle":"2024-04-01T17:51:58.942864Z","shell.execute_reply.started":"2024-04-01T17:51:58.929684Z","shell.execute_reply":"2024-04-01T17:51:58.941958Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **EVALUATE FEATURES IMPORTANCES**","metadata":{}},{"cell_type":"code","source":"def get_features_importances(predictors, model):\n    importance_df = model.get_features_importances_df(predictors)\n    mean_importance = importance_df.groupby('feature').mean().reset_index()\n    mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n    mean_importance.to_csv('feature_importance_gain{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    mean_importance.sort_values(by= 'split', ascending=False, inplace=True)\n    mean_importance.to_csv('feature_importance_split{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.944221Z","iopub.execute_input":"2024-04-01T17:51:58.944580Z","iopub.status.idle":"2024-04-01T17:51:58.954797Z","shell.execute_reply.started":"2024-04-01T17:51:58.944549Z","shell.execute_reply":"2024-04-01T17:51:58.953939Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **FEATURE ENGINEERING FUNCTION**","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    \n    \n    \n    df=df.pipe(Pipeline.handle_dates) \n    #df=df.pipe(Pipeline.filter_cols)\n    \n\n    \n    \n    \n    columns_to_add = [\n        (pl.col(\"days30_165L\")/ pl.col(\"days360_512L\")).alias(\"ratio_queries_30\"),\n        ((pl.col(\"days90_310L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_90\")),\n        ((pl.col(\"days120_123L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_120\")),\n        ((pl.col(\"days180_256L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_180\")),\n        \n        \n        ((pl.col(\"credamount_770A\") / pl.col(\"max_mainoccupationinc_437A\")).alias(\"CREDIT_INCOME_PERCENT\")),\n        ((pl.col(\"annuity_780A\") / pl.col(\"max_mainoccupationinc_437A\")).alias(\"ANNUITY_INCOME_PERCENT\")),\n        ((pl.col(\"credamount_770A\") / pl.col(\"annuity_780A\")).alias(\"CREDIT_ANNUITY_PERCENT\")),\n        \n        ((pl.col(\"annuity_780A\") / pl.col(\"credamount_770A\")).alias(\"CREDIT_TERM\")),\n        ((pl.col(\"max_mainoccupationinc_437A\") / pl.col(\"max_childnum_21L\")).alias(\"CHILDREN_CNT_INCOME_PERCENT\")),\n        \n        #data['ANNUITY_LENGTH_EMPLOYED_PERCENT'] = data['CREDIT_TERM']/ data['DAYS_EMPLOYED']\n        ((pl.col(\"CREDIT_TERM\") / pl.col(\"max_empl_employedfrom_271D\")).alias(\"ANNUITY_LENGTH_EMPLOYED_PERCENT\")),\n    ]\n        #data['PHONE_CHANGE_EMP_PERCENT'] = data['DAYS_LAST_PHONE_CHANGE']/data['DAYS_EMPLOYED']\n        \n    ''' \n        \n        #((pl.col(\"credamount_590A\") / pl.col(\"byoccupationinc_3656910L\")).alias(\"credit_income_percent\")),\n        \n        \n        \n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"collater_typofvalofguarant_407M\")).alias(\"sum_collater\")),\n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_active\")),\n        #((pl.col(\"collater_typofvalofguarant_407M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_close\")),\n        #((pl.col(\"overdueamount_31A\") + pl.col(\"overdueamount_659A\")).alias(\"sum_overdue_amount\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"totalamount\")).alias(\"sum_totalcredit_contract\")),\n        #((pl.col(\"totalamount_503A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_active\")),\n        #((pl.col(\"totalamount_6A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_close\")),\n        #((pl.col(\"totaldebtoverduevalue_178A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_active\")),\n        #((pl.col(\"totaldebtoverduevalue_718A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_close\")),\n        #((pl.col(\"numberofinstls_229L\") + pl.col(\"numberofinstls_320L\")).alias(\"sum_instalments\")),\n        #((pl.col(\"numberofinstls_320L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_active\")),\n        #((pl.col(\"numberofinstls_229L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_close\"))\n    \n    '''\n# Add the calculated columns to the DataFrame\n    \n    \n    for column in columns_to_add:\n        df = df.with_columns([column])\n        \n    new_cols=[\"ratio_queries_30\",\"ratio_queries_90\",\"ratio_queries_120\",\"ratio_queries_180\",\"CREDIT_INCOME_PERCENT\",\n                       \"ANNUITY_INCOME_PERCENT\",\"CREDIT_ANNUITY_PERCENT\",\"CREDIT_TERM\",\"CHILDREN_CNT_INCOME_PERCENT\",\n                        \"ANNUITY_LENGTH_EMPLOYED_PERCENT\"]\n    for column_name in new_cols:\n        if column_name in df.columns:\n            df=df.with_columns(\n            pl.when(pl.col(column_name).is_infinite())\n            .then(None)\n            .otherwise(pl.col(column_name))\n            .keep_name()\n        )\n        \n        \n\n    \n    df=df.pipe(Pipeline.filter_cols)\n    \n    columns_to_drop=[\n     \n   'min_pmts_year_1139T',              \n'mean_pmts_year_1139T',                            \n'max_pmts_year_1139T',                             \n'min_pmts_year_507T' ,                               \n'mean_pmts_year_507T',                              \n'max_pmts_year_507T',\n        \n        'min_overdueamountmaxdateyear_2T'  ,               \n'mean_overdueamountmaxdateyear_2T'   ,              \n'max_overdueamountmaxdateyear_2T'  ,              \n'min_overdueamountmaxdateyear_994T'  ,              \n'mean_overdueamountmaxdateyear_994T' ,             \n'max_overdueamountmaxdateyear_994T'\n]\n    columns_to_drop_existing = [col for col in columns_to_drop if col in df.columns]\n\n    df=df.drop(columns_to_drop_existing)\n    \n    \n    \n    features400=[\n            #0\n        'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n        \n        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n        \n         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n        'pmtscount_423L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n        'numinsttopaygr_769L','sum_credacc_actualbalance_314A','numinstunpaidmax_3546851L','max_empl_employedtotal_800L','price_1097A',\n        'ratio_queries_30','max_postype_4733339M','mean_creationdate_885D','maxdebt4_972A',\n        'mean_amount_4527230A','thirdquarter_1082L','sum_mainoccupationinc_437A','sum_pmtamount_36A','maxdpdinstldate_3546855D',\n        'min_purposeofcred_874M','max_description_351M','description_5085714M','min_dtlastpmt_581D',\n        'totaldebt_9A','max_contractst_964M','currdebt_22A','min_firstnonzeroinstldate_307D',\n        \n         'max_contractst_545M','max_purposeofcred_426M','max_pmts_dpd_303P',\n        'max_classificationofcontr_13M','max_financialinstitution_591M','max_classificationofcontr_400M','max_financialinstitution_382M','count_num_group1',\n        'eir_270L','interestrate_311L','min_purposeofcred_426M','maxdbddpdlast1m_3658939P',\n        'sum_revolvingaccount_394A','max_firstnonzeroinstldate_307D','min_dateactivated_425D','firstdatedue_489D','min_approvaldate_319D',\n        'min_refreshdate_3813885D','totalsettled_863A','count_persontype_1072L','max_credacc_cards_status_52L','min_creationdate_885D',\n        'max_pmtamount_36A','min_pmtnum_8L','min_tenor_203L','count_num_group1_tax_registry_c','max_credacc_status_367L',\n        'sumoutstandtotalest_4493215A','datelastunpaid_3546854D','sum_persontype_1072L','sumoutstandtotal_3546847A',\n        'numinstls_657L','mean_dtlastpmtallstes_3545839D','max_familystate_447L','mean_pmtamount_36A',\n        'max_purposeofcred_874M','maxdpdlast3m_392P','mean_credacc_actualbalance_314A','currdebtcredtyperange_828A',\n        'sum_persontype_792L','lastapplicationdate_877D','max_creationdate_885D','max_outstandingdebt_522A','max_credacc_actualbalance_314A',\n        \n        'max_currdebt_94A','max_pmts_dpd_1073P','mean_credamount_590A','sum_personindex_1023L','min_credacc_actualbalance_314A',\n        'sum_outstandingdebt_522A','min_amount_4527230A','mean_persontype_792L','mean_credacc_credlmt_575A','lastactivateddate_801D',\n        'sum_currdebt_94A','count_personindex_1023L','count_relationshiptoclient_642T','max_dateactivated_425D','max_housetype_905L',\n        'min_currdebt_94A','max_relationshiptoclient_415T','min_lastupdate_388D','min_processingdate_168D',\n        'min_dateofrealrepmt_138D','mean_lastupdate_388D','max_persontype_1072L','max_persontype_792L','min_pmtamount_36A',\n        'lastapprcommoditycat_1041M','pmtaverage_4527227A','annuity_780A','mean_dateofrealrepmt_138D',\n        'sum_annuity_853A','mean_pmts_dpd_1073P','min_credacc_credlmt_575A','lastapprdate_640D','mean_credacc_minhisbal_90A',\n        'min_dateofcredend_353D','mean_dtlastpmt_581D','max_credacc_minhisbal_90A','avgoutstandbalancel6m_4187114A','max_approvaldate_319D',\n        'maxannuity_159A','min_credacc_minhisbal_90A','sum_amount_4917619A','cntincpaycont9m_3716944L','min_dateofcredstart_181D',\n        'mean_refreshdate_3813885D','max_remitter_829L','pmtaverage_3A','avglnamtstart24m_4525187A','education_1103M',\n       'mean_dpdmax_139P','mean_numberofoverdueinstlmax_1039L','min_recorddate_4527225D','min_annuity_853A',\n        'max_dpdmax_139P','sum_dpdmax_139P','lastdelinqdate_224D','mean_persontype_1072L',\n        'count_num_group1_cb_a_2','count_num_group2_cb_a_2','twobodfilling_608L','sum_numberofoverdueinstlmax_1039L','homephncnt_628L',\n        'count_num_group1_tax_registry_b','datefirstoffer_1144D','max_numberofoverdueinstlmax_1039L','min_numberofoverdueinstlmax_1039L',\n        'mean_downpmt_134A','max_empls_economicalst_849M','min_revolvingaccount_394A','responsedate_4527233D',\n        'sum_isbidproduct_390L','max_mainoccupationinc_437A','count_familystate_447L','min_dateofcredstart_739D','max_amount_4917619A',\n        'mean_dateofcredend_353D','min_dpdmax_139P','mean_revolvingaccount_394A','maininc_215A','lastrejectcredamount_222A',\n        'max_processingdate_168D','min_totaldebtoverduevalue_178A','inittransactioncode_186L','max_deductiondate_4917603D',\n        'min_deductiondate_4917603D','mean_processingdate_168D','contractssum_5085716L','mean_dateofcredstart_181D','applicationscnt_867L',\n        'mean_amount_4917619A','max_revolvingaccount_394A','mean_isbidproduct_390L','mean_dateofcredstart_739D','min_pmts_dpd_1073P',\n        \n        'sum_credacc_credlmt_575A','min_pmts_dpd_303P','lastapprcredamount_781A','max_empl_industry_691L',\n        'min_amount_4917619A','mean_annuity_853A',\n       'max_overdueamountmaxdatemonth_365T','max_downpmt_134A','disbursementtype_67L',\n        'min_overdueamountmaxdatemonth_284T','sum_numberofcontrsvalue_358L','count_num_group1_person2','sum_byoccupationinc_3656910L','mean_deductiondate_4917603D',\n        'sellerplacescnt_216L','mean_overdueamountmaxdatemonth_365T','max_overdueamountmaxdatemonth_284T','mean_approvaldate_319D','credtype_322L',\n        'max_numberofcontrsvalue_358L','mean_numberofcontrsvalue_358L','min_downpmt_134A','min_credacc_maxhisbal_375A','mean_isdebitcard_527L',\n        'min_mainoccupationinc_384A','bankacctype_710L','mean_pmts_overdue_1152A','min_numberofcontrsvalue_358L','min_mainoccupationinc_437A',\n        'min_residualamount_856A','mean_byoccupationinc_3656910L','downpmt_116A','isbidproduct_1095L','clientscnt12m_3712952L',\n        'mean_credacc_maxhisbal_375A','max_nominalrate_281L','mean_dateactivated_425D','sum_downpmt_134A','mean_totaldebtoverduevalue_178A',\n        \n         'mean_numberofinstls_320L','max_numberofinstls_320L','max_isdebitcard_527L','mean_nominalrate_281L','dtlastpmtallstes_4499206D',\n        'max_lastupdate_388D','responsedate_4917613D','sum_credacc_minhisbal_90A','max_byoccupationinc_3656910L',\n        'min_credacc_transactions_402L','min_instlamount_768A','inittransactionamount_650A','max_totaldebtoverduevalue_178A','min_isdebitcard_527L',\n        'clientscnt6m_3712949L','mean_credacc_transactions_402L','max_credacc_maxhisbal_375A','min_numberofinstls_320L','mean_totalamount_996A',\n        'validfrom_1069D','count_num_group1_cb_a_1','mean_residualamount_856A','max_debtoverdue_47A',\n        'max_dateofrealrepmt_138D','mean_totalamount_6A','min_pmts_overdue_1152A',\"max_cancelreason_3545846M\",\n    ]\n    \n    features125=[\n        \n        'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n        \n        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n        'pmtscount_423L','max_empl_industry_691L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n        \n    ]\n    \n    features200=[\n         'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n        \n        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n        \n         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n        'pmtscount_423L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n        'numinsttopaygr_769L','sum_credacc_actualbalance_314A','numinstunpaidmax_3546851L','max_empl_employedtotal_800L','price_1097A',\n        'ratio_queries_30','max_postype_4733339M','mean_creationdate_885D','maxdebt4_972A',\n        'mean_amount_4527230A','thirdquarter_1082L','sum_mainoccupationinc_437A','sum_pmtamount_36A','maxdpdinstldate_3546855D',\n        'min_purposeofcred_874M','max_description_351M','description_5085714M','min_dtlastpmt_581D',\n        'totaldebt_9A','max_contractst_964M','currdebt_22A','min_firstnonzeroinstldate_307D',\n        \n         'max_contractst_545M','max_purposeofcred_426M','max_pmts_dpd_303P',\n        'max_classificationofcontr_13M','max_financialinstitution_591M','max_classificationofcontr_400M','max_financialinstitution_382M','count_num_group1',\n        'eir_270L','interestrate_311L','min_purposeofcred_426M','maxdbddpdlast1m_3658939P',\n        'sum_revolvingaccount_394A','max_firstnonzeroinstldate_307D','min_dateactivated_425D','firstdatedue_489D','min_approvaldate_319D',\n        'min_refreshdate_3813885D','totalsettled_863A','count_persontype_1072L','max_credacc_cards_status_52L','min_creationdate_885D',\n        'max_pmtamount_36A','min_pmtnum_8L','min_tenor_203L','count_num_group1_tax_registry_c','max_credacc_status_367L',\n        'sumoutstandtotalest_4493215A','datelastunpaid_3546854D','sum_persontype_1072L','sumoutstandtotal_3546847A',\n        'numinstls_657L','mean_dtlastpmtallstes_3545839D','max_familystate_447L','mean_pmtamount_36A',\n        'max_purposeofcred_874M','maxdpdlast3m_392P','mean_credacc_actualbalance_314A','currdebtcredtyperange_828A',\n        'sum_persontype_792L','lastapplicationdate_877D','max_creationdate_885D','max_outstandingdebt_522A','max_credacc_actualbalance_314A',\n        \n        'max_currdebt_94A','max_pmts_dpd_1073P','mean_credamount_590A','sum_personindex_1023L','min_credacc_actualbalance_314A',\n        'sum_outstandingdebt_522A','min_amount_4527230A','mean_persontype_792L','mean_credacc_credlmt_575A','lastactivateddate_801D',\n        'sum_currdebt_94A','count_personindex_1023L','count_relationshiptoclient_642T','max_dateactivated_425D','max_housetype_905L',\n        'min_currdebt_94A','max_relationshiptoclient_415T','min_lastupdate_388D','min_processingdate_168D',\n        'min_dateofrealrepmt_138D','mean_lastupdate_388D','max_persontype_1072L','max_persontype_792L','min_pmtamount_36A',\n        'lastapprcommoditycat_1041M','pmtaverage_4527227A','annuity_780A','mean_dateofrealrepmt_138D'\n    ]\n    seen = set()\n    duplicates = []\n    for item in features400:\n        if item in seen:\n            duplicates.append(item)\n        else:\n            seen.add(item)\n    print(\"duplicates:\")\n    print(duplicates)\n    print()\n    \n    print(\"Length of features400: \", len(features400))\n    print(\"Length of features200: \", len(features200))\n    print(\"Length of features125: \", len(features125))\n    missing_columns = [col for col in features400 if col not in df.columns]\n\n    # Print missing columns, if any\n    if missing_columns:\n        print(\"The following columns are missing in the DataFrame:\")\n        for col in missing_columns:\n            print(col)\n    else:\n        print(\"All columns from features400 are present in the DataFrame.\")\n    \n    # Columns to preserve\n    preserved_columns = ['target', 'case_id', 'WEEK_NUM']\n\n    # Identify columns to drop excluding the preserved columns\n    columns_to_drop = [col for col in df.columns if col not in preserved_columns + features125]\n\n    # Drop columns that are not in features_selected and not preserved\n    \n    df=df.drop(columns=columns_to_drop)\n    \n    test = df.filter(df['target'].is_null())\n    if BALANCE_COLUMNS and len(test)>10:\n\n        train = df.filter(df['target'].is_not_null())\n        \n\n        valid_percentage_train = []\n        valid_percentage_test=[]\n        for col in df.columns:\n            valid_percentage_train.append(train[col].count())\n            valid_percentage_test.append(test[col].count())\n\n        valid_percentage_train = pd.Series(valid_percentage_train)\n        valid_percentage_test= pd.Series(valid_percentage_test)\n\n        print(train.count())\n        print(test.count())\n        print(\"length of train\",len(valid_percentage_train))\n        print(\"length of test\",len(valid_percentage_test))\n        print(\"df columns\",len(df.columns))\n\n        info_df = pd.DataFrame({'column': df.columns, 'valid_train': valid_percentage_train, 'valid_test': valid_percentage_test})\n        irrelevant_columns = info_df[info_df['valid_test'] < 0.05 ]['column'].to_list()\n        columns_to_drop = [col for col in df.columns if (col not in preserved_columns) and (col in irrelevant_columns) ]\n        df = df.drop(columns=columns_to_drop)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:58.957700Z","iopub.execute_input":"2024-04-01T17:51:58.958069Z","iopub.status.idle":"2024-04-01T17:51:59.027705Z","shell.execute_reply.started":"2024-04-01T17:51:58.958039Z","shell.execute_reply":"2024-04-01T17:51:59.026431Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"'''\ndf = get_base(DATA_DIRECTORY)\ndf_applprev1 = get_applprev1(DATA_DIRECTORY)\ndf_applprev1 = df_applprev1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\ndf = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\nprint(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\nprint(\"DATAFRAME shape:\", df.shape)\ndel df_applprev1\ngc.collect()\n\n\ndf=feature_engineering(df)\nprint(df['credit_income_percent'])\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.029061Z","iopub.execute_input":"2024-04-01T17:51:59.029482Z","iopub.status.idle":"2024-04-01T17:51:59.039444Z","shell.execute_reply.started":"2024-04-01T17:51:59.029446Z","shell.execute_reply":"2024-04-01T17:51:59.038086Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'\\ndf = get_base(DATA_DIRECTORY)\\ndf_applprev1 = get_applprev1(DATA_DIRECTORY)\\ndf_applprev1 = df_applprev1.filter(pl.col(\\'case_id\\').is_in(df[\\'case_id\\'].unique()))\\ndf = df.join(df_applprev1, on=\\'case_id\\', how=\\'left\\', suffix=\\'_applprev1\\')\\nprint(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\\nprint(\"DATAFRAME shape:\", df.shape)\\ndel df_applprev1\\ngc.collect()\\n\\n\\ndf=feature_engineering(df)\\nprint(df[\\'credit_income_percent\\'])\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"# **GET FUNCTIONS**","metadata":{}},{"cell_type":"markdown","source":"### get_base()","metadata":{}},{"cell_type":"code","source":"def get_base(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    train={}\n    test={}\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet'))\n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet')).limit(num_rows) \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_base.parquet'))    \n    length=len(test)\n    nan_series=pl.Series([None] * length)\n    test = test.select(pl.col(\"*\"), nan_series.alias(\"target\"))\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    \n    \n    df = df.with_columns(pl.col('date_decision').cast(pl.Date))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.040996Z","iopub.execute_input":"2024-04-01T17:51:59.041374Z","iopub.status.idle":"2024-04-01T17:51:59.052902Z","shell.execute_reply.started":"2024-04-01T17:51:59.041345Z","shell.execute_reply":"2024-04-01T17:51:59.051580Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### get_static()","metadata":{}},{"cell_type":"code","source":"def get_static(path, num_rows = None):\n# Read the Parquet file using scan() method\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path,low_memory=True).pipe(Pipeline.set_table_dtypes) )\n    train = (pl.concat(chunks, how=\"vertical_relaxed\")).pipe(Pipeline.filter_cols)\n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n        \n        train=df1\n        del df2\n        gc.collect()\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path,low_memory=True).pipe(Pipeline.set_table_dtypes) )\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    \n    \n    columns_to_keep = train.columns\n\n# Find columns in 'test' that are not in 'train'\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n\n# Drop columns from 'test' that are not in 'train'\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.054647Z","iopub.execute_input":"2024-04-01T17:51:59.055197Z","iopub.status.idle":"2024-04-01T17:51:59.066448Z","shell.execute_reply.started":"2024-04-01T17:51:59.055163Z","shell.execute_reply":"2024-04-01T17:51:59.065452Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### get_static_cb()","metadata":{}},{"cell_type":"code","source":"def get_static_cb(path, num_rows = None):\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet'),low_memory=True).limit(num_rows).pipe(Pipeline.set_table_dtypes) \n       \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_static_cb_0.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.067832Z","iopub.execute_input":"2024-04-01T17:51:59.069076Z","iopub.status.idle":"2024-04-01T17:51:59.083311Z","shell.execute_reply.started":"2024-04-01T17:51:59.069039Z","shell.execute_reply":"2024-04-01T17:51:59.082243Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev1(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev1(path, num_rows = None):\n    \n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    train = pl.concat(chunks, how=\"vertical_relaxed\")#.pipe(Pipeline.filter_cols)\n    \n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n\n        train=df1\n        del df2   \n        gc.collect()\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    del test;del train;gc.collect()\n    agg_df = group(df, '', APPLPREV1_AGG)\n    del df;gc.collect()\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.084703Z","iopub.execute_input":"2024-04-01T17:51:59.085766Z","iopub.status.idle":"2024-04-01T17:51:59.096213Z","shell.execute_reply.started":"2024-04-01T17:51:59.085733Z","shell.execute_reply":"2024-04-01T17:51:59.095155Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev2(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev2(path, num_rows = None):\n    train={}\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n       \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', APPLPREV2_AGG)\n    del df ;gc.collect()\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.097394Z","iopub.execute_input":"2024-04-01T17:51:59.097703Z","iopub.status.idle":"2024-04-01T17:51:59.111130Z","shell.execute_reply.started":"2024-04-01T17:51:59.097678Z","shell.execute_reply":"2024-04-01T17:51:59.110306Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### get_person1","metadata":{}},{"cell_type":"code","source":"def get_person1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n      \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', PERSON1_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.112150Z","iopub.execute_input":"2024-04-01T17:51:59.113067Z","iopub.status.idle":"2024-04-01T17:51:59.122235Z","shell.execute_reply.started":"2024-04-01T17:51:59.113002Z","shell.execute_reply":"2024-04-01T17:51:59.121088Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### get_person2","metadata":{}},{"cell_type":"code","source":"def get_person2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes)\n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', PERSON2_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.123737Z","iopub.execute_input":"2024-04-01T17:51:59.124103Z","iopub.status.idle":"2024-04-01T17:51:59.139803Z","shell.execute_reply.started":"2024-04-01T17:51:59.124061Z","shell.execute_reply":"2024-04-01T17:51:59.138605Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### other","metadata":{}},{"cell_type":"code","source":"def get_other(path, num_rows = None):\n     # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n         \n    test = pl.read_parquet(os.path.join(path, 'test/test_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', OTHER_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.141460Z","iopub.execute_input":"2024-04-01T17:51:59.142285Z","iopub.status.idle":"2024-04-01T17:51:59.152570Z","shell.execute_reply.started":"2024-04-01T17:51:59.142251Z","shell.execute_reply":"2024-04-01T17:51:59.151453Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## get_debitcard","metadata":{}},{"cell_type":"code","source":"def get_debitcard(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n      \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', DEBITCARD_AGG)\n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.158405Z","iopub.execute_input":"2024-04-01T17:51:59.158759Z","iopub.status.idle":"2024-04-01T17:51:59.168465Z","shell.execute_reply.started":"2024-04-01T17:51:59.158730Z","shell.execute_reply":"2024-04-01T17:51:59.167360Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_a","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_a(path, num_rows = None):\n    \n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n  \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', TAX_REGISTRY_A_AGG)    \n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.169905Z","iopub.execute_input":"2024-04-01T17:51:59.170243Z","iopub.status.idle":"2024-04-01T17:51:59.180583Z","shell.execute_reply.started":"2024-04-01T17:51:59.170215Z","shell.execute_reply":"2024-04-01T17:51:59.179561Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_b","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_b(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes)\n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', TAX_REGISTRY_B_AGG) \n    del df;gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.181907Z","iopub.execute_input":"2024-04-01T17:51:59.182323Z","iopub.status.idle":"2024-04-01T17:51:59.197579Z","shell.execute_reply.started":"2024-04-01T17:51:59.182291Z","shell.execute_reply":"2024-04-01T17:51:59.196090Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_c","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_c(path, num_rows = None):\n     # Read the Parquet file using scan() method\n# Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test;gc.collect()\n    agg_df = group(df, '', TAX_REGISTRY_C_AGG)    \n    del df; gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.199708Z","iopub.execute_input":"2024-04-01T17:51:59.200201Z","iopub.status.idle":"2024-04-01T17:51:59.210626Z","shell.execute_reply.started":"2024-04-01T17:51:59.200162Z","shell.execute_reply":"2024-04-01T17:51:59.209718Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_1(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df; gc.collect()\n    \n    \n    train_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        train_agg_df.vstack(agg_chunk)\n    train_agg_df.rechunk()\n        \n        \n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df; gc.collect()\n        \n        \n    test_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        test_agg_df.vstack(agg_chunk)\n    test_agg_df.rechunk()\n    \n    \n    \n\n    \n    agg_df=train_agg_df\n    agg_df.extend(test_agg_df)\n    \n\n    \n    \n    print(\"agg df \", agg_df.shape)\n   \n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.211924Z","iopub.execute_input":"2024-04-01T17:51:59.213107Z","iopub.status.idle":"2024-04-01T17:51:59.227487Z","shell.execute_reply.started":"2024-04-01T17:51:59.213065Z","shell.execute_reply":"2024-04-01T17:51:59.226417Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    del train; del test ; gc.collect()\n    agg_df = group(df, '', CREDIT_BUREAU_B_1_AGG) \n    \n    \n    del df; gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.228884Z","iopub.execute_input":"2024-04-01T17:51:59.229347Z","iopub.status.idle":"2024-04-01T17:51:59.243052Z","shell.execute_reply.started":"2024-04-01T17:51:59.229314Z","shell.execute_reply":"2024-04-01T17:51:59.242005Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_2(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df;gc.collect()\n    \n    train_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        train_agg_df.vstack(agg_chunk)\n    train_agg_df.rechunk()\n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df;gc.collect()\n    \n    test_agg_df=agg_chunks[0]\n    for agg_chunk in agg_chunks[1:]:\n        test_agg_df.vstack(agg_chunk)\n    test_agg_df.rechunk()\n    \n    agg_df=train_agg_df\n    agg_df.extend(test_agg_df)\n    \n    print(\"agg df \", agg_df.shape)\n   \n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.244333Z","iopub.execute_input":"2024-04-01T17:51:59.244630Z","iopub.status.idle":"2024-04-01T17:51:59.257800Z","shell.execute_reply.started":"2024-04-01T17:51:59.244606Z","shell.execute_reply":"2024-04-01T17:51:59.256820Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n\n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    #train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    \n    df=pl.concat([train, test])\n    del train;del test; gc.collect()\n    agg_df = group(df, '', CREDIT_BUREAU_B_2_AGG) \n    \n    del df; gc.collect()\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.259733Z","iopub.execute_input":"2024-04-01T17:51:59.260060Z","iopub.status.idle":"2024-04-01T17:51:59.272543Z","shell.execute_reply.started":"2024-04-01T17:51:59.260033Z","shell.execute_reply":"2024-04-01T17:51:59.271591Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# **EXECUTION** <a id='execution'></a>\n\n[CONFIGURATION](#configuration) \n\n[MAIN FUNCTION](#main_function)\n\n[MODEL](#model)\n\n[EXECUTION](#execution)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    pd.set_option('display.max_rows', 60)\n    pd.set_option('display.max_columns', 100)\n    with timer(\"Pipeline total time\"):\n        main(debug= False)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:51:59.273930Z","iopub.execute_input":"2024-04-01T17:51:59.274515Z","iopub.status.idle":"2024-04-01T17:52:36.156303Z","shell.execute_reply.started":"2024-04-01T17:51:59.274478Z","shell.execute_reply":"2024-04-01T17:52:36.154935Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Notebook started:\nfloat16     116\nfloat32      17\ncategory      5\ncategory      3\ncategory      2\ncategory      2\nint32         1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\nint8          1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\ncategory      1\nName: count, dtype: int64\nDATAFRAME shape: (11121, 178)\nImporting processed dataframe - done in 0s\nMemory usage after optimization is: 3.68 MB\nDecreased by 0.6%\nTrain/valid shape: (11111, 178), \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e3efada721448b18a1f639c56b1841d"}},"metadata":{}},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.941261\tvalid_1's auc: 0.717708\n[200]\ttraining's auc: 0.968225\tvalid_1's auc: 0.706354\nEarly stopping, best iteration is:\n[105]\ttraining's auc: 0.944036\tvalid_1's auc: 0.718689\nFold  1 AUC : 0.718689. Elapsed time: 2.59 seconds. Remaining time: 23.28 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.938122\tvalid_1's auc: 0.719482\nEarly stopping, best iteration is:\n[3]\ttraining's auc: 0.805206\tvalid_1's auc: 0.738353\nFold  2 AUC : 0.738353. Elapsed time: 4.34 seconds. Remaining time: 17.35 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.937959\tvalid_1's auc: 0.642962\n[200]\ttraining's auc: 0.964639\tvalid_1's auc: 0.654924\nEarly stopping, best iteration is:\n[176]\ttraining's auc: 0.959466\tvalid_1's auc: 0.657245\nFold  3 AUC : 0.657245. Elapsed time: 8.06 seconds. Remaining time: 18.80 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.938364\tvalid_1's auc: 0.724125\n[200]\ttraining's auc: 0.965643\tvalid_1's auc: 0.731252\n[300]\ttraining's auc: 0.979773\tvalid_1's auc: 0.736687\n[400]\ttraining's auc: 0.988799\tvalid_1's auc: 0.742531\n[500]\ttraining's auc: 0.994164\tvalid_1's auc: 0.744279\nEarly stopping, best iteration is:\n[464]\ttraining's auc: 0.992401\tvalid_1's auc: 0.746791\nFold  4 AUC : 0.746791. Elapsed time: 15.10 seconds. Remaining time: 22.65 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.941136\tvalid_1's auc: 0.739172\n[200]\ttraining's auc: 0.970475\tvalid_1's auc: 0.737178\nEarly stopping, best iteration is:\n[140]\ttraining's auc: 0.956081\tvalid_1's auc: 0.742285\nFold  5 AUC : 0.742285. Elapsed time: 18.41 seconds. Remaining time: 18.41 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.937126\tvalid_1's auc: 0.680895\nEarly stopping, best iteration is:\n[99]\ttraining's auc: 0.936773\tvalid_1's auc: 0.682424\nFold  6 AUC : 0.682424. Elapsed time: 21.25 seconds. Remaining time: 14.17 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.938705\tvalid_1's auc: 0.741029\n[200]\ttraining's auc: 0.961775\tvalid_1's auc: 0.745453\n[300]\ttraining's auc: 0.979717\tvalid_1's auc: 0.749167\nEarly stopping, best iteration is:\n[274]\ttraining's auc: 0.975767\tvalid_1's auc: 0.752089\nFold  7 AUC : 0.752089. Elapsed time: 25.95 seconds. Remaining time: 11.12 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.938589\tvalid_1's auc: 0.772025\nEarly stopping, best iteration is:\n[99]\ttraining's auc: 0.938648\tvalid_1's auc: 0.772516\nFold  8 AUC : 0.772516. Elapsed time: 28.72 seconds. Remaining time: 7.18 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.936941\tvalid_1's auc: 0.660701\nEarly stopping, best iteration is:\n[68]\ttraining's auc: 0.921393\tvalid_1's auc: 0.663569\nFold  9 AUC : 0.663569. Elapsed time: 31.18 seconds. Remaining time: 3.46 seconds.\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's auc: 0.937085\tvalid_1's auc: 0.728837\nEarly stopping, best iteration is:\n[11]\ttraining's auc: 0.849451\tvalid_1's auc: 0.74064\nFold 10 AUC : 0.740640. Elapsed time: 32.99 seconds. Remaining time: 0.00 seconds.\nFull AUC score 0.701735\nGini Score of the valid set: 0.35994851654416127\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2957656830.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['PREDICTIONS'] = oof_preds.copy()\n","output_type":"stream"},{"name":"stdout","text":"Model training - done in 36s\nFeature importance assesment - done in 0s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ? models/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4079a5c7b81443a084e709eff1e8eb67"}},"metadata":{}},{"name":"stdout","text":"[[0.9704946  0.0295054 ]\n [0.89440959 0.10559041]\n [0.98813545 0.01186455]\n [0.94687766 0.05312234]\n [0.96375748 0.03624252]\n [0.98622628 0.01377372]\n [0.97918743 0.02081257]\n [0.98706662 0.01293338]\n [0.95294556 0.04705444]\n [0.97740462 0.02259538]]\n[[0.9690931  0.0309069 ]\n [0.96526579 0.03473421]\n [0.97060024 0.02939976]\n [0.96841545 0.03158455]\n [0.96681055 0.03318945]\n [0.97077306 0.02922694]\n [0.97040049 0.02959951]\n [0.97040049 0.02959951]\n [0.97040049 0.02959951]\n [0.97040049 0.02959951]]\n[[0.97430724 0.02569276]\n [0.90924229 0.09075771]\n [0.98909073 0.01090927]\n [0.95621473 0.04378527]\n [0.95537484 0.04462516]\n [0.99043427 0.00956573]\n [0.98046755 0.01953245]\n [0.98479102 0.01520898]\n [0.95505102 0.04494898]\n [0.95383703 0.04616297]]\n[[0.98501093 0.01498907]\n [0.91620248 0.08379752]\n [0.99505724 0.00494276]\n [0.980987   0.019013  ]\n [0.97498678 0.02501322]\n [0.99583726 0.00416274]\n [0.98256001 0.01743999]\n [0.99628939 0.00371061]\n [0.96482427 0.03517573]\n [0.98864659 0.01135341]]\n[[0.95223306 0.04776694]\n [0.86349115 0.13650885]\n [0.98906968 0.01093032]\n [0.96739985 0.03260015]\n [0.95710394 0.04289606]\n [0.98684788 0.01315212]\n [0.95980087 0.04019913]\n [0.98186988 0.01813012]\n [0.93470761 0.06529239]\n [0.95788233 0.04211767]]\n[[0.95906941 0.04093059]\n [0.92410422 0.07589578]\n [0.98695682 0.01304318]\n [0.95869982 0.04130018]\n [0.96157469 0.03842531]\n [0.98989749 0.01010251]\n [0.98427482 0.01572518]\n [0.98851705 0.01148295]\n [0.98055023 0.01944977]\n [0.97713575 0.02286425]]\n[[0.98652646 0.01347354]\n [0.95759352 0.04240648]\n [0.99152039 0.00847961]\n [0.97175597 0.02824403]\n [0.95842181 0.04157819]\n [0.99124651 0.00875349]\n [0.98297727 0.01702273]\n [0.99152828 0.00847172]\n [0.95913952 0.04086048]\n [0.96359368 0.03640632]]\n[[0.97192289 0.02807711]\n [0.94262526 0.05737474]\n [0.9820097  0.0179903 ]\n [0.97370033 0.02629967]\n [0.95824428 0.04175572]\n [0.98512669 0.01487331]\n [0.97630137 0.02369863]\n [0.97656832 0.02343168]\n [0.949389   0.050611  ]\n [0.94529844 0.05470156]]\n[[0.96409301 0.03590699]\n [0.90353367 0.09646633]\n [0.98391719 0.01608281]\n [0.94074233 0.05925767]\n [0.94730715 0.05269285]\n [0.98681635 0.01318365]\n [0.98050229 0.01949771]\n [0.98010861 0.01989139]\n [0.94994507 0.05005493]\n [0.9735183  0.0264817 ]]\n[[0.96198657 0.03801343]\n [0.95673115 0.04326885]\n [0.97351519 0.02648481]\n [0.95610749 0.04389251]\n [0.9663632  0.0336368 ]\n [0.97306333 0.02693667]\n [0.97339722 0.02660278]\n [0.97259483 0.02740517]\n [0.97008603 0.02991397]\n [0.96991096 0.03008904]]\nSubmission file has been created.\nSubmission - done in 1s\nNOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\nPipeline total time - done in 37s\n","output_type":"stream"}]}]}