{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom glob import glob\nfrom pathlib import Path\nfrom datetime import datetime\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score \nfrom sklearn.metrics import roc_curve, auc\nfrom tqdm.notebook import tqdm\nimport joblib\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:05.892237Z","iopub.execute_input":"2024-03-21T20:49:05.893248Z","iopub.status.idle":"2024-03-21T20:49:08.125126Z","shell.execute_reply.started":"2024-03-21T20:49:05.893193Z","shell.execute_reply":"2024-03-21T20:49:08.124297Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **CONFIGURATION**","metadata":{}},{"cell_type":"code","source":"# GENERAL CONFIGURATIONS\nNUM_THREADS = 4\nDATA_DIRECTORY = \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/\"\nSUBMISSION_SUFIX = \"_model2_0\"\n# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\nGENERATE_SUBMISSION_FILES = True\nEVALUATE_VALIDATION_SET = True\nSHOW_REPORT = True\nSTRATIFIED_KFOLD = True\nRANDOM_SEED = 737851\nNUM_FOLDS = 10\nEARLY_STOPPING = 100\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nLIGHTGBM_PARAMS = {\n    'boosting_type': 'goss',\n    'n_estimators': 1000,\n    'learning_rate': 0.005134,\n    'num_leaves': 54,\n    'max_depth': 10,\n    'subsample_for_bin': 240000,\n    'reg_alpha': 0.436193,\n    'reg_lambda': 0.479169,\n    'colsample_bytree': 0.508716,\n    'min_split_gain': 0.024766,\n    'subsample': 1,\n    'is_unbalance': False,\n    'silent':-1,\n    'verbose':-1,\n    \n}","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.127008Z","iopub.execute_input":"2024-03-21T20:49:08.127645Z","iopub.status.idle":"2024-03-21T20:49:08.134702Z","shell.execute_reply.started":"2024-03-21T20:49:08.127610Z","shell.execute_reply":"2024-03-21T20:49:08.133549Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Set aggregations","metadata":{}},{"cell_type":"code","source":"# AGGREGATIONS\n\n\n\nAPPLPREV1_AGG = {\n\n    'cancelreason_3545846M' : ['min','mean','max'],\n    'employedfrom_700D' : ['max','mean','min'],\n    'dtlastpmt_581D' : ['max','min','mean'],\n    'pmtnum_8L' : ['mean','min','max'],\n    'tenor_203L' : ['mean','min','max'],\n    'firstnonzeroinstldate_307D':['max','mean','min'],\n    'mainoccupationinc_437A' : ['min','max','mean','count','sum'],\n    'annuity_853A': ['min','max','mean','count','sum'],\n    'maxdpdtolerance_577P':['mean','max','min'],\n    'dtlastpmtallstes_3545839D':['max','mean','min'],\n    'credamount_590A':['min','mean','max'],\n    'outstandingdebt_522A':['mean','max','min'],\n    'dtlastpmt_581D':['min','mean','max'],\n    'creationdate_885D':['max','min','mean'],\n    'rejectreason_755M':['min'],\n    'currdebt_94A':['max'],\n    'approvaldate_319D':['max','min','mean'],\n    'dateactivated_425D':['mean','min','max'],\n    'familystate_726L':['max','min','mean'],\n    'inittransactioncode_279L':['min'],\n    'credtype_587L':['min'],\n    'education_1138M':['min','max','mean'],\n    'num_group1':['count']\n    \n}\nAPPLPREV2_AGG = {\n    'num_group1':['count']\n    \n}\nPERSON1_AGG={\n    'birth_259D': ['max'],\n    'empl_employedfrom_271D':['max','mean','min'],\n    'incometype_1044T':['max','min','mean'],\n    'mainoccupationinc_384A':['min','max','mean','count','sum'],\n    'relationshiptoclient_415T':['max','min','mean'],\n    'relationshiptoclient_642T':['max','mean','min'],\n    'language1_981M':['min','max'],\n    'familystate_447L':['max','min','mean'],\n    'sex_738L':['max','min','mean'],\n    'num_group1':['count']\n    \n    \n}\nPERSON2_AGG={\n    'num_group1':['count']\n}\nOTHER_AGG={\n    'num_group1':['count']\n}\nDEBITCARD_AGG={\n    'num_group1':['count']\n}\nTAX_REGISTRY_A_AGG={\n    'amount_4527230A': ['max','mean','min'],\n    'num_group1':['count']\n    \n}\nTAX_REGISTRY_B_AGG={\n    \n    'amount_4917619A':['min','mean','max'],\n    'num_group1':['count']\n}\nTAX_REGISTRY_C_AGG={\n    \n    'pmtamount_36A':['min','mean','max'],\n    'processingdate_168D':['mean','min','max'],\n    'num_group1':['count']\n}\nCREDIT_BUREAU_A_1_AGG={\n    \n    'overdueamountmaxdatemonth_365T':['min','max','mean','count','sum'],\n    'num_group1':['min','max','mean','count','sum'],\n    'dpdmax_757P':['min','max','mean','count','sum'],\n    'instlamount_768A':['min','max','mean','count','sum'],\n    'outstandingamount_362A':['min','max','mean','count','sum'],\n    'overdueamount_31A':['min','max','mean','count','sum'],\n    'overdueamount_659A':['min','max','mean','count','sum']\n    \n}\nCREDIT_BUREAU_B_1_AGG={\n    'num_group1':['min','max','mean','count','sum']\n}\nCREDIT_BUREAU_A_2_AGG={\n    \n    'collater_valueofguarantee_1124L':['min','max','mean','count','sum'],\n    'collater_valueofguarantee_876L':['min','max','mean','count','sum'],\n    'pmts_year_1139T':['min','max','mean','count','sum'],\n    'pmts_overdue_1152A':['min','max','mean','count','sum'],\n    \n    \n    'num_group1':['min','max','mean','count','sum'],\n    'num_group2':['min','max','mean','count','sum']\n}\nCREDIT_BUREAU_B_2_AGG={\n    'num_group1':['count']\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.136466Z","iopub.execute_input":"2024-03-21T20:49:08.137157Z","iopub.status.idle":"2024-03-21T20:49:08.152079Z","shell.execute_reply.started":"2024-03-21T20:49:08.137124Z","shell.execute_reply":"2024-03-21T20:49:08.151209Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **MAIN FUNCTION**","metadata":{}},{"cell_type":"code","source":"def main(debug= False):\n    num_rows = 11111 if debug else None\n    with timer(\"base\"):\n        \n        df = get_base(DATA_DIRECTORY, num_rows=num_rows)\n        #df=df.to_pandas()\n        print(\"base dataframe shape:\", df.shape)\n        #df=reduce_mem_usage(df)\n        \n        \n    with timer(\"static\"):\n       \n        df_static = get_static(DATA_DIRECTORY, num_rows=num_rows)\n        df = df.join(df_static, on='case_id', how='left', suffix='_static')\n        print(\"static dataframe shape:\", df_static.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        \n        del df_static\n        gc.collect()\n\n    with timer(\"static_cb\"):\n       \n        df_static_cb = get_static_cb(DATA_DIRECTORY, num_rows=num_rows)\n        df = df.join(df_static_cb, on='case_id', how='left', suffix='_static_cb')\n        print(\"static cb dataframe shape:\", df_static_cb.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_static_cb\n        gc.collect()\n\n    with timer(\"Previous applications depth 1 test\"):\n       \n        df_applprev1 = get_applprev1(DATA_DIRECTORY, num_rows=num_rows)\n        df_applprev1 = df_applprev1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n        print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_applprev1\n        gc.collect()\n\n    with timer(\"Previous applications depth 2 test\"):\n       \n        df_applprev2 = get_applprev2(DATA_DIRECTORY, num_rows=num_rows)\n        df_applprev2 = df_applprev2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_applprev2, on='case_id', how='left', suffix='_applprev2')\n        print(\"Previous applications depth 2 test dataframe shape:\", df_applprev2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_applprev2\n        gc.collect()\n\n    with timer(\"Person depth 1 test\"):\n       \n        df_person1 = get_person1(DATA_DIRECTORY, num_rows=num_rows)\n        df_person1 = df_person1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_person1, on='case_id', how='left', suffix='_person1')\n        print(\"Person depth 1 test dataframe shape:\", df_person1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_person1\n        gc.collect()\n\n    with timer(\"Person depth 2 test\"):\n     \n        df_person2 = get_person2(DATA_DIRECTORY, num_rows=num_rows)\n        df_person2 = df_person2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_person2, on='case_id', how='left', suffix='_person2')\n        print(\"Person depth 2 test dataframe shape:\", df_person2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_person2\n        gc.collect()\n\n    with timer(\"Other test\"):\n        \n        df_other = get_other(DATA_DIRECTORY, num_rows=num_rows)\n        df_other = df_other.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_other, on='case_id', how='left', suffix='_other')\n        print(\"Other test dataframe shape:\", df_other.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_other\n        gc.collect()\n\n    with timer(\"Debit card test\"):\n      \n        df_debitcard = get_debitcard(DATA_DIRECTORY, num_rows=num_rows)\n        df_debitcard = df_debitcard.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_debitcard, on='case_id', how='left', suffix='_debitcard')\n        print(\"Debit card test dataframe shape:\", df_debitcard.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_debitcard\n        gc.collect()\n\n    with timer(\"Tax registry a test\"):\n        \n        df_tax_registry_a = get_tax_registry_a(DATA_DIRECTORY, num_rows=num_rows)\n        df_tax_registry_a = df_tax_registry_a.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_tax_registry_a, on='case_id', how='left', suffix='_tax_registry_a')\n        print(\"Tax registry a test dataframe shape:\", df_tax_registry_a.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_tax_registry_a\n        gc.collect()\n\n    with timer(\"Tax registry b test\"):\n       \n        df_tax_registry_b = get_tax_registry_b(DATA_DIRECTORY, num_rows=num_rows)\n        df_tax_registry_b = df_tax_registry_b.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_tax_registry_b, on='case_id', how='left', suffix='_tax_registry_b')\n        print(\"Tax registry b test dataframe shape:\", df_tax_registry_b.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_tax_registry_b\n        gc.collect()\n\n    with timer(\"Tax registry c test\"):\n        \n        df_tax_registry_c = get_tax_registry_c(DATA_DIRECTORY, num_rows=num_rows)\n        df_tax_registry_c = df_tax_registry_c.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_tax_registry_c, on='case_id', how='left', suffix='_tax_registry_c')\n        print(\"Tax registry c test dataframe shape:\", df_tax_registry_c.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_tax_registry_c\n        gc.collect()\n    \n        \n \n    with timer(\"Credit bureau a 1 test\"):\n\n        df_credit_bureau_a_1 = get_credit_bureau_a_1(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_a_1 = df_credit_bureau_a_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_a_1, on='case_id', how='left', suffix='_cb_a_1')\n        print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_a_1\n        gc.collect()\n    with timer(\"Credit bureau b 1 test\"):\n       \n        df_credit_bureau_b_1 = get_credit_bureau_b_1(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_b_1 = df_credit_bureau_b_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_b_1, on='case_id', how='left', suffix='_cb_b_1')\n        print(\"Credit bureau b 1 test dataframe shape:\", df_credit_bureau_b_1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_b_1\n        gc.collect()\n\n\n    \n    \n    with timer(\"Credit bureau a 2 test\"):\n       \n        df_credit_bureau_a_2 = get_credit_bureau_a_2(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_a_2 = df_credit_bureau_a_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_a_2, on='case_id', how='left', suffix='_cb_a_2')\n        print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_a_2\n        gc.collect()\n    \n    with timer(\"Credit bureau b 2 test\"):\n       \n        df_credit_bureau_b_2 = get_credit_bureau_b_2(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_b_2 = df_credit_bureau_b_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_b_2, on='case_id', how='left', suffix='_cb_b_2')\n        print(\"Credit bureau b 2 test dataframe shape:\", df_credit_bureau_b_2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_b_2\n        gc.collect()\n    with timer(\"Feature engineering / preprocessing\"): \n     \n        df=feature_engineering(df)\n        print(\"DATAFRAME shape:\", df.shape)\n   \n    with timer(\"Model training\"):\n        get_info(df)\n        df, cat_cols = to_pandas(df)\n        \n        model = kfold_lightgbm_sklearn(df, cat_cols)\n       \n    with timer(\"Feature importance assesment\"):\n      \n        get_features_importances(df, model)\n        if SHOW_REPORT:\n            make_report(num_rows, df, model)\n        \n    with timer(\"Submission\"):\n    \n        if generate_submission_file(df, model):\n         \n            print(\"Submission file has been created.\")\n        \n    \n  \n    \n    print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n    \n    return df, model","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.154918Z","iopub.execute_input":"2024-03-21T20:49:08.155594Z","iopub.status.idle":"2024-03-21T20:49:08.270191Z","shell.execute_reply.started":"2024-03-21T20:49:08.155560Z","shell.execute_reply":"2024-03-21T20:49:08.269273Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY FUNCTIONS**","metadata":{}},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"class Pipeline:\n    @staticmethod\n    \n    \n    # Sets datatypes accordingly\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))            \n\n        return df\n    \n    \n    # Changes the values of all date columns. The result will not be a date but number of days since date_decision.\n    @staticmethod\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n                df = df.with_columns(pl.col(col).dt.total_days())\n                \n        df = df.drop(\"date_decision\", \"MONTH\")\n\n        return df\n    \n    # It drops columns with a lot of NaN values.\n    @staticmethod\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n\n                if isnull > 0.95:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.271558Z","iopub.execute_input":"2024-03-21T20:49:08.272091Z","iopub.status.idle":"2024-03-21T20:49:08.286082Z","shell.execute_reply.started":"2024-03-21T20:49:08.272014Z","shell.execute_reply":"2024-03-21T20:49:08.285078Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_info(dataframe):\n    \"\"\"\n    View data types, shape, and calculate the percentage of NaN (missing) values in each column\n    of a Polars DataFrame simultaneously.\n    \n    Parameters:\n    dataframe (polars.DataFrame): The DataFrame to analyze.\n    \n    Returns:\n    None\n    \"\"\"\n    # Print DataFrame shape\n    print(\"DataFrame Shape:\", dataframe.shape)\n    print(\"-\" * 60)\n    \n    # Print column information\n    print(\"{:<50} {:<30} {:<20}\".format(\"Column Name\", \"Data Type\", \"NaN Percentage\"))\n    print(\"-\" * 60)\n    \n    # Total number of rows in the DataFrame\n    total_rows = len(dataframe)\n    \n    # Iterate over each column\n    for column in dataframe.columns:\n        # Get the data type of the column\n        dtype = str(dataframe[column].dtype)\n        \n        # Count the number of NaN values in the column\n        nan_count = dataframe[column].null_count()\n        \n        # Calculate the percentage of NaN values\n        nan_percentage = (nan_count / total_rows) * 100\n        \n        # Print the information\n        print(\"{:<50} {:<30} {:.2f}%\".format(column, dtype, nan_percentage))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.287419Z","iopub.execute_input":"2024-03-21T20:49:08.288220Z","iopub.status.idle":"2024-03-21T20:49:08.299687Z","shell.execute_reply.started":"2024-03-21T20:49:08.288190Z","shell.execute_reply":"2024-03-21T20:49:08.298866Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    \n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    \n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    \n    return df_data, cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.300946Z","iopub.execute_input":"2024-03-21T20:49:08.301215Z","iopub.status.idle":"2024-03-21T20:49:08.310164Z","shell.execute_reply.started":"2024-03-21T20:49:08.301193Z","shell.execute_reply":"2024-03-21T20:49:08.309390Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.311231Z","iopub.execute_input":"2024-03-21T20:49:08.311519Z","iopub.status.idle":"2024-03-21T20:49:08.323119Z","shell.execute_reply.started":"2024-03-21T20:49:08.311496Z","shell.execute_reply":"2024-03-21T20:49:08.322322Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.324466Z","iopub.execute_input":"2024-03-21T20:49:08.324962Z","iopub.status.idle":"2024-03-21T20:49:08.334017Z","shell.execute_reply.started":"2024-03-21T20:49:08.324861Z","shell.execute_reply":"2024-03-21T20:49:08.333275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    \n\n    temp=base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\").mean()\n   \n    week_nums_to_drop = temp[(temp[\"target\"] == 0) | (temp[\"target\"] == 1)].index.tolist()\n\n    base_filtered = base[~base[\"WEEK_NUM\"].isin(week_nums_to_drop)]\n\n    # Apply the aggregator\n    gini_in_time = base_filtered.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]] \\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n\n    \n\n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a * x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.nanmean(gini_in_time)  # Use np.nanmean to handle NaN values\n    \n    if SHOW_REPORT:\n        # Display the plot of x on y\n        plt.figure(figsize=(8, 6))\n        plt.plot(x, y, 'o', label='Gini in Time')\n        plt.plot(x, y_hat, '-', label='Fitted line (slope={:.2f}, intercept={:.2f})'.format(a, b))\n        plt.xlabel('Week')\n        plt.ylabel('Gini in Time')\n        plt.title('Gini Stability Over Time')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    \n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.338438Z","iopub.execute_input":"2024-03-21T20:49:08.338757Z","iopub.status.idle":"2024-03-21T20:49:08.348502Z","shell.execute_reply.started":"2024-03-21T20:49:08.338731Z","shell.execute_reply":"2024-03-21T20:49:08.347786Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Report function","metadata":{}},{"cell_type":"code","source":"def make_report(num_rows, data, model):\n    # 1. time\n    current_time = datetime.now()\n    # Print the current time\n    print(\"Current Time:\", current_time)\n    \n    # 2. specification\n    if not num_rows:\n        print(\"The notebook was run in full mode.\")\n    else:\n        print(\"The notebook was run in debug mode. Number of rows: \" + str(num_rows))\n    \n    # 3. features\n    feat_importances_df = model.get_features_importances_df(data)\n    feat_importances_df['gain'] = feat_importances_df['gain'].round(0)\n    print(feat_importances_df.shape)\n    \n    predictions = pd.Series(model.get_predictions())\n   \n    numerical_columns = data.select_dtypes(include=['int', 'float']).columns\n\n    # Compute correlations of each numerical column with 'PREDICTIONS'\n    correlations = {}\n    \n    # Compute correlations of each numerical column with 'feat'\n    for column in numerical_columns:\n        correlations[column] = predictions.corr(data[column])\n\n    # Create a new DataFrame with 'features' and 'correlation' columns\n    correlation_df = pd.DataFrame(list(correlations.items()), columns=['features', 'correlation'])\n\n    # Round the correlation numbers to three decimal places\n    correlation_df['correlation'] = correlation_df['correlation'].round(3)\n\n    # Merge feat_importances_df and correlation_df on 'feature'\n    combined_df = pd.merge(feat_importances_df, correlation_df, left_on=\"feature\", right_on='features', how='left')\n\n    # Handle categorical features with no correlation\n    combined_df['correlation'] = combined_df['correlation'].fillna(value=np.nan)\n    \n\n    # Compute and add valid percentage for each feature\n    valid_percentage = (data[0:-10].count() / len(data[0:-10]))\n    valid_percentage = valid_percentage.round(3)\n    combined_df['valid_percentage'] = combined_df['feature'].map(valid_percentage)\n\n    # Print the combined_df DataFrame\n    print(combined_df.to_string(index=False))\n    print()\n    roc_score=roc_auc_score(data['target'][0:-10],predictions)\n    print(\"ROC score: \",roc_score)\n\n    # Compute false positive rate, true positive rate, and thresholds for ROC curve\n    fpr, tpr, thresholds = roc_curve(data['target'][0:-10], predictions)\n\n    # Plot ROC curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_score)\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.349818Z","iopub.execute_input":"2024-03-21T20:49:08.350382Z","iopub.status.idle":"2024-03-21T20:49:08.362440Z","shell.execute_reply.started":"2024-03-21T20:49:08.350343Z","shell.execute_reply":"2024-03-21T20:49:08.361508Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#  **MODEL**","metadata":{}},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n        # Use tqdm to create a progress bar during the prediction\n        with tqdm(total=len(self.estimators), desc=\"Predicting\", unit=\" models\") as pbar:\n            for i, estimator in enumerate(self.estimators):\n                y_preds[i] = estimator.predict_proba(X)\n                pbar.update(1)  # Update the progress bar\n        return np.mean(y_preds, axis=0)\n\n    \n    def get_splits(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for x in self.estimators:\n            feature_importances_list.append(x.booster_.feature_importance(importance_type='split'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None   \n        return combined_importances\n    \n    \n    def get_gains(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for model in self.estimators:\n            feature_importances_list.append(x.booster_.feature_importance(importance_type='gain'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None\n              \n        return combined_importances\n    \n    def get_features_importances_df(self, df):\n        del_features = ['target', 'case_id']\n        predictors = list(filter(lambda v: v not in del_features, df.columns))\n        importance_df = pd.DataFrame()\n        eval_results = dict()\n        for model in self.estimators:\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = predictors\n            fold_importance[\"gain\"] = model.booster_.feature_importance(importance_type='gain')\n            fold_importance[\"split\"] = model.booster_.feature_importance(importance_type='split')\n            importance_df = pd.concat([importance_df, fold_importance], axis=0)\n            importance_df= importance_df.groupby('feature').mean().reset_index()\n        return importance_df\n    \n    \n    def add_predictions(self, predictions):\n        self.predictions=predictions\n        \n    def get_predictions(self):\n        return self.predictions\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.363802Z","iopub.execute_input":"2024-03-21T20:49:08.364080Z","iopub.status.idle":"2024-03-21T20:49:08.380076Z","shell.execute_reply.started":"2024-03-21T20:49:08.364056Z","shell.execute_reply":"2024-03-21T20:49:08.378524Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def kfold_lightgbm_sklearn(data, categorical_feature = None):\n    start_time = time.time()\n    df = data[data['target'].notnull()]\n    test = data[data['target'].isnull()]\n    print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n    del_features = ['target', 'case_id']\n    predictors = list(filter(lambda v: v not in del_features, df.columns))\n\n    if not STRATIFIED_KFOLD:\n        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    else:\n        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    \n        # Hold oof predictions, test predictions, feature importance and training/valid auc\n    oof_preds = np.zeros(df.shape[0])\n    \n    importance_df = pd.DataFrame()\n    eval_results = dict()\n    \n    fitted_models = []\n    with tqdm(total=NUM_FOLDS) as pbar:\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):\n            train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]\n            valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]\n\n            params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n            clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n\n\n            if not categorical_feature:\n                    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                            eval_metric='auc' )\n            else:\n                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n                        feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n\n\n            fitted_models.append(clf)\n\n            if EVALUATE_VALIDATION_SET:\n                oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n\n\n\n                # Feature importance by GAIN and SPLIT\n\n            eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n            eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n\n            elapsed_time = time.time() - start_time\n            remaining_time = elapsed_time * (NUM_FOLDS - n_fold - 1) / (n_fold + 1)\n            print('Fold %2d AUC : %.6f. Elapsed time: %.2f seconds. Remaining time: %.2f seconds.'\n                  % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx]), elapsed_time, remaining_time))\n            del clf, train_x, train_y, valid_x, valid_y\n            gc.collect()\n            pbar.update(1)\n            \n    print('Full AUC score %.6f' % roc_auc_score(df['target'], oof_preds))\n    # Get the average feature importance between folds\n    \n    \n    \n    if len(df)>0:\n        base=get_base(DATA_DIRECTORY, len(df))\n        base, cat_cols = to_pandas(base)\n        base=base[base['target'].notnull()]\n        base['score']= oof_preds\n        gini_score = gini_stability(base)\n        print(\"Gini Score of the valid set:\", gini_score)\n    \n    \n    \n    \n    # Save feature importance, test predictions and oof predictions as csv\n    if GENERATE_SUBMISSION_FILES:\n\n        # Generate oof csv\n        oof = pd.DataFrame()\n        oof['case_id'] = df['case_id'].copy()\n        df['PREDICTIONS'] = oof_preds.copy()\n        df['target'] = df['target'].copy()\n        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n        \n  \n        \n        \n    model = VotingModel(fitted_models)\n    model.add_predictions(oof_preds.copy())\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.381558Z","iopub.execute_input":"2024-03-21T20:49:08.382475Z","iopub.status.idle":"2024-03-21T20:49:08.400223Z","shell.execute_reply.started":"2024-03-21T20:49:08.382436Z","shell.execute_reply":"2024-03-21T20:49:08.399192Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# **SUBMISSION**","metadata":{}},{"cell_type":"code","source":"def generate_submission_file(data, model):\n   \n    test = data[data['target'].isnull()]\n    \n    '''\n    length=len(test)\n    y_pred = pd.Series([0.5] * length,index=test['case_id'])\n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n    df_subm.to_csv(\"submission.csv\")\n    '''\n    \n    del_features = ['target', 'case_id']\n    predictors = list(filter(lambda v: v not in del_features, data.columns))\n    y_pred = pd.Series(model.predict_proba(test[predictors])[:, 1], index=test['case_id']) \n   \n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n  \n    df_subm.to_csv(\"submission.csv\")\n    \n    return True\n   \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.401436Z","iopub.execute_input":"2024-03-21T20:49:08.401819Z","iopub.status.idle":"2024-03-21T20:49:08.413325Z","shell.execute_reply.started":"2024-03-21T20:49:08.401785Z","shell.execute_reply":"2024-03-21T20:49:08.412461Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# **EVALUATE FEATURES IMPORTANCES**","metadata":{}},{"cell_type":"code","source":"def get_features_importances(data, model):\n    importance_df = model.get_features_importances_df(data)\n    mean_importance = importance_df.groupby('feature').mean().reset_index()\n    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n    mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.414664Z","iopub.execute_input":"2024-03-21T20:49:08.415074Z","iopub.status.idle":"2024-03-21T20:49:08.425152Z","shell.execute_reply.started":"2024-03-21T20:49:08.415039Z","shell.execute_reply":"2024-03-21T20:49:08.424365Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **FEATURE ENGINEERING FUNCTION**","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    \n    \n    df=df.pipe(Pipeline.handle_dates) \n    df=df.pipe(Pipeline.filter_cols)\n    \n    \n    columns_to_add = [\n        (pl.col(\"days30_165L\")/ pl.col(\"days360_512L\")).alias(\"ratio_queries_30\"),\n        ((pl.col(\"days90_310L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_90\")),\n        ((pl.col(\"days120_123L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_120\")),\n        ((pl.col(\"days180_256L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_180\")),\n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"collater_typofvalofguarant_407M\")).alias(\"sum_collater\")),\n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_active\")),\n        #((pl.col(\"collater_typofvalofguarant_407M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_close\")),\n        #((pl.col(\"overdueamount_31A\") + pl.col(\"overdueamount_659A\")).alias(\"sum_overdue_amount\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"totalamount\")).alias(\"sum_totalcredit_contract\")),\n        #((pl.col(\"totalamount_503A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_active\")),\n        #((pl.col(\"totalamount_6A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_close\")),\n        #((pl.col(\"totaldebtoverduevalue_178A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_active\")),\n        #((pl.col(\"totaldebtoverduevalue_718A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_close\")),\n        #((pl.col(\"numberofinstls_229L\") + pl.col(\"numberofinstls_320L\")).alias(\"sum_instalments\")),\n        #((pl.col(\"numberofinstls_320L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_active\")),\n        #((pl.col(\"numberofinstls_229L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_close\"))\n    ]\n\n# Add the calculated columns to the DataFrame\n    for column in columns_to_add:\n        df = df.with_columns([column])\n        \n        \n    '''\n    df = df.with_columns(\n    'ratio_queries_30',\n    df['days30_165L'] / df['days360_512L']\n)\n    df['ratio_queries_90'] = df['days90_310L'] / df['days360_512L']\n    df['ratio_queries_120'] = df['days120_123L'] / df['days360_512L']\n    df['ratio_queries_180'] = df['days180_256L'] / df['days360_512L']\n    df['sum_collater'] = df['collater_typofvalofguarant_298M'] +    df['collater_typofvalofguarant_407M']\n    df['ratio_collater_active'] = df['collater_typofvalofguarant_298M'] +    df['sum_collater']\n    df['ratio_collater_close'] = df['collater_typofvalofguarant_407M'] +    df['sum_collater']\n    df['sum_overdue_amount'] = df['overdueamount_31A'] +    df['overdueamount_659A']\n    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['sum_overdue_amount']\n    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['sum_overdue_amount']\n    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['total_overdue_amount']\n    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['total_overdue_amount']\n    df['sum_totalcredit_contract'] = df['totalamount']\n    df['ratio_totalcredit_contract_active'] = df['totalamount_503A'] /    df['sum_totalcredit_contract']\n    df['ratio_totalcredit_contract_close'] = df['totalamount_6A'] /    df['sum_totalcredit_contract']\n    df['ratio_overdue_debt_active'] = df['totaldebtoverduevalue_178A'] /    df['totaldebt_9A']\n    df['ratio_overdue_debt_close'] = df['totaldebtoverduevalue_718A'] /    df['totaldebt_9A']\n    df['sum_instalments'] = df['numberofinstls_229L'] +    df['numberofinstls_320L']\n    df['ratio_instalments_active'] = df['numberofinstls_320L'] /    df['sum_instalments']\n    df['ratio_instalments_close'] = df['numberofinstls_229L'] /    df['sum_instalments']\n    '''\n    df=df.pipe(Pipeline.filter_cols)\n    \n    columns_to_drop=[\n     \n    \"fourthquarter_440L\",\n    \"firstquarter_103L\",\n    \"lastrejectcommoditycat_161M\",\n    \"numinstpaidearly5dobd_4499205L\",\n    \"clientscnt3m_3712950L\",\n    \"numinstpaidearly5d_1087L\",\n    \"maritalst_385M\",\n    \"maxdpdinstlnum_3546846P\",\n    \"max_credamount_590A\",\n    \"maxoutstandbalancel12m_4187113A\",\n    \"cardtype_51L\",\n    \"avginstallast24m_3658937A\",\n    \"numinstpaidearlyest_4493214L\",\n    \"count_num_group1_applprev2\",\n    \"mean_mainoccupationinc_384A\",\n    \"totinstallast1m_4525188A\",\n    \"numinstlswithdpd5_4187116L\",\n    \"clientscnt_1022L\",\n    \"numnotactivated_1143L\",\n    \"pmtcount_693L\",\n    \"numinstpaidlate1d_3546852L\",\n    \"numinstpaidearly5dest_4493211L\",\n    \"annuitynextmonth_57A\",\n    \"avgpmtlast12m_4525200A\",\n    \"maxlnamtstart6m_4525199A\",\n    \"assignmentdate_238D\",\n    \"min_outstandingdebt_522A\",\n    \"posfstqpd30lastmonth_3976962P\",\n    \"actualdpdtolerance_344P\",\n    \"applicationscnt_1086L\",\n    \"maxpmtlast3m_4525190A\",\n    \"count_num_group1_person1\",\n    \"pmtcount_4527229L\",\n    \"typesuite_864L\",\n    \"isdebitcard_729L\",\n    \"applications30d_658L\",\n    \"max_cancelreason_3545846M\",\n    \"sellerplacecnt_915L\",\n    \"posfpd30lastmonth_3976960P\",\n    \"count_num_group1_debitcard\",\n    \"numactiverelcontr_750L\",\n    \"assignmentdate_4527235D\",\n    \"clientscnt_887L\",\n    \"responsedate_1012D\",\n    \"maritalst_893M\",\n    \"applicationscnt_464L\",\n    \"numactivecredschannel_414L\",\n    \"numactivecreds_622L\",\n    \"applicationscnt_629L\",\n    \"paytype1st_925L\",\n    \"clientscnt_533L\",\n    \"posfpd10lastmonth_333P\",\n    \"clientscnt_304L\",\n    \"clientscnt_946L\",\n    \"equalitydataagreement_891L\",\n    \"clientscnt_100L\",\n    \"paytype_783L\",\n    \"commnoinclast6m_3546845L\",\n    \"clientscnt_1130L\",\n    \"clientscnt_360L\",\n    \"clientscnt_1071L\",\n    \"clientscnt_157L\",\n    \"education_88M\",\n    \"opencred_647L\",\n    \"numpmtchanneldd_318L\",\n    \"clientscnt_493L\",\n    \"clientscnt_257L\",\n    \"mastercontrelectronic_519L\",\n    \"mastercontrexist_109L\",\n    \"deferredmnthsnum_166L\",\n    \"applicationcnt_361L\",\n    \"lastrejectcommodtypec_5251769M\"\n]\n    columns_to_drop_existing = [col for col in columns_to_drop if col in df.columns]\n\n    df=df.drop(columns_to_drop_existing)\n    \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.426573Z","iopub.execute_input":"2024-03-21T20:49:08.427336Z","iopub.status.idle":"2024-03-21T20:49:08.441670Z","shell.execute_reply.started":"2024-03-21T20:49:08.427302Z","shell.execute_reply":"2024-03-21T20:49:08.440722Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **GET FUNCTIONS**","metadata":{}},{"cell_type":"code","source":"def group(df_to_agg, prefix, aggregations, aggregate_by='case_id', datatype='polars'):\n    # Create a dictionary mapping aggregation functions to their string representations\n    \n    if datatype=='polars':\n        func_mapping = {\n        'min': pl.min,\n        'max': pl.max,\n        'mean': pl.mean,\n        'sum': pl.sum,\n        'count': pl.count\n        }\n\n    # Perform the aggregation\n        agg_df = df_to_agg.group_by(aggregate_by).agg(**{\n            f\"{func}_{col}\": func_mapping[func](col) for col, funcs in aggregations.items() for func in funcs\n        })\n        '''\n        # Rename columns\n        for col, funcs in aggregations.items():\n            for func in funcs:\n                old_name = f\"{col}_{func}\"\n                new_name = f\"{prefix}{col}_{func.upper()}\"\n                agg_df = agg_df.select(pl.col(old_name).alias(new_name))\n        '''\n        return agg_df\n    \n    if datatype=='pandas':\n            # Create a dictionary mapping aggregation functions to their string representations\n        func_mapping = {\n            'min': 'min',\n            'max': 'max',\n            'mean': 'mean',\n            'sum': 'sum',\n            'count': 'count'\n        }\n\n        # Perform the aggregation\n        agg_df = df_to_agg.groupby(aggregate_by).agg(**{\n            f\"{prefix}{col}_{func.upper()}\": (col, func_mapping[func]) for col, funcs in aggregations.items() for func in funcs\n        }).reset_index()\n        \n        return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.443140Z","iopub.execute_input":"2024-03-21T20:49:08.443641Z","iopub.status.idle":"2024-03-21T20:49:08.455263Z","shell.execute_reply.started":"2024-03-21T20:49:08.443611Z","shell.execute_reply":"2024-03-21T20:49:08.454477Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### get_base()","metadata":{}},{"cell_type":"code","source":"def get_base(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    train={}\n    test={}\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet'))\n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet')).limit(num_rows) \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_base.parquet'))    \n    length=len(test)\n    nan_series=pl.Series([None] * length)\n    test = test.select(pl.col(\"*\"), nan_series.alias(\"target\"))\n    df=pl.concat([train, test])\n    df = df.with_columns(pl.col('date_decision').cast(pl.Date))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.456486Z","iopub.execute_input":"2024-03-21T20:49:08.456945Z","iopub.status.idle":"2024-03-21T20:49:08.468231Z","shell.execute_reply.started":"2024-03-21T20:49:08.456919Z","shell.execute_reply":"2024-03-21T20:49:08.467408Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### get_static()","metadata":{}},{"cell_type":"code","source":"def get_static(path, num_rows = None):\n# Read the Parquet file using scan() method\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes) )\n    train = (pl.concat(chunks, how=\"vertical_relaxed\")).pipe(Pipeline.filter_cols)\n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n        \n        train=df1\n        del df2\n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes) )\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    \n    \n    columns_to_keep = train.columns\n\n# Find columns in 'test' that are not in 'train'\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n\n# Drop columns from 'test' that are not in 'train'\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.469279Z","iopub.execute_input":"2024-03-21T20:49:08.469581Z","iopub.status.idle":"2024-03-21T20:49:08.481144Z","shell.execute_reply.started":"2024-03-21T20:49:08.469556Z","shell.execute_reply":"2024-03-21T20:49:08.480223Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### get_static_cb()","metadata":{}},{"cell_type":"code","source":"def get_static_cb(path, num_rows = None):\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes) \n       \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.482477Z","iopub.execute_input":"2024-03-21T20:49:08.482784Z","iopub.status.idle":"2024-03-21T20:49:08.491692Z","shell.execute_reply.started":"2024-03-21T20:49:08.482758Z","shell.execute_reply":"2024-03-21T20:49:08.490647Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev1(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev1(path, num_rows = None):\n    \n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n    \n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n\n        train=df1\n        del df2   \n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    agg_df = group(df, '', APPLPREV1_AGG)\n    del df \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.493344Z","iopub.execute_input":"2024-03-21T20:49:08.494484Z","iopub.status.idle":"2024-03-21T20:49:08.505078Z","shell.execute_reply.started":"2024-03-21T20:49:08.494444Z","shell.execute_reply":"2024-03-21T20:49:08.504257Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev2(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev2(path, num_rows = None):\n    train={}\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes) \n       \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', APPLPREV2_AGG)\n    del df \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.506300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get_person1","metadata":{}},{"cell_type":"code","source":"def get_person1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).pipe(Pipeline.set_table_dtypes) \n      \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', PERSON1_AGG)\n    del df\n    \n    return agg_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get_person2","metadata":{}},{"cell_type":"code","source":"def get_person2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).pipe(Pipeline.set_table_dtypes)\n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', PERSON2_AGG)\n    del df\n    \n    return agg_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### other","metadata":{}},{"cell_type":"code","source":"def get_other(path, num_rows = None):\n     # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).pipe(Pipeline.set_table_dtypes) \n         \n    test = pl.read_parquet(os.path.join(path, 'test/test_other_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', OTHER_AGG)\n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.execute_input":"2024-03-21T20:49:08.543596Z","iopub.status.idle":"2024-03-21T20:49:08.552379Z","shell.execute_reply.started":"2024-03-21T20:49:08.543556Z","shell.execute_reply":"2024-03-21T20:49:08.551550Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## get_debitcard","metadata":{}},{"cell_type":"code","source":"def get_debitcard(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes) \n      \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', DEBITCARD_AGG)\n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.553707Z","iopub.execute_input":"2024-03-21T20:49:08.554243Z","iopub.status.idle":"2024-03-21T20:49:08.566381Z","shell.execute_reply.started":"2024-03-21T20:49:08.554215Z","shell.execute_reply":"2024-03-21T20:49:08.565568Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_a","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_a(path, num_rows = None):\n    \n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes) \n  \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', TAX_REGISTRY_A_AGG)    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.567770Z","iopub.execute_input":"2024-03-21T20:49:08.568290Z","iopub.status.idle":"2024-03-21T20:49:08.580403Z","shell.execute_reply.started":"2024-03-21T20:49:08.568261Z","shell.execute_reply":"2024-03-21T20:49:08.579526Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_b","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_b(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', TAX_REGISTRY_B_AGG) \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.585039Z","iopub.execute_input":"2024-03-21T20:49:08.585649Z","iopub.status.idle":"2024-03-21T20:49:08.593270Z","shell.execute_reply.started":"2024-03-21T20:49:08.585582Z","shell.execute_reply":"2024-03-21T20:49:08.592288Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_c","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_c(path, num_rows = None):\n     # Read the Parquet file using scan() method\n# Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', TAX_REGISTRY_C_AGG)    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.594493Z","iopub.execute_input":"2024-03-21T20:49:08.594971Z","iopub.status.idle":"2024-03-21T20:49:08.605513Z","shell.execute_reply.started":"2024-03-21T20:49:08.594944Z","shell.execute_reply":"2024-03-21T20:49:08.604704Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_1(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    train_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    test_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    agg_df=pl.concat([train_agg_df,test_agg_df],how=\"vertical_relaxed\")\n    del train_agg_df\n    del test_agg_df\n    \n    \n    print(\"agg df \", agg_df.shape)\n    print(agg_df.head())\n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.606671Z","iopub.execute_input":"2024-03-21T20:49:08.607147Z","iopub.status.idle":"2024-03-21T20:49:08.615980Z","shell.execute_reply.started":"2024-03-21T20:49:08.607121Z","shell.execute_reply":"2024-03-21T20:49:08.615201Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#A=get_credit_bureau_a_1(DATA_DIRECTORY)\n#print(A.head())\n#del A","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.616945Z","iopub.execute_input":"2024-03-21T20:49:08.617565Z","iopub.status.idle":"2024-03-21T20:49:08.627871Z","shell.execute_reply.started":"2024-03-21T20:49:08.617538Z","shell.execute_reply":"2024-03-21T20:49:08.627068Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', CREDIT_BUREAU_B_1_AGG) \n    \n    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.628805Z","iopub.execute_input":"2024-03-21T20:49:08.629591Z","iopub.status.idle":"2024-03-21T20:49:08.637626Z","shell.execute_reply.started":"2024-03-21T20:49:08.629565Z","shell.execute_reply":"2024-03-21T20:49:08.636871Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_2(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    train_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    test_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    agg_df=pl.concat([train_agg_df,test_agg_df],how=\"vertical_relaxed\")\n    del train_agg_df\n    del test_agg_df\n    \n    \n    print(\"agg df \", agg_df.shape)\n    print(agg_df.head())\n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.638702Z","iopub.execute_input":"2024-03-21T20:49:08.639168Z","iopub.status.idle":"2024-03-21T20:49:08.651435Z","shell.execute_reply.started":"2024-03-21T20:49:08.639135Z","shell.execute_reply":"2024-03-21T20:49:08.650427Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n\n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    \n    df=pl.concat([train, test])\n    agg_df = group(df, '', CREDIT_BUREAU_B_2_AGG) \n    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.654132Z","iopub.execute_input":"2024-03-21T20:49:08.654542Z","iopub.status.idle":"2024-03-21T20:49:08.664422Z","shell.execute_reply.started":"2024-03-21T20:49:08.654509Z","shell.execute_reply":"2024-03-21T20:49:08.663127Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# **EXECUTION**","metadata":{}},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    pd.set_option('display.max_rows', 60)\n    pd.set_option('display.max_columns', 100)\n    with timer(\"Pipeline total time\"):\n        main(debug= True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.665832Z","iopub.execute_input":"2024-03-21T20:49:08.666457Z","iopub.status.idle":"2024-03-21T20:53:34.460461Z","shell.execute_reply.started":"2024-03-21T20:49:08.666422Z","shell.execute_reply":"2024-03-21T20:53:34.459388Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"base dataframe shape: (11121, 5)\nbase - done in 0s\nstatic dataframe shape: (11141, 156)\nDATAFRAME shape: (11121, 160)\nstatic - done in 6s\nstatic cb dataframe shape: (11121, 23)\nDATAFRAME shape: (11121, 182)\nstatic_cb - done in 1s\nPrevious applications depth 1 test dataframe shape: (2, 61)\nDATAFRAME shape: (11121, 242)\nPrevious applications depth 1 test - done in 11s\nPrevious applications depth 2 test dataframe shape: (4530, 2)\nDATAFRAME shape: (11121, 243)\nPrevious applications depth 2 test - done in 3s\nPerson depth 1 test dataframe shape: (11117, 28)\nDATAFRAME shape: (11121, 270)\nPerson depth 1 test - done in 5s\nPerson depth 2 test dataframe shape: (10706, 2)\nDATAFRAME shape: (11121, 271)\nPerson depth 2 test - done in 1s\nOther test dataframe shape: (2, 2)\nDATAFRAME shape: (11121, 272)\nOther test - done in 0s\nDebit card test dataframe shape: (152, 2)\nDATAFRAME shape: (11121, 273)\nDebit card test - done in 0s\nTax registry a test dataframe shape: (0, 5)\nDATAFRAME shape: (11121, 277)\nTax registry a test - done in 1s\nTax registry b test dataframe shape: (2, 5)\nDATAFRAME shape: (11121, 281)\nTax registry b test - done in 0s\nTax registry c test dataframe shape: (6592, 8)\nDATAFRAME shape: (11121, 288)\nTax registry c test - done in 1s\nagg df  (1386278, 36)\nshape: (5, 36)\n┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n│ case_id ┆ min_overdu ┆ max_overd ┆ mean_over ┆ … ┆ max_overd ┆ mean_over ┆ count_ove ┆ sum_overd │\n│ ---     ┆ eamountmax ┆ ueamountm ┆ dueamount ┆   ┆ ueamount_ ┆ dueamount ┆ rdueamoun ┆ ueamount_ │\n│ i64     ┆ datemonth_ ┆ axdatemon ┆ maxdatemo ┆   ┆ 659A      ┆ _659A     ┆ t_659A    ┆ 659A      │\n│         ┆ 36…        ┆ th_36…    ┆ nth_3…    ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n│         ┆ ---        ┆ ---       ┆ ---       ┆   ┆ f64       ┆ f64       ┆ u32       ┆ f64       │\n│         ┆ f64        ┆ f64       ┆ f64       ┆   ┆           ┆           ┆           ┆           │\n╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n│ 1938982 ┆ 2.0        ┆ 11.0      ┆ 6.5       ┆ … ┆ 0.0       ┆ 0.0       ┆ 2         ┆ 0.0       │\n│ 1000976 ┆ 9.0        ┆ 9.0       ┆ 9.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 1         ┆ 0.0       │\n│ 232672  ┆ 1.0        ┆ 4.0       ┆ 2.5       ┆ … ┆ 0.0       ┆ 0.0       ┆ 2         ┆ 0.0       │\n│ 236689  ┆ 1.0        ┆ 5.0       ┆ 3.333333  ┆ … ┆ 41.8      ┆ 13.933333 ┆ 3         ┆ 41.8      │\n│ 258378  ┆ 2.0        ┆ 11.0      ┆ 8.5       ┆ … ┆ 0.0       ┆ 0.0       ┆ 4         ┆ 0.0       │\n└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\nNumber of unique values in 'case_id' column: 1386278\nCredit bureau a 1 test dataframe shape: (7286, 36)\nDATAFRAME shape: (11121, 323)\nCredit bureau a 1 test - done in 28s\nCredit bureau b 1 test dataframe shape: (15, 6)\nDATAFRAME shape: (11121, 328)\nCredit bureau b 1 test - done in 0s\nagg df  (1385300, 31)\nshape: (5, 31)\n┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n│ case_id ┆ min_collat ┆ max_colla ┆ mean_coll ┆ … ┆ max_num_g ┆ mean_num_ ┆ count_num ┆ sum_num_g │\n│ ---     ┆ er_valueof ┆ ter_value ┆ ater_valu ┆   ┆ roup2     ┆ group2    ┆ _group2   ┆ roup2     │\n│ i64     ┆ guarantee_ ┆ ofguarant ┆ eofguaran ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n│         ┆ 11…        ┆ ee_11…    ┆ tee_1…    ┆   ┆ i64       ┆ f64       ┆ u32       ┆ i64       │\n│         ┆ ---        ┆ ---       ┆ ---       ┆   ┆           ┆           ┆           ┆           │\n│         ┆ f64        ┆ f64       ┆ f64       ┆   ┆           ┆           ┆           ┆           │\n╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n│ 200065  ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 35        ┆ 15.177419 ┆ 372       ┆ 5646      │\n│ 214338  ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 35        ┆ 10.954545 ┆ 528       ┆ 5784      │\n│ 211998  ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 35        ┆ 12.5      ┆ 288       ┆ 3600      │\n│ 950499  ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 35        ┆ 15.1      ┆ 60        ┆ 906       │\n│ 1753548 ┆ null       ┆ null      ┆ null      ┆ … ┆ 11        ┆ 5.5       ┆ 12        ┆ 66        │\n└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\nNumber of unique values in 'case_id' column: 1385300\nCredit bureau a 1 test dataframe shape: (7260, 31)\nDATAFRAME shape: (11121, 358)\nCredit bureau a 2 test - done in 51s\nCredit bureau b 2 test dataframe shape: (15, 2)\nDATAFRAME shape: (11121, 359)\nCredit bureau b 2 test - done in 0s\nDATAFRAME shape: (11121, 147)\nFeature engineering / preprocessing - done in 0s\nDataFrame Shape: (11121, 147)\n------------------------------------------------------------\nColumn Name                                        Data Type                      NaN Percentage      \n------------------------------------------------------------\ncase_id                                            Int64                          0.00%\nWEEK_NUM                                           Int64                          0.00%\ntarget                                             Int64                          0.09%\nannuity_780A                                       Float64                        0.00%\napplicationscnt_867L                               Float64                        0.00%\nbankacctype_710L                                   String                         18.50%\nclientscnt12m_3712952L                             Float64                        0.00%\nclientscnt6m_3712949L                              Float64                        0.00%\ncredamount_770A                                    Float64                        0.00%\ncredtype_322L                                      String                         0.00%\ncurrdebt_22A                                       Float64                        0.00%\ncurrdebtcredtyperange_828A                         Float64                        0.00%\ndisbursedcredamount_1113A                          Float64                        0.00%\ndisbursementtype_67L                               String                         0.00%\ndownpmt_116A                                       Float64                        0.00%\neir_270L                                           Float64                        15.46%\nhomephncnt_628L                                    Float64                        0.00%\ninittransactionamount_650A                         Float64                        84.54%\ninittransactioncode_186L                           String                         0.00%\ninterestrate_311L                                  Float64                        15.46%\nisbidproduct_1095L                                 Boolean                        0.00%\nlastapplicationdate_877D                           Int64                          59.19%\nlastapprcommoditycat_1041M                         String                         0.00%\nlastcancelreason_561M                              String                         0.00%\nlastrejectcredamount_222A                          Float64                        65.47%\nlastrejectdate_50D                                 Int64                          65.47%\nlastrejectreason_759M                              String                         0.00%\nlastrejectreasonclient_4145040M                    String                         0.00%\nlastst_736L                                        String                         59.19%\nmaxannuity_159A                                    Float64                        58.71%\nmaxdebt4_972A                                      Float64                        58.71%\nmaxdpdfrom6mto36m_3546853P                         Float64                        81.71%\nmaxdpdlast12m_727P                                 Float64                        58.71%\nmaxdpdlast24m_143P                                 Float64                        58.71%\nmaxdpdlast3m_392P                                  Float64                        58.71%\nmaxdpdlast6m_474P                                  Float64                        58.71%\nmaxdpdlast9m_1059P                                 Float64                        58.71%\nmaxdpdtolerance_374P                               Float64                        58.71%\nmobilephncnt_593L                                  Float64                        0.00%\nnumcontrs3months_479L                              Float64                        0.00%\nnuminstls_657L                                     Float64                        0.00%\nnumrejects9m_859L                                  Float64                        0.00%\npmtnum_254L                                        Float64                        14.35%\nprice_1097A                                        Float64                        88.00%\nsellerplacescnt_216L                               Float64                        0.00%\ntotaldebt_9A                                       Float64                        0.00%\ntotalsettled_863A                                  Float64                        0.00%\ntwobodfilling_608L                                 String                         0.09%\nbirthdate_574D                                     Int64                          5.85%\ndateofbirth_337D                                   Int64                          34.70%\ndays120_123L                                       Float64                        34.70%\ndays180_256L                                       Float64                        34.70%\ndays30_165L                                        Float64                        34.70%\ndays360_512L                                       Float64                        34.70%\ndays90_310L                                        Float64                        34.70%\neducation_1103M                                    String                         5.22%\nnumberofqueries_373L                               Float64                        34.70%\npmtaverage_3A                                      Float64                        66.76%\npmtscount_423L                                     Float64                        39.65%\npmtssum_45A                                        Float64                        39.65%\nsecondquarter_766L                                 Float64                        34.70%\nthirdquarter_1082L                                 Float64                        34.70%\nmax_birth_259D                                     Int64                          0.04%\nmax_empl_employedfrom_271D                         Int64                          34.63%\nmean_empl_employedfrom_271D                        Int64                          34.63%\nmin_empl_employedfrom_271D                         Int64                          34.63%\nmax_incometype_1044T                               String                         0.04%\nmin_incometype_1044T                               String                         0.04%\nmin_mainoccupationinc_384A                         Float64                        0.04%\nmax_mainoccupationinc_384A                         Float64                        0.04%\ncount_mainoccupationinc_384A                       UInt32                         0.04%\nsum_mainoccupationinc_384A                         Float64                        0.04%\nmax_relationshiptoclient_415T                      String                         0.24%\nmin_relationshiptoclient_415T                      String                         0.24%\nmax_relationshiptoclient_642T                      String                         0.24%\nmin_relationshiptoclient_642T                      String                         0.24%\nmin_language1_981M                                 String                         0.04%\nmax_language1_981M                                 String                         0.04%\nmax_familystate_447L                               String                         0.05%\nmin_familystate_447L                               String                         0.05%\nmax_sex_738L                                       String                         0.04%\nmin_sex_738L                                       String                         0.04%\ncount_num_group1_person2                           UInt32                         3.73%\nmin_pmtamount_36A                                  Float64                        40.72%\nmean_pmtamount_36A                                 Float64                        40.72%\nmax_pmtamount_36A                                  Float64                        40.72%\nmean_processingdate_168D                           Int64                          40.72%\nmin_processingdate_168D                            Int64                          40.72%\nmax_processingdate_168D                            Int64                          40.72%\ncount_num_group1_tax_registry_c                    UInt32                         40.72%\nmin_overdueamountmaxdatemonth_365T                 Float64                        35.73%\nmax_overdueamountmaxdatemonth_365T                 Float64                        35.73%\nmean_overdueamountmaxdatemonth_365T                Float64                        35.73%\ncount_overdueamountmaxdatemonth_365T               UInt32                         34.48%\nsum_overdueamountmaxdatemonth_365T                 Float64                        34.48%\nmin_num_group1                                     Int64                          34.48%\nmax_num_group1                                     Int64                          34.48%\nmean_num_group1                                    Float64                        34.48%\ncount_num_group1_cb_a_1                            UInt32                         34.48%\nsum_num_group1                                     Int64                          34.48%\ncount_dpdmax_757P                                  UInt32                         34.48%\nsum_dpdmax_757P                                    Float64                        34.48%\nmin_instlamount_768A                               Float64                        53.90%\nmax_instlamount_768A                               Float64                        53.90%\nmean_instlamount_768A                              Float64                        53.90%\ncount_instlamount_768A                             UInt32                         34.48%\nsum_instlamount_768A                               Float64                        34.48%\nmin_outstandingamount_362A                         Float64                        62.61%\nmax_outstandingamount_362A                         Float64                        62.61%\nmean_outstandingamount_362A                        Float64                        62.61%\ncount_outstandingamount_362A                       UInt32                         34.48%\nsum_outstandingamount_362A                         Float64                        34.48%\ncount_overdueamount_31A                            UInt32                         34.48%\nsum_overdueamount_31A                              Float64                        34.48%\nmin_overdueamount_659A                             Float64                        35.82%\nmax_overdueamount_659A                             Float64                        35.82%\nmean_overdueamount_659A                            Float64                        35.82%\ncount_overdueamount_659A                           UInt32                         34.48%\nsum_overdueamount_659A                             Float64                        34.48%\nmin_collater_valueofguarantee_1124L                Float64                        35.71%\nmax_collater_valueofguarantee_1124L                Float64                        35.71%\nmean_collater_valueofguarantee_1124L               Float64                        35.71%\ncount_collater_valueofguarantee_1124L              UInt32                         34.72%\nsum_collater_valueofguarantee_1124L                Float64                        34.72%\ncount_collater_valueofguarantee_876L               UInt32                         34.72%\nsum_collater_valueofguarantee_876L                 Float64                        34.72%\nmin_pmts_year_1139T                                Float64                        35.71%\nmax_pmts_year_1139T                                Float64                        35.71%\nmean_pmts_year_1139T                               Float64                        35.71%\ncount_pmts_year_1139T                              UInt32                         34.72%\nsum_pmts_year_1139T                                Float64                        34.72%\ncount_pmts_overdue_1152A                           UInt32                         34.72%\nsum_pmts_overdue_1152A                             Float64                        34.72%\nmin_num_group1_cb_a_2                              Int64                          34.72%\nmax_num_group1_cb_a_2                              Int64                          34.72%\nmean_num_group1_cb_a_2                             Float64                        34.72%\ncount_num_group1_cb_a_2                            UInt32                         34.72%\nsum_num_group1_cb_a_2                              Int64                          34.72%\nmin_num_group2                                     Int64                          34.72%\nmax_num_group2                                     Int64                          34.72%\nmean_num_group2                                    Float64                        34.72%\ncount_num_group2                                   UInt32                         34.72%\nsum_num_group2                                     Int64                          34.72%\nratio_queries_30                                   Float64                        34.70%\nratio_queries_90                                   Float64                        34.70%\nratio_queries_120                                  Float64                        34.70%\nratio_queries_180                                  Float64                        34.70%\nTrain/valid shape: (11111, 147), test shape: (10, 147)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c937f68ad7544226aa26ab4d0e50e409"}},"metadata":{}},{"name":"stdout","text":"Fold  1 AUC : 0.692426. Elapsed time: 13.18 seconds. Remaining time: 118.63 seconds.\nFold  2 AUC : 0.699683. Elapsed time: 28.70 seconds. Remaining time: 114.80 seconds.\nFold  3 AUC : 0.677344. Elapsed time: 44.32 seconds. Remaining time: 103.41 seconds.\nFold  4 AUC : 0.639030. Elapsed time: 59.85 seconds. Remaining time: 89.78 seconds.\nFold  5 AUC : 0.621853. Elapsed time: 75.24 seconds. Remaining time: 75.24 seconds.\nFold  6 AUC : 0.708204. Elapsed time: 90.57 seconds. Remaining time: 60.38 seconds.\nFold  7 AUC : 0.676143. Elapsed time: 106.35 seconds. Remaining time: 45.58 seconds.\nFold  8 AUC : 0.638784. Elapsed time: 121.67 seconds. Remaining time: 30.42 seconds.\nFold  9 AUC : 0.724296. Elapsed time: 137.26 seconds. Remaining time: 15.25 seconds.\nFold 10 AUC : 0.661630. Elapsed time: 152.64 seconds. Remaining time: 0.00 seconds.\nFull AUC score 0.673902\nGini Score of the valid set: 0.3080170936940813\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/511156292.py:80: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['PREDICTIONS'] = oof_preds.copy()\n/tmp/ipykernel_33/511156292.py:81: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['target'] = df['target'].copy()\n","output_type":"stream"},{"name":"stdout","text":"Model training - done in 154s\nFeature importance assesment - done in 0s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ? models/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64531779f424a25a12b3eecf8293ce7"}},"metadata":{}},{"name":"stdout","text":"Submission file has been created.\nSubmission - done in 0s\nNOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\nPipeline total time - done in 266s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}