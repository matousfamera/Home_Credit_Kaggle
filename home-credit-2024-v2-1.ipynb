{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom glob import glob\nfrom pathlib import Path\nfrom datetime import datetime\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score \nfrom sklearn.metrics import roc_curve, auc\nfrom tqdm.notebook import tqdm\nimport joblib\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:05.892237Z","iopub.execute_input":"2024-03-21T20:49:05.893248Z","iopub.status.idle":"2024-03-21T20:49:08.125126Z","shell.execute_reply.started":"2024-03-21T20:49:05.893193Z","shell.execute_reply":"2024-03-21T20:49:08.124297Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **CONFIGURATION**","metadata":{}},{"cell_type":"code","source":"# GENERAL CONFIGURATIONS\nNUM_THREADS = 4\nDATA_DIRECTORY = \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/\"\nSUBMISSION_SUFIX = \"_model2_0\"\n# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\nGENERATE_SUBMISSION_FILES = True\nEVALUATE_VALIDATION_SET = True\nSHOW_REPORT = True\nSTRATIFIED_KFOLD = True\nRANDOM_SEED = 737851\nNUM_FOLDS = 10\nEARLY_STOPPING = 100\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nLIGHTGBM_PARAMS = {\n    'boosting_type': 'goss',\n    'n_estimators': 1000,\n    'learning_rate': 0.005134,\n    'num_leaves': 54,\n    'max_depth': 10,\n    'subsample_for_bin': 240000,\n    'reg_alpha': 0.436193,\n    'reg_lambda': 0.479169,\n    'colsample_bytree': 0.508716,\n    'min_split_gain': 0.024766,\n    'subsample': 1,\n    'is_unbalance': False,\n    'silent':-1,\n    'verbose':-1,\n    \n}","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.127008Z","iopub.execute_input":"2024-03-21T20:49:08.127645Z","iopub.status.idle":"2024-03-21T20:49:08.134702Z","shell.execute_reply.started":"2024-03-21T20:49:08.127610Z","shell.execute_reply":"2024-03-21T20:49:08.133549Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Set aggregations","metadata":{}},{"cell_type":"code","source":"# AGGREGATIONS\n\n\n\nAPPLPREV1_AGG = {\n\n    'cancelreason_3545846M' : ['min','mean','max'],\n    'employedfrom_700D' : ['max','mean','min'],\n    'dtlastpmt_581D' : ['max','min','mean'],\n    'pmtnum_8L' : ['mean','min','max'],\n    'tenor_203L' : ['mean','min','max'],\n    'firstnonzeroinstldate_307D':['max','mean','min'],\n    'mainoccupationinc_437A' : ['min','max','mean','count','sum'],\n    'annuity_853A': ['min','max','mean','count','sum'],\n    'maxdpdtolerance_577P':['mean','max','min'],\n    'dtlastpmtallstes_3545839D':['max','mean','min'],\n    'credamount_590A':['min','mean','max'],\n    'outstandingdebt_522A':['mean','max','min'],\n    'dtlastpmt_581D':['min','mean','max'],\n    'creationdate_885D':['max','min','mean'],\n    'rejectreason_755M':['min'],\n    'currdebt_94A':['max'],\n    'approvaldate_319D':['max','min','mean'],\n    'dateactivated_425D':['mean','min','max'],\n    'familystate_726L':['max','min','mean'],\n    'inittransactioncode_279L':['min'],\n    'credtype_587L':['min'],\n    'education_1138M':['min','max','mean'],\n    'num_group1':['count']\n    \n}\nAPPLPREV2_AGG = {\n    'num_group1':['count']\n    \n}\nPERSON1_AGG={\n    'birth_259D': ['max'],\n    'empl_employedfrom_271D':['max','mean','min'],\n    'incometype_1044T':['max','min','mean'],\n    'mainoccupationinc_384A':['min','max','mean','count','sum'],\n    'relationshiptoclient_415T':['max','min','mean'],\n    'relationshiptoclient_642T':['max','mean','min'],\n    'language1_981M':['min','max'],\n    'familystate_447L':['max','min','mean'],\n    'sex_738L':['max','min','mean'],\n    'num_group1':['count']\n    \n    \n}\nPERSON2_AGG={\n    'num_group1':['count']\n}\nOTHER_AGG={\n    'num_group1':['count']\n}\nDEBITCARD_AGG={\n    'num_group1':['count']\n}\nTAX_REGISTRY_A_AGG={\n    'amount_4527230A': ['max','mean','min'],\n    'num_group1':['count']\n    \n}\nTAX_REGISTRY_B_AGG={\n    \n    'amount_4917619A':['min','mean','max'],\n    'num_group1':['count']\n}\nTAX_REGISTRY_C_AGG={\n    \n    'pmtamount_36A':['min','mean','max'],\n    'processingdate_168D':['mean','min','max'],\n    'num_group1':['count']\n}\nCREDIT_BUREAU_A_1_AGG={\n    \n    'overdueamountmaxdatemonth_365T':['min','max','mean','count','sum'],\n    'num_group1':['min','max','mean','count','sum'],\n    'dpdmax_757P':['min','max','mean','count','sum'],\n    'instlamount_768A':['min','max','mean','count','sum'],\n    'outstandingamount_362A':['min','max','mean','count','sum'],\n    'overdueamount_31A':['min','max','mean','count','sum'],\n    'overdueamount_659A':['min','max','mean','count','sum']\n    \n}\nCREDIT_BUREAU_B_1_AGG={\n    'num_group1':['min','max','mean','count','sum']\n}\nCREDIT_BUREAU_A_2_AGG={\n    \n    'collater_valueofguarantee_1124L':['min','max','mean','count','sum'],\n    'collater_valueofguarantee_876L':['min','max','mean','count','sum'],\n    'pmts_year_1139T':['min','max','mean','count','sum'],\n    'pmts_overdue_1152A':['min','max','mean','count','sum'],\n    \n    \n    'num_group1':['min','max','mean','count','sum'],\n    'num_group2':['min','max','mean','count','sum']\n}\nCREDIT_BUREAU_B_2_AGG={\n    'num_group1':['count']\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.136466Z","iopub.execute_input":"2024-03-21T20:49:08.137157Z","iopub.status.idle":"2024-03-21T20:49:08.152079Z","shell.execute_reply.started":"2024-03-21T20:49:08.137124Z","shell.execute_reply":"2024-03-21T20:49:08.151209Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **MAIN FUNCTION**","metadata":{}},{"cell_type":"code","source":"def main(debug= False):\n    num_rows = 11111 if debug else None\n    with timer(\"base\"):\n        \n        df = get_base(DATA_DIRECTORY, num_rows=num_rows)\n        #df=df.to_pandas()\n        print(\"base dataframe shape:\", df.shape)\n        #df=reduce_mem_usage(df)\n        \n        \n    with timer(\"static\"):\n       \n        df_static = get_static(DATA_DIRECTORY, num_rows=num_rows)\n        df = df.join(df_static, on='case_id', how='left', suffix='_static')\n        print(\"static dataframe shape:\", df_static.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        \n        del df_static\n        gc.collect()\n\n    with timer(\"static_cb\"):\n       \n        df_static_cb = get_static_cb(DATA_DIRECTORY, num_rows=num_rows)\n        df = df.join(df_static_cb, on='case_id', how='left', suffix='_static_cb')\n        print(\"static cb dataframe shape:\", df_static_cb.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_static_cb\n        gc.collect()\n\n    with timer(\"Previous applications depth 1 test\"):\n       \n        df_applprev1 = get_applprev1(DATA_DIRECTORY, num_rows=num_rows)\n        df_applprev1 = df_applprev1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n        print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_applprev1\n        gc.collect()\n\n    with timer(\"Previous applications depth 2 test\"):\n       \n        df_applprev2 = get_applprev2(DATA_DIRECTORY, num_rows=num_rows)\n        df_applprev2 = df_applprev2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_applprev2, on='case_id', how='left', suffix='_applprev2')\n        print(\"Previous applications depth 2 test dataframe shape:\", df_applprev2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_applprev2\n        gc.collect()\n\n    with timer(\"Person depth 1 test\"):\n       \n        df_person1 = get_person1(DATA_DIRECTORY, num_rows=num_rows)\n        df_person1 = df_person1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_person1, on='case_id', how='left', suffix='_person1')\n        print(\"Person depth 1 test dataframe shape:\", df_person1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_person1\n        gc.collect()\n\n    with timer(\"Person depth 2 test\"):\n     \n        df_person2 = get_person2(DATA_DIRECTORY, num_rows=num_rows)\n        df_person2 = df_person2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_person2, on='case_id', how='left', suffix='_person2')\n        print(\"Person depth 2 test dataframe shape:\", df_person2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_person2\n        gc.collect()\n\n    with timer(\"Other test\"):\n        \n        df_other = get_other(DATA_DIRECTORY, num_rows=num_rows)\n        df_other = df_other.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_other, on='case_id', how='left', suffix='_other')\n        print(\"Other test dataframe shape:\", df_other.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_other\n        gc.collect()\n\n    with timer(\"Debit card test\"):\n      \n        df_debitcard = get_debitcard(DATA_DIRECTORY, num_rows=num_rows)\n        df_debitcard = df_debitcard.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_debitcard, on='case_id', how='left', suffix='_debitcard')\n        print(\"Debit card test dataframe shape:\", df_debitcard.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_debitcard\n        gc.collect()\n\n    with timer(\"Tax registry a test\"):\n        \n        df_tax_registry_a = get_tax_registry_a(DATA_DIRECTORY, num_rows=num_rows)\n        df_tax_registry_a = df_tax_registry_a.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_tax_registry_a, on='case_id', how='left', suffix='_tax_registry_a')\n        print(\"Tax registry a test dataframe shape:\", df_tax_registry_a.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_tax_registry_a\n        gc.collect()\n\n    with timer(\"Tax registry b test\"):\n       \n        df_tax_registry_b = get_tax_registry_b(DATA_DIRECTORY, num_rows=num_rows)\n        df_tax_registry_b = df_tax_registry_b.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_tax_registry_b, on='case_id', how='left', suffix='_tax_registry_b')\n        print(\"Tax registry b test dataframe shape:\", df_tax_registry_b.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_tax_registry_b\n        gc.collect()\n\n    with timer(\"Tax registry c test\"):\n        \n        df_tax_registry_c = get_tax_registry_c(DATA_DIRECTORY, num_rows=num_rows)\n        df_tax_registry_c = df_tax_registry_c.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_tax_registry_c, on='case_id', how='left', suffix='_tax_registry_c')\n        print(\"Tax registry c test dataframe shape:\", df_tax_registry_c.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_tax_registry_c\n        gc.collect()\n    \n        \n \n    with timer(\"Credit bureau a 1 test\"):\n\n        df_credit_bureau_a_1 = get_credit_bureau_a_1(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_a_1 = df_credit_bureau_a_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_a_1, on='case_id', how='left', suffix='_cb_a_1')\n        print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_a_1\n        gc.collect()\n    with timer(\"Credit bureau b 1 test\"):\n       \n        df_credit_bureau_b_1 = get_credit_bureau_b_1(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_b_1 = df_credit_bureau_b_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_b_1, on='case_id', how='left', suffix='_cb_b_1')\n        print(\"Credit bureau b 1 test dataframe shape:\", df_credit_bureau_b_1.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_b_1\n        gc.collect()\n\n\n    \n    \n    with timer(\"Credit bureau a 2 test\"):\n       \n        df_credit_bureau_a_2 = get_credit_bureau_a_2(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_a_2 = df_credit_bureau_a_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_a_2, on='case_id', how='left', suffix='_cb_a_2')\n        print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_a_2\n        gc.collect()\n    \n    with timer(\"Credit bureau b 2 test\"):\n       \n        df_credit_bureau_b_2 = get_credit_bureau_b_2(DATA_DIRECTORY, num_rows=num_rows)\n        df_credit_bureau_b_2 = df_credit_bureau_b_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n        df = df.join(df_credit_bureau_b_2, on='case_id', how='left', suffix='_cb_b_2')\n        print(\"Credit bureau b 2 test dataframe shape:\", df_credit_bureau_b_2.shape)\n        print(\"DATAFRAME shape:\", df.shape)\n        del df_credit_bureau_b_2\n        gc.collect()\n    with timer(\"Feature engineering / preprocessing\"): \n     \n        df=feature_engineering(df)\n        print(\"DATAFRAME shape:\", df.shape)\n   \n    with timer(\"Model training\"):\n        get_info(df)\n        df, cat_cols = to_pandas(df)\n        \n        model = kfold_lightgbm_sklearn(df, cat_cols)\n       \n    with timer(\"Feature importance assesment\"):\n      \n        get_features_importances(df, model)\n        if SHOW_REPORT:\n            make_report(num_rows, df, model)\n        \n    with timer(\"Submission\"):\n    \n        if generate_submission_file(df, model):\n         \n            print(\"Submission file has been created.\")\n        \n    \n  \n    \n    print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n    \n    return df, model","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.154918Z","iopub.execute_input":"2024-03-21T20:49:08.155594Z","iopub.status.idle":"2024-03-21T20:49:08.270191Z","shell.execute_reply.started":"2024-03-21T20:49:08.155560Z","shell.execute_reply":"2024-03-21T20:49:08.269273Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY FUNCTIONS**","metadata":{}},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"class Pipeline:\n    @staticmethod\n    \n    \n    # Sets datatypes accordingly\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))            \n\n        return df\n    \n    \n    # Changes the values of all date columns. The result will not be a date but number of days since date_decision.\n    @staticmethod\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n                df = df.with_columns(pl.col(col).dt.total_days())\n                \n        df = df.drop(\"date_decision\", \"MONTH\")\n\n        return df\n    \n    # It drops columns with a lot of NaN values.\n    @staticmethod\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n\n                if isnull > 0.95:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.271558Z","iopub.execute_input":"2024-03-21T20:49:08.272091Z","iopub.status.idle":"2024-03-21T20:49:08.286082Z","shell.execute_reply.started":"2024-03-21T20:49:08.272014Z","shell.execute_reply":"2024-03-21T20:49:08.285078Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_info(dataframe):\n    \"\"\"\n    View data types, shape, and calculate the percentage of NaN (missing) values in each column\n    of a Polars DataFrame simultaneously.\n    \n    Parameters:\n    dataframe (polars.DataFrame): The DataFrame to analyze.\n    \n    Returns:\n    None\n    \"\"\"\n    # Print DataFrame shape\n    print(\"DataFrame Shape:\", dataframe.shape)\n    print(\"-\" * 60)\n    \n    # Print column information\n    print(\"{:<50} {:<30} {:<20}\".format(\"Column Name\", \"Data Type\", \"NaN Percentage\"))\n    print(\"-\" * 60)\n    \n    # Total number of rows in the DataFrame\n    total_rows = len(dataframe)\n    \n    # Iterate over each column\n    for column in dataframe.columns:\n        # Get the data type of the column\n        dtype = str(dataframe[column].dtype)\n        \n        # Count the number of NaN values in the column\n        nan_count = dataframe[column].null_count()\n        \n        # Calculate the percentage of NaN values\n        nan_percentage = (nan_count / total_rows) * 100\n        \n        # Print the information\n        print(\"{:<50} {:<30} {:.2f}%\".format(column, dtype, nan_percentage))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.287419Z","iopub.execute_input":"2024-03-21T20:49:08.288220Z","iopub.status.idle":"2024-03-21T20:49:08.299687Z","shell.execute_reply.started":"2024-03-21T20:49:08.288190Z","shell.execute_reply":"2024-03-21T20:49:08.298866Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    \n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    \n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    \n    return df_data, cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.300946Z","iopub.execute_input":"2024-03-21T20:49:08.301215Z","iopub.status.idle":"2024-03-21T20:49:08.310164Z","shell.execute_reply.started":"2024-03-21T20:49:08.301193Z","shell.execute_reply":"2024-03-21T20:49:08.309390Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.311231Z","iopub.execute_input":"2024-03-21T20:49:08.311519Z","iopub.status.idle":"2024-03-21T20:49:08.323119Z","shell.execute_reply.started":"2024-03-21T20:49:08.311496Z","shell.execute_reply":"2024-03-21T20:49:08.322322Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.324466Z","iopub.execute_input":"2024-03-21T20:49:08.324962Z","iopub.status.idle":"2024-03-21T20:49:08.334017Z","shell.execute_reply.started":"2024-03-21T20:49:08.324861Z","shell.execute_reply":"2024-03-21T20:49:08.333275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    \n\n    temp=base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\").mean()\n   \n    week_nums_to_drop = temp[(temp[\"target\"] == 0) | (temp[\"target\"] == 1)].index.tolist()\n\n    base_filtered = base[~base[\"WEEK_NUM\"].isin(week_nums_to_drop)]\n\n    # Apply the aggregator\n    gini_in_time = base_filtered.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n        .sort_values(\"WEEK_NUM\") \\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]] \\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n\n    \n\n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a * x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.nanmean(gini_in_time)  # Use np.nanmean to handle NaN values\n    \n    if SHOW_REPORT:\n        # Display the plot of x on y\n        plt.figure(figsize=(8, 6))\n        plt.plot(x, y, 'o', label='Gini in Time')\n        plt.plot(x, y_hat, '-', label='Fitted line (slope={:.2f}, intercept={:.2f})'.format(a, b))\n        plt.xlabel('Week')\n        plt.ylabel('Gini in Time')\n        plt.title('Gini Stability Over Time')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    \n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.338438Z","iopub.execute_input":"2024-03-21T20:49:08.338757Z","iopub.status.idle":"2024-03-21T20:49:08.348502Z","shell.execute_reply.started":"2024-03-21T20:49:08.338731Z","shell.execute_reply":"2024-03-21T20:49:08.347786Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Report function","metadata":{}},{"cell_type":"code","source":"def make_report(num_rows, data, model):\n    # 1. time\n    current_time = datetime.now()\n    # Print the current time\n    print(\"Current Time:\", current_time)\n    \n    # 2. specification\n    if not num_rows:\n        print(\"The notebook was run in full mode.\")\n    else:\n        print(\"The notebook was run in debug mode. Number of rows: \" + str(num_rows))\n    \n    # 3. features\n    feat_importances_df = model.get_features_importances_df(data)\n    feat_importances_df['gain'] = feat_importances_df['gain'].round(0)\n    print(feat_importances_df.shape)\n    \n    predictions = pd.Series(model.get_predictions())\n   \n    numerical_columns = data.select_dtypes(include=['int', 'float']).columns\n\n    # Compute correlations of each numerical column with 'PREDICTIONS'\n    correlations = {}\n    \n    # Compute correlations of each numerical column with 'feat'\n    for column in numerical_columns:\n        correlations[column] = predictions.corr(data[column])\n\n    # Create a new DataFrame with 'features' and 'correlation' columns\n    correlation_df = pd.DataFrame(list(correlations.items()), columns=['features', 'correlation'])\n\n    # Round the correlation numbers to three decimal places\n    correlation_df['correlation'] = correlation_df['correlation'].round(3)\n\n    # Merge feat_importances_df and correlation_df on 'feature'\n    combined_df = pd.merge(feat_importances_df, correlation_df, left_on=\"feature\", right_on='features', how='left')\n\n    # Handle categorical features with no correlation\n    combined_df['correlation'] = combined_df['correlation'].fillna(value=np.nan)\n    \n\n    # Compute and add valid percentage for each feature\n    valid_percentage = (data[0:-10].count() / len(data[0:-10]))\n    valid_percentage = valid_percentage.round(3)\n    combined_df['valid_percentage'] = combined_df['feature'].map(valid_percentage)\n\n    # Print the combined_df DataFrame\n    print(combined_df.to_string(index=False))\n    print()\n    roc_score=roc_auc_score(data['target'][0:-10],predictions)\n    print(\"ROC score: \",roc_score)\n\n    # Compute false positive rate, true positive rate, and thresholds for ROC curve\n    fpr, tpr, thresholds = roc_curve(data['target'][0:-10], predictions)\n\n    # Plot ROC curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_score)\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.349818Z","iopub.execute_input":"2024-03-21T20:49:08.350382Z","iopub.status.idle":"2024-03-21T20:49:08.362440Z","shell.execute_reply.started":"2024-03-21T20:49:08.350343Z","shell.execute_reply":"2024-03-21T20:49:08.361508Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#  **MODEL**","metadata":{}},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n        # Use tqdm to create a progress bar during the prediction\n        with tqdm(total=len(self.estimators), desc=\"Predicting\", unit=\" models\") as pbar:\n            for i, estimator in enumerate(self.estimators):\n                y_preds[i] = estimator.predict_proba(X)\n                pbar.update(1)  # Update the progress bar\n        return np.mean(y_preds, axis=0)\n\n    \n    def get_splits(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for x in self.estimators:\n            feature_importances_list.append(x.booster_.feature_importance(importance_type='split'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None   \n        return combined_importances\n    \n    \n    def get_gains(self, aggregation_method=np.mean):\n        \n        feature_importances_list=[]\n        for model in self.estimators:\n            feature_importances_list.append(x.booster_.feature_importance(importance_type='gain'))\n            \n        # Aggregate feature importances across all models\n        if all(importances is not None for importances in feature_importances_list):\n            combined_importances = aggregation_method(feature_importances_list, axis=0)\n        else:\n            combined_importances = None\n              \n        return combined_importances\n    \n    def get_features_importances_df(self, df):\n        del_features = ['target', 'case_id']\n        predictors = list(filter(lambda v: v not in del_features, df.columns))\n        importance_df = pd.DataFrame()\n        eval_results = dict()\n        for model in self.estimators:\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = predictors\n            fold_importance[\"gain\"] = model.booster_.feature_importance(importance_type='gain')\n            fold_importance[\"split\"] = model.booster_.feature_importance(importance_type='split')\n            importance_df = pd.concat([importance_df, fold_importance], axis=0)\n            importance_df= importance_df.groupby('feature').mean().reset_index()\n        return importance_df\n    \n    \n    def add_predictions(self, predictions):\n        self.predictions=predictions\n        \n    def get_predictions(self):\n        return self.predictions\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.363802Z","iopub.execute_input":"2024-03-21T20:49:08.364080Z","iopub.status.idle":"2024-03-21T20:49:08.380076Z","shell.execute_reply.started":"2024-03-21T20:49:08.364056Z","shell.execute_reply":"2024-03-21T20:49:08.378524Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def kfold_lightgbm_sklearn(data, categorical_feature = None):\n    start_time = time.time()\n    df = data[data['target'].notnull()]\n    test = data[data['target'].isnull()]\n    print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n    del_features = ['target', 'case_id']\n    predictors = list(filter(lambda v: v not in del_features, df.columns))\n\n    if not STRATIFIED_KFOLD:\n        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    else:\n        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    \n        # Hold oof predictions, test predictions, feature importance and training/valid auc\n    oof_preds = np.zeros(df.shape[0])\n    \n    importance_df = pd.DataFrame()\n    eval_results = dict()\n    \n    fitted_models = []\n    with tqdm(total=NUM_FOLDS) as pbar:\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):\n            train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]\n            valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]\n\n            params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n            clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n\n\n            if not categorical_feature:\n                    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                            eval_metric='auc' )\n            else:\n                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n                        feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n\n\n            fitted_models.append(clf)\n\n            if EVALUATE_VALIDATION_SET:\n                oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n\n\n\n                # Feature importance by GAIN and SPLIT\n\n            eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n            eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n\n            elapsed_time = time.time() - start_time\n            remaining_time = elapsed_time * (NUM_FOLDS - n_fold - 1) / (n_fold + 1)\n            print('Fold %2d AUC : %.6f. Elapsed time: %.2f seconds. Remaining time: %.2f seconds.'\n                  % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx]), elapsed_time, remaining_time))\n            del clf, train_x, train_y, valid_x, valid_y\n            gc.collect()\n            pbar.update(1)\n            \n    print('Full AUC score %.6f' % roc_auc_score(df['target'], oof_preds))\n    # Get the average feature importance between folds\n    \n    \n    \n    if len(df)>0:\n        base=get_base(DATA_DIRECTORY, len(df))\n        base, cat_cols = to_pandas(base)\n        base=base[base['target'].notnull()]\n        base['score']= oof_preds\n        gini_score = gini_stability(base)\n        print(\"Gini Score of the valid set:\", gini_score)\n    \n    \n    \n    \n    # Save feature importance, test predictions and oof predictions as csv\n    if GENERATE_SUBMISSION_FILES:\n\n        # Generate oof csv\n        oof = pd.DataFrame()\n        oof['case_id'] = df['case_id'].copy()\n        df['PREDICTIONS'] = oof_preds.copy()\n        df['target'] = df['target'].copy()\n        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n        \n  \n        \n        \n    model = VotingModel(fitted_models)\n    model.add_predictions(oof_preds.copy())\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.381558Z","iopub.execute_input":"2024-03-21T20:49:08.382475Z","iopub.status.idle":"2024-03-21T20:49:08.400223Z","shell.execute_reply.started":"2024-03-21T20:49:08.382436Z","shell.execute_reply":"2024-03-21T20:49:08.399192Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# **SUBMISSION**","metadata":{}},{"cell_type":"code","source":"def generate_submission_file(data, model):\n   \n    test = data[data['target'].isnull()]\n    \n    '''\n    length=len(test)\n    y_pred = pd.Series([0.5] * length,index=test['case_id'])\n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n    df_subm.to_csv(\"submission.csv\")\n    '''\n    \n    del_features = ['target', 'case_id']\n    predictors = list(filter(lambda v: v not in del_features, data.columns))\n    y_pred = pd.Series(model.predict_proba(test[predictors])[:, 1], index=test['case_id']) \n   \n    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n    df_subm = df_subm.set_index(\"case_id\")\n    df_subm[\"score\"] = y_pred\n  \n    df_subm.to_csv(\"submission.csv\")\n    \n    return True\n   \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.401436Z","iopub.execute_input":"2024-03-21T20:49:08.401819Z","iopub.status.idle":"2024-03-21T20:49:08.413325Z","shell.execute_reply.started":"2024-03-21T20:49:08.401785Z","shell.execute_reply":"2024-03-21T20:49:08.412461Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# **EVALUATE FEATURES IMPORTANCES**","metadata":{}},{"cell_type":"code","source":"def get_features_importances(data, model):\n    importance_df = model.get_features_importances_df(data)\n    mean_importance = importance_df.groupby('feature').mean().reset_index()\n    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n    mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.414664Z","iopub.execute_input":"2024-03-21T20:49:08.415074Z","iopub.status.idle":"2024-03-21T20:49:08.425152Z","shell.execute_reply.started":"2024-03-21T20:49:08.415039Z","shell.execute_reply":"2024-03-21T20:49:08.424365Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **FEATURE ENGINEERING FUNCTION**","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    \n    \n    df=df.pipe(Pipeline.handle_dates) \n    df=df.pipe(Pipeline.filter_cols)\n    \n    \n    columns_to_add = [\n        (pl.col(\"days30_165L\")/ pl.col(\"days360_512L\")).alias(\"ratio_queries_30\"),\n        ((pl.col(\"days90_310L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_90\")),\n        ((pl.col(\"days120_123L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_120\")),\n        ((pl.col(\"days180_256L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_180\")),\n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"collater_typofvalofguarant_407M\")).alias(\"sum_collater\")),\n        #((pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_active\")),\n        #((pl.col(\"collater_typofvalofguarant_407M\") + pl.col(\"sum_collater\"))).alias(\"ratio_collater_close\")),\n        #((pl.col(\"overdueamount_31A\") + pl.col(\"overdueamount_659A\")).alias(\"sum_overdue_amount\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"overdueamount_31A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_active\")),\n        #((pl.col(\"overdueamount_659A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_close\")),\n        #((pl.col(\"totalamount\")).alias(\"sum_totalcredit_contract\")),\n        #((pl.col(\"totalamount_503A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_active\")),\n        #((pl.col(\"totalamount_6A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_close\")),\n        #((pl.col(\"totaldebtoverduevalue_178A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_active\")),\n        #((pl.col(\"totaldebtoverduevalue_718A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_close\")),\n        #((pl.col(\"numberofinstls_229L\") + pl.col(\"numberofinstls_320L\")).alias(\"sum_instalments\")),\n        #((pl.col(\"numberofinstls_320L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_active\")),\n        #((pl.col(\"numberofinstls_229L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_close\"))\n    ]\n\n# Add the calculated columns to the DataFrame\n    for column in columns_to_add:\n        df = df.with_columns([column])\n        \n        \n    '''\n    df = df.with_columns(\n    'ratio_queries_30',\n    df['days30_165L'] / df['days360_512L']\n)\n    df['ratio_queries_90'] = df['days90_310L'] / df['days360_512L']\n    df['ratio_queries_120'] = df['days120_123L'] / df['days360_512L']\n    df['ratio_queries_180'] = df['days180_256L'] / df['days360_512L']\n    df['sum_collater'] = df['collater_typofvalofguarant_298M'] +    df['collater_typofvalofguarant_407M']\n    df['ratio_collater_active'] = df['collater_typofvalofguarant_298M'] +    df['sum_collater']\n    df['ratio_collater_close'] = df['collater_typofvalofguarant_407M'] +    df['sum_collater']\n    df['sum_overdue_amount'] = df['overdueamount_31A'] +    df['overdueamount_659A']\n    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['sum_overdue_amount']\n    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['sum_overdue_amount']\n    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['total_overdue_amount']\n    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['total_overdue_amount']\n    df['sum_totalcredit_contract'] = df['totalamount']\n    df['ratio_totalcredit_contract_active'] = df['totalamount_503A'] /    df['sum_totalcredit_contract']\n    df['ratio_totalcredit_contract_close'] = df['totalamount_6A'] /    df['sum_totalcredit_contract']\n    df['ratio_overdue_debt_active'] = df['totaldebtoverduevalue_178A'] /    df['totaldebt_9A']\n    df['ratio_overdue_debt_close'] = df['totaldebtoverduevalue_718A'] /    df['totaldebt_9A']\n    df['sum_instalments'] = df['numberofinstls_229L'] +    df['numberofinstls_320L']\n    df['ratio_instalments_active'] = df['numberofinstls_320L'] /    df['sum_instalments']\n    df['ratio_instalments_close'] = df['numberofinstls_229L'] /    df['sum_instalments']\n    '''\n    df=df.pipe(Pipeline.filter_cols)\n    \n    columns_to_drop=[\n     \n    \"fourthquarter_440L\",\n    \"firstquarter_103L\",\n    \"lastrejectcommoditycat_161M\",\n    \"numinstpaidearly5dobd_4499205L\",\n    \"clientscnt3m_3712950L\",\n    \"numinstpaidearly5d_1087L\",\n    \"maritalst_385M\",\n    \"maxdpdinstlnum_3546846P\",\n    \"max_credamount_590A\",\n    \"maxoutstandbalancel12m_4187113A\",\n    \"cardtype_51L\",\n    \"avginstallast24m_3658937A\",\n    \"numinstpaidearlyest_4493214L\",\n    \"count_num_group1_applprev2\",\n    \"mean_mainoccupationinc_384A\",\n    \"totinstallast1m_4525188A\",\n    \"numinstlswithdpd5_4187116L\",\n    \"clientscnt_1022L\",\n    \"numnotactivated_1143L\",\n    \"pmtcount_693L\",\n    \"numinstpaidlate1d_3546852L\",\n    \"numinstpaidearly5dest_4493211L\",\n    \"annuitynextmonth_57A\",\n    \"avgpmtlast12m_4525200A\",\n    \"maxlnamtstart6m_4525199A\",\n    \"assignmentdate_238D\",\n    \"min_outstandingdebt_522A\",\n    \"posfstqpd30lastmonth_3976962P\",\n    \"actualdpdtolerance_344P\",\n    \"applicationscnt_1086L\",\n    \"maxpmtlast3m_4525190A\",\n    \"count_num_group1_person1\",\n    \"pmtcount_4527229L\",\n    \"typesuite_864L\",\n    \"isdebitcard_729L\",\n    \"applications30d_658L\",\n    \"max_cancelreason_3545846M\",\n    \"sellerplacecnt_915L\",\n    \"posfpd30lastmonth_3976960P\",\n    \"count_num_group1_debitcard\",\n    \"numactiverelcontr_750L\",\n    \"assignmentdate_4527235D\",\n    \"clientscnt_887L\",\n    \"responsedate_1012D\",\n    \"maritalst_893M\",\n    \"applicationscnt_464L\",\n    \"numactivecredschannel_414L\",\n    \"numactivecreds_622L\",\n    \"applicationscnt_629L\",\n    \"paytype1st_925L\",\n    \"clientscnt_533L\",\n    \"posfpd10lastmonth_333P\",\n    \"clientscnt_304L\",\n    \"clientscnt_946L\",\n    \"equalitydataagreement_891L\",\n    \"clientscnt_100L\",\n    \"paytype_783L\",\n    \"commnoinclast6m_3546845L\",\n    \"clientscnt_1130L\",\n    \"clientscnt_360L\",\n    \"clientscnt_1071L\",\n    \"clientscnt_157L\",\n    \"education_88M\",\n    \"opencred_647L\",\n    \"numpmtchanneldd_318L\",\n    \"clientscnt_493L\",\n    \"clientscnt_257L\",\n    \"mastercontrelectronic_519L\",\n    \"mastercontrexist_109L\",\n    \"deferredmnthsnum_166L\",\n    \"applicationcnt_361L\",\n    \"lastrejectcommodtypec_5251769M\"\n]\n    columns_to_drop_existing = [col for col in columns_to_drop if col in df.columns]\n\n    df=df.drop(columns_to_drop_existing)\n    \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.426573Z","iopub.execute_input":"2024-03-21T20:49:08.427336Z","iopub.status.idle":"2024-03-21T20:49:08.441670Z","shell.execute_reply.started":"2024-03-21T20:49:08.427302Z","shell.execute_reply":"2024-03-21T20:49:08.440722Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **GET FUNCTIONS**","metadata":{}},{"cell_type":"code","source":"def group(df_to_agg, prefix, aggregations, aggregate_by='case_id', datatype='polars'):\n    # Create a dictionary mapping aggregation functions to their string representations\n    \n    if datatype=='polars':\n        func_mapping = {\n        'min': pl.min,\n        'max': pl.max,\n        'mean': pl.mean,\n        'sum': pl.sum,\n        'count': pl.count\n        }\n\n    # Perform the aggregation\n        agg_df = df_to_agg.group_by(aggregate_by).agg(**{\n            f\"{func}_{col}\": func_mapping[func](col) for col, funcs in aggregations.items() for func in funcs\n        })\n        '''\n        # Rename columns\n        for col, funcs in aggregations.items():\n            for func in funcs:\n                old_name = f\"{col}_{func}\"\n                new_name = f\"{prefix}{col}_{func.upper()}\"\n                agg_df = agg_df.select(pl.col(old_name).alias(new_name))\n        '''\n        return agg_df\n    \n    if datatype=='pandas':\n            # Create a dictionary mapping aggregation functions to their string representations\n        func_mapping = {\n            'min': 'min',\n            'max': 'max',\n            'mean': 'mean',\n            'sum': 'sum',\n            'count': 'count'\n        }\n\n        # Perform the aggregation\n        agg_df = df_to_agg.groupby(aggregate_by).agg(**{\n            f\"{prefix}{col}_{func.upper()}\": (col, func_mapping[func]) for col, funcs in aggregations.items() for func in funcs\n        }).reset_index()\n        \n        return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.443140Z","iopub.execute_input":"2024-03-21T20:49:08.443641Z","iopub.status.idle":"2024-03-21T20:49:08.455263Z","shell.execute_reply.started":"2024-03-21T20:49:08.443611Z","shell.execute_reply":"2024-03-21T20:49:08.454477Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### get_base()","metadata":{}},{"cell_type":"code","source":"def get_base(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    train={}\n    test={}\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet'))\n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet')).limit(num_rows) \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_base.parquet'))    \n    length=len(test)\n    nan_series=pl.Series([None] * length)\n    test = test.select(pl.col(\"*\"), nan_series.alias(\"target\"))\n    df=pl.concat([train, test])\n    df = df.with_columns(pl.col('date_decision').cast(pl.Date))\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.456486Z","iopub.execute_input":"2024-03-21T20:49:08.456945Z","iopub.status.idle":"2024-03-21T20:49:08.468231Z","shell.execute_reply.started":"2024-03-21T20:49:08.456919Z","shell.execute_reply":"2024-03-21T20:49:08.467408Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### get_static()","metadata":{}},{"cell_type":"code","source":"def get_static(path, num_rows = None):\n# Read the Parquet file using scan() method\n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes) )\n    train = (pl.concat(chunks, how=\"vertical_relaxed\")).pipe(Pipeline.filter_cols)\n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n        \n        train=df1\n        del df2\n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_static_0_*.parquet')):\n        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes) )\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    \n    \n    columns_to_keep = train.columns\n\n# Find columns in 'test' that are not in 'train'\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n\n# Drop columns from 'test' that are not in 'train'\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.469279Z","iopub.execute_input":"2024-03-21T20:49:08.469581Z","iopub.status.idle":"2024-03-21T20:49:08.481144Z","shell.execute_reply.started":"2024-03-21T20:49:08.469556Z","shell.execute_reply":"2024-03-21T20:49:08.480223Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### get_static_cb()","metadata":{}},{"cell_type":"code","source":"def get_static_cb(path, num_rows = None):\n    \n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes) \n       \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.482477Z","iopub.execute_input":"2024-03-21T20:49:08.482784Z","iopub.status.idle":"2024-03-21T20:49:08.491692Z","shell.execute_reply.started":"2024-03-21T20:49:08.482758Z","shell.execute_reply":"2024-03-21T20:49:08.490647Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev1(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev1(path, num_rows = None):\n    \n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('train/train_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n    \n    \n    if num_rows!= None:\n        df1 = train.slice(0,num_rows)\n        df2 = train.slice(num_rows,len(train))\n\n        train=df1\n        del df2   \n    \n    chunks = []\n    for path in glob(DATA_DIRECTORY+str('test/test_applprev_1_*.parquet')):\n        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n    test = pl.concat(chunks, how=\"vertical_relaxed\")\n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    df=pl.concat([train, test])\n    agg_df = group(df, '', APPLPREV1_AGG)\n    del df \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.493344Z","iopub.execute_input":"2024-03-21T20:49:08.494484Z","iopub.status.idle":"2024-03-21T20:49:08.505078Z","shell.execute_reply.started":"2024-03-21T20:49:08.494444Z","shell.execute_reply":"2024-03-21T20:49:08.504257Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### get_applprev2(DATA_DIRECTORY, num_rows=num_rows)","metadata":{}},{"cell_type":"code","source":"def get_applprev2(path, num_rows = None):\n    train={}\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes) \n       \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', APPLPREV2_AGG)\n    del df \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.506300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get_person1","metadata":{}},{"cell_type":"code","source":"def get_person1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).pipe(Pipeline.set_table_dtypes) \n      \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', PERSON1_AGG)\n    del df\n    \n    return agg_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get_person2","metadata":{}},{"cell_type":"code","source":"def get_person2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).pipe(Pipeline.set_table_dtypes)\n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_person_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', PERSON2_AGG)\n    del df\n    \n    return agg_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### other","metadata":{}},{"cell_type":"code","source":"def get_other(path, num_rows = None):\n     # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).pipe(Pipeline.set_table_dtypes) \n         \n    test = pl.read_parquet(os.path.join(path, 'test/test_other_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', OTHER_AGG)\n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.execute_input":"2024-03-21T20:49:08.543596Z","iopub.status.idle":"2024-03-21T20:49:08.552379Z","shell.execute_reply.started":"2024-03-21T20:49:08.543556Z","shell.execute_reply":"2024-03-21T20:49:08.551550Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## get_debitcard","metadata":{}},{"cell_type":"code","source":"def get_debitcard(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n     \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes) \n      \n        \n    test = pl.read_parquet(os.path.join(path, 'test/test_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', DEBITCARD_AGG)\n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.553707Z","iopub.execute_input":"2024-03-21T20:49:08.554243Z","iopub.status.idle":"2024-03-21T20:49:08.566381Z","shell.execute_reply.started":"2024-03-21T20:49:08.554215Z","shell.execute_reply":"2024-03-21T20:49:08.565568Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_a","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_a(path, num_rows = None):\n    \n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes) \n  \n    \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', TAX_REGISTRY_A_AGG)    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.567770Z","iopub.execute_input":"2024-03-21T20:49:08.568290Z","iopub.status.idle":"2024-03-21T20:49:08.580403Z","shell.execute_reply.started":"2024-03-21T20:49:08.568261Z","shell.execute_reply":"2024-03-21T20:49:08.579526Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_b","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_b(path, num_rows = None):\n    # Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', TAX_REGISTRY_B_AGG) \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.585039Z","iopub.execute_input":"2024-03-21T20:49:08.585649Z","iopub.status.idle":"2024-03-21T20:49:08.593270Z","shell.execute_reply.started":"2024-03-21T20:49:08.585582Z","shell.execute_reply":"2024-03-21T20:49:08.592288Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### get_tax_registry_c","metadata":{}},{"cell_type":"code","source":"def get_tax_registry_c(path, num_rows = None):\n     # Read the Parquet file using scan() method\n# Read the Parquet file using scan() method\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', TAX_REGISTRY_C_AGG)    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.594493Z","iopub.execute_input":"2024-03-21T20:49:08.594971Z","iopub.status.idle":"2024-03-21T20:49:08.605513Z","shell.execute_reply.started":"2024-03-21T20:49:08.594944Z","shell.execute_reply":"2024-03-21T20:49:08.604704Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_1(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    train_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_1_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    test_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    agg_df=pl.concat([train_agg_df,test_agg_df],how=\"vertical_relaxed\")\n    del train_agg_df\n    del test_agg_df\n    \n    \n    print(\"agg df \", agg_df.shape)\n    print(agg_df.head())\n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.606671Z","iopub.execute_input":"2024-03-21T20:49:08.607147Z","iopub.status.idle":"2024-03-21T20:49:08.615980Z","shell.execute_reply.started":"2024-03-21T20:49:08.607121Z","shell.execute_reply":"2024-03-21T20:49:08.615201Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#A=get_credit_bureau_a_1(DATA_DIRECTORY)\n#print(A.head())\n#del A","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.616945Z","iopub.execute_input":"2024-03-21T20:49:08.617565Z","iopub.status.idle":"2024-03-21T20:49:08.627871Z","shell.execute_reply.started":"2024-03-21T20:49:08.617538Z","shell.execute_reply":"2024-03-21T20:49:08.627068Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_1","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_1(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n        \n        \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n\n    df=pl.concat([train, test])\n    agg_df = group(df, '', CREDIT_BUREAU_B_1_AGG) \n    \n    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.628805Z","iopub.execute_input":"2024-03-21T20:49:08.629591Z","iopub.status.idle":"2024-03-21T20:49:08.637626Z","shell.execute_reply.started":"2024-03-21T20:49:08.629565Z","shell.execute_reply":"2024-03-21T20:49:08.636871Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_a_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_a_2(path, num_rows = None):\n    \n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    train_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    \n    agg_chunks=[]\n    \n    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_2_*.parquet'):\n        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n        agg_chunks.append(agg_file_df)\n        del file_df\n    \n    test_agg_df = pl.concat(agg_chunks, how=\"vertical_relaxed\") \n    for df in agg_chunks:\n        del df\n    \n    agg_df=pl.concat([train_agg_df,test_agg_df],how=\"vertical_relaxed\")\n    del train_agg_df\n    del test_agg_df\n    \n    \n    print(\"agg df \", agg_df.shape)\n    print(agg_df.head())\n    unique_count = agg_df['case_id'].n_unique()\n\n    print(\"Number of unique values in 'case_id' column:\", unique_count)\n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.638702Z","iopub.execute_input":"2024-03-21T20:49:08.639168Z","iopub.status.idle":"2024-03-21T20:49:08.651435Z","shell.execute_reply.started":"2024-03-21T20:49:08.639135Z","shell.execute_reply":"2024-03-21T20:49:08.650427Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### get_credit_bureau_b_2","metadata":{}},{"cell_type":"code","source":"def get_credit_bureau_b_2(path, num_rows = None):\n    if num_rows == None:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n   \n    else:\n        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n\n    \n    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n    \n    train = train.pipe(Pipeline.filter_cols)\n   \n    columns_to_keep = train.columns\n    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n    test = test.drop(columns_to_remove)\n    \n    df=pl.concat([train, test])\n    agg_df = group(df, '', CREDIT_BUREAU_B_2_AGG) \n    \n    del df\n    \n    return agg_df","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.654132Z","iopub.execute_input":"2024-03-21T20:49:08.654542Z","iopub.status.idle":"2024-03-21T20:49:08.664422Z","shell.execute_reply.started":"2024-03-21T20:49:08.654509Z","shell.execute_reply":"2024-03-21T20:49:08.663127Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# **EXECUTION**","metadata":{}},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    pd.set_option('display.max_rows', 60)\n    pd.set_option('display.max_columns', 100)\n    with timer(\"Pipeline total time\"):\n        main(debug= True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T20:49:08.665832Z","iopub.execute_input":"2024-03-21T20:49:08.666457Z","iopub.status.idle":"2024-03-21T20:53:34.460461Z","shell.execute_reply.started":"2024-03-21T20:49:08.666422Z","shell.execute_reply":"2024-03-21T20:53:34.459388Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"base dataframe shape: (11121, 5)\nbase - done in 0s\nstatic dataframe shape: (11141, 156)\nDATAFRAME shape: (11121, 160)\nstatic - done in 6s\nstatic cb dataframe shape: (11121, 23)\nDATAFRAME shape: (11121, 182)\nstatic_cb - done in 1s\nPrevious applications depth 1 test dataframe shape: (2, 61)\nDATAFRAME shape: (11121, 242)\nPrevious applications depth 1 test - done in 11s\nPrevious applications depth 2 test dataframe shape: (4530, 2)\nDATAFRAME shape: (11121, 243)\nPrevious applications depth 2 test - done in 3s\nPerson depth 1 test dataframe shape: (11117, 28)\nDATAFRAME shape: (11121, 270)\nPerson depth 1 test - done in 5s\nPerson depth 2 test dataframe shape: (10706, 2)\nDATAFRAME shape: (11121, 271)\nPerson depth 2 test - done in 1s\nOther test dataframe shape: (2, 2)\nDATAFRAME shape: (11121, 272)\nOther test - done in 0s\nDebit card test dataframe shape: (152, 2)\nDATAFRAME shape: (11121, 273)\nDebit card test - done in 0s\nTax registry a test dataframe shape: (0, 5)\nDATAFRAME shape: (11121, 277)\nTax registry a test - done in 1s\nTax registry b test dataframe shape: (2, 5)\nDATAFRAME shape: (11121, 281)\nTax registry b test - done in 0s\nTax registry c test dataframe shape: (6592, 8)\nDATAFRAME shape: (11121, 288)\nTax registry c test - done in 1s\nagg df  (1386278, 36)\nshape: (5, 36)\n\n case_id  min_overdu  max_overd  mean_over    max_overd  mean_over  count_ove  sum_overd \n ---      eamountmax  ueamountm  dueamount     ueamount_  dueamount  rdueamoun  ueamount_ \n i64      datemonth_  axdatemon  maxdatemo     659A       _659A      t_659A     659A      \n          36         th_36     nth_3        ---        ---        ---        ---       \n          ---         ---        ---           f64        f64        u32        f64       \n          f64         f64        f64                                                      \n\n 1938982  2.0         11.0       6.5          0.0        0.0        2          0.0       \n 1000976  9.0         9.0        9.0          0.0        0.0        1          0.0       \n 232672   1.0         4.0        2.5          0.0        0.0        2          0.0       \n 236689   1.0         5.0        3.333333     41.8       13.933333  3          41.8      \n 258378   2.0         11.0       8.5          0.0        0.0        4          0.0       \n\nNumber of unique values in 'case_id' column: 1386278\nCredit bureau a 1 test dataframe shape: (7286, 36)\nDATAFRAME shape: (11121, 323)\nCredit bureau a 1 test - done in 28s\nCredit bureau b 1 test dataframe shape: (15, 6)\nDATAFRAME shape: (11121, 328)\nCredit bureau b 1 test - done in 0s\nagg df  (1385300, 31)\nshape: (5, 31)\n\n case_id  min_collat  max_colla  mean_coll    max_num_g  mean_num_  count_num  sum_num_g \n ---      er_valueof  ter_value  ater_valu     roup2      group2     _group2    roup2     \n i64      guarantee_  ofguarant  eofguaran     ---        ---        ---        ---       \n          11         ee_11     tee_1        i64        f64        u32        i64       \n          ---         ---        ---                                                      \n          f64         f64        f64                                                      \n\n 200065   0.0         0.0        0.0          35         15.177419  372        5646      \n 214338   0.0         0.0        0.0          35         10.954545  528        5784      \n 211998   0.0         0.0        0.0          35         12.5       288        3600      \n 950499   0.0         0.0        0.0          35         15.1       60         906       \n 1753548  null        null       null         11         5.5        12         66        \n\nNumber of unique values in 'case_id' column: 1385300\nCredit bureau a 1 test dataframe shape: (7260, 31)\nDATAFRAME shape: (11121, 358)\nCredit bureau a 2 test - done in 51s\nCredit bureau b 2 test dataframe shape: (15, 2)\nDATAFRAME shape: (11121, 359)\nCredit bureau b 2 test - done in 0s\nDATAFRAME shape: (11121, 147)\nFeature engineering / preprocessing - done in 0s\nDataFrame Shape: (11121, 147)\n------------------------------------------------------------\nColumn Name                                        Data Type                      NaN Percentage      \n------------------------------------------------------------\ncase_id                                            Int64                          0.00%\nWEEK_NUM                                           Int64                          0.00%\ntarget                                             Int64                          0.09%\nannuity_780A                                       Float64                        0.00%\napplicationscnt_867L                               Float64                        0.00%\nbankacctype_710L                                   String                         18.50%\nclientscnt12m_3712952L                             Float64                        0.00%\nclientscnt6m_3712949L                              Float64                        0.00%\ncredamount_770A                                    Float64                        0.00%\ncredtype_322L                                      String                         0.00%\ncurrdebt_22A                                       Float64                        0.00%\ncurrdebtcredtyperange_828A                         Float64                        0.00%\ndisbursedcredamount_1113A                          Float64                        0.00%\ndisbursementtype_67L                               String                         0.00%\ndownpmt_116A                                       Float64                        0.00%\neir_270L                                           Float64                        15.46%\nhomephncnt_628L                                    Float64                        0.00%\ninittransactionamount_650A                         Float64                        84.54%\ninittransactioncode_186L                           String                         0.00%\ninterestrate_311L                                  Float64                        15.46%\nisbidproduct_1095L                                 Boolean                        0.00%\nlastapplicationdate_877D                           Int64                          59.19%\nlastapprcommoditycat_1041M                         String                         0.00%\nlastcancelreason_561M                              String                         0.00%\nlastrejectcredamount_222A                          Float64                        65.47%\nlastrejectdate_50D                                 Int64                          65.47%\nlastrejectreason_759M                              String                         0.00%\nlastrejectreasonclient_4145040M                    String                         0.00%\nlastst_736L                                        String                         59.19%\nmaxannuity_159A                                    Float64                        58.71%\nmaxdebt4_972A                                      Float64                        58.71%\nmaxdpdfrom6mto36m_3546853P                         Float64                        81.71%\nmaxdpdlast12m_727P                                 Float64                        58.71%\nmaxdpdlast24m_143P                                 Float64                        58.71%\nmaxdpdlast3m_392P                                  Float64                        58.71%\nmaxdpdlast6m_474P                                  Float64                        58.71%\nmaxdpdlast9m_1059P                                 Float64                        58.71%\nmaxdpdtolerance_374P                               Float64                        58.71%\nmobilephncnt_593L                                  Float64                        0.00%\nnumcontrs3months_479L                              Float64                        0.00%\nnuminstls_657L                                     Float64                        0.00%\nnumrejects9m_859L                                  Float64                        0.00%\npmtnum_254L                                        Float64                        14.35%\nprice_1097A                                        Float64                        88.00%\nsellerplacescnt_216L                               Float64                        0.00%\ntotaldebt_9A                                       Float64                        0.00%\ntotalsettled_863A                                  Float64                        0.00%\ntwobodfilling_608L                                 String                         0.09%\nbirthdate_574D                                     Int64                          5.85%\ndateofbirth_337D                                   Int64                          34.70%\ndays120_123L                                       Float64                        34.70%\ndays180_256L                                       Float64                        34.70%\ndays30_165L                                        Float64                        34.70%\ndays360_512L                                       Float64                        34.70%\ndays90_310L                                        Float64                        34.70%\neducation_1103M                                    String                         5.22%\nnumberofqueries_373L                               Float64                        34.70%\npmtaverage_3A                                      Float64                        66.76%\npmtscount_423L                                     Float64                        39.65%\npmtssum_45A                                        Float64                        39.65%\nsecondquarter_766L                                 Float64                        34.70%\nthirdquarter_1082L                                 Float64                        34.70%\nmax_birth_259D                                     Int64                          0.04%\nmax_empl_employedfrom_271D                         Int64                          34.63%\nmean_empl_employedfrom_271D                        Int64                          34.63%\nmin_empl_employedfrom_271D                         Int64                          34.63%\nmax_incometype_1044T                               String                         0.04%\nmin_incometype_1044T                               String                         0.04%\nmin_mainoccupationinc_384A                         Float64                        0.04%\nmax_mainoccupationinc_384A                         Float64                        0.04%\ncount_mainoccupationinc_384A                       UInt32                         0.04%\nsum_mainoccupationinc_384A                         Float64                        0.04%\nmax_relationshiptoclient_415T                      String                         0.24%\nmin_relationshiptoclient_415T                      String                         0.24%\nmax_relationshiptoclient_642T                      String                         0.24%\nmin_relationshiptoclient_642T                      String                         0.24%\nmin_language1_981M                                 String                         0.04%\nmax_language1_981M                                 String                         0.04%\nmax_familystate_447L                               String                         0.05%\nmin_familystate_447L                               String                         0.05%\nmax_sex_738L                                       String                         0.04%\nmin_sex_738L                                       String                         0.04%\ncount_num_group1_person2                           UInt32                         3.73%\nmin_pmtamount_36A                                  Float64                        40.72%\nmean_pmtamount_36A                                 Float64                        40.72%\nmax_pmtamount_36A                                  Float64                        40.72%\nmean_processingdate_168D                           Int64                          40.72%\nmin_processingdate_168D                            Int64                          40.72%\nmax_processingdate_168D                            Int64                          40.72%\ncount_num_group1_tax_registry_c                    UInt32                         40.72%\nmin_overdueamountmaxdatemonth_365T                 Float64                        35.73%\nmax_overdueamountmaxdatemonth_365T                 Float64                        35.73%\nmean_overdueamountmaxdatemonth_365T                Float64                        35.73%\ncount_overdueamountmaxdatemonth_365T               UInt32                         34.48%\nsum_overdueamountmaxdatemonth_365T                 Float64                        34.48%\nmin_num_group1                                     Int64                          34.48%\nmax_num_group1                                     Int64                          34.48%\nmean_num_group1                                    Float64                        34.48%\ncount_num_group1_cb_a_1                            UInt32                         34.48%\nsum_num_group1                                     Int64                          34.48%\ncount_dpdmax_757P                                  UInt32                         34.48%\nsum_dpdmax_757P                                    Float64                        34.48%\nmin_instlamount_768A                               Float64                        53.90%\nmax_instlamount_768A                               Float64                        53.90%\nmean_instlamount_768A                              Float64                        53.90%\ncount_instlamount_768A                             UInt32                         34.48%\nsum_instlamount_768A                               Float64                        34.48%\nmin_outstandingamount_362A                         Float64                        62.61%\nmax_outstandingamount_362A                         Float64                        62.61%\nmean_outstandingamount_362A                        Float64                        62.61%\ncount_outstandingamount_362A                       UInt32                         34.48%\nsum_outstandingamount_362A                         Float64                        34.48%\ncount_overdueamount_31A                            UInt32                         34.48%\nsum_overdueamount_31A                              Float64                        34.48%\nmin_overdueamount_659A                             Float64                        35.82%\nmax_overdueamount_659A                             Float64                        35.82%\nmean_overdueamount_659A                            Float64                        35.82%\ncount_overdueamount_659A                           UInt32                         34.48%\nsum_overdueamount_659A                             Float64                        34.48%\nmin_collater_valueofguarantee_1124L                Float64                        35.71%\nmax_collater_valueofguarantee_1124L                Float64                        35.71%\nmean_collater_valueofguarantee_1124L               Float64                        35.71%\ncount_collater_valueofguarantee_1124L              UInt32                         34.72%\nsum_collater_valueofguarantee_1124L                Float64                        34.72%\ncount_collater_valueofguarantee_876L               UInt32                         34.72%\nsum_collater_valueofguarantee_876L                 Float64                        34.72%\nmin_pmts_year_1139T                                Float64                        35.71%\nmax_pmts_year_1139T                                Float64                        35.71%\nmean_pmts_year_1139T                               Float64                        35.71%\ncount_pmts_year_1139T                              UInt32                         34.72%\nsum_pmts_year_1139T                                Float64                        34.72%\ncount_pmts_overdue_1152A                           UInt32                         34.72%\nsum_pmts_overdue_1152A                             Float64                        34.72%\nmin_num_group1_cb_a_2                              Int64                          34.72%\nmax_num_group1_cb_a_2                              Int64                          34.72%\nmean_num_group1_cb_a_2                             Float64                        34.72%\ncount_num_group1_cb_a_2                            UInt32                         34.72%\nsum_num_group1_cb_a_2                              Int64                          34.72%\nmin_num_group2                                     Int64                          34.72%\nmax_num_group2                                     Int64                          34.72%\nmean_num_group2                                    Float64                        34.72%\ncount_num_group2                                   UInt32                         34.72%\nsum_num_group2                                     Int64                          34.72%\nratio_queries_30                                   Float64                        34.70%\nratio_queries_90                                   Float64                        34.70%\nratio_queries_120                                  Float64                        34.70%\nratio_queries_180                                  Float64                        34.70%\nTrain/valid shape: (11111, 147), test shape: (10, 147)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c937f68ad7544226aa26ab4d0e50e409"}},"metadata":{}},{"name":"stdout","text":"Fold  1 AUC : 0.692426. Elapsed time: 13.18 seconds. Remaining time: 118.63 seconds.\nFold  2 AUC : 0.699683. Elapsed time: 28.70 seconds. Remaining time: 114.80 seconds.\nFold  3 AUC : 0.677344. Elapsed time: 44.32 seconds. Remaining time: 103.41 seconds.\nFold  4 AUC : 0.639030. Elapsed time: 59.85 seconds. Remaining time: 89.78 seconds.\nFold  5 AUC : 0.621853. Elapsed time: 75.24 seconds. Remaining time: 75.24 seconds.\nFold  6 AUC : 0.708204. Elapsed time: 90.57 seconds. Remaining time: 60.38 seconds.\nFold  7 AUC : 0.676143. Elapsed time: 106.35 seconds. Remaining time: 45.58 seconds.\nFold  8 AUC : 0.638784. Elapsed time: 121.67 seconds. Remaining time: 30.42 seconds.\nFold  9 AUC : 0.724296. Elapsed time: 137.26 seconds. Remaining time: 15.25 seconds.\nFold 10 AUC : 0.661630. Elapsed time: 152.64 seconds. Remaining time: 0.00 seconds.\nFull AUC score 0.673902\nGini Score of the valid set: 0.3080170936940813\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/511156292.py:80: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['PREDICTIONS'] = oof_preds.copy()\n/tmp/ipykernel_33/511156292.py:81: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['target'] = df['target'].copy()\n","output_type":"stream"},{"name":"stdout","text":"Model training - done in 154s\nFeature importance assesment - done in 0s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/10 [00:00<?, ? models/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64531779f424a25a12b3eecf8293ce7"}},"metadata":{}},{"name":"stdout","text":"Submission file has been created.\nSubmission - done in 0s\nNOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\nPipeline total time - done in 266s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}