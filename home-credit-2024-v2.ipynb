{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **LIBRARIES**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:39.106704Z","iopub.status.busy":"2024-03-25T16:01:39.106363Z","iopub.status.idle":"2024-03-25T16:01:41.671161Z","shell.execute_reply":"2024-03-25T16:01:41.670272Z","shell.execute_reply.started":"2024-03-25T16:01:39.106675Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import time\n","import numpy as np\n","import pandas as pd\n","from contextlib import contextmanager\n","import multiprocessing as mp\n","from functools import partial\n","from scipy.stats import kurtosis, iqr, skew\n","import lightgbm as lgb\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","from glob import glob\n","from pathlib import Path\n","from datetime import datetime\n","import polars as pl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.metrics import roc_auc_score \n","from sklearn.metrics import roc_curve, auc\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from tqdm.notebook import tqdm\n","import joblib\n","import warnings\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n"]},{"cell_type":"markdown","metadata":{},"source":["# **CONFIGURATION**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.673665Z","iopub.status.busy":"2024-03-25T16:01:41.672941Z","iopub.status.idle":"2024-03-25T16:01:41.680658Z","shell.execute_reply":"2024-03-25T16:01:41.679508Z","shell.execute_reply.started":"2024-03-25T16:01:41.673630Z"},"trusted":true},"outputs":[],"source":["# GENERAL CONFIGURATIONS\n","NUM_THREADS = 16\n","DATA_DIRECTORY = \"../parquet_files/\"\n","SUBMISSION_SUFIX = \"_model_2.1_31\"\n","#MODE CONFIGURATION\n","SHOW_REPORT = False\n","SELECTKBEST = False\n","# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\n","GENERATE_SUBMISSION_FILES = True\n","EVALUATE_VALIDATION_SET = True\n","STRATIFIED_KFOLD = True\n","RANDOM_SEED = 324\n","NUM_FOLDS = 10\n","EARLY_STOPPING = 100\n","ROOT            = Path(\"./\")\n","\n","LIGHTGBM_PARAMS = {\n","   \"boosting_type\": \"gbdt\",\n","    \"objective\": \"binary\",\n","    \"metric\": \"auc\",\n","    \"max_depth\": 5,\n","    \"learning_rate\": 0.03,\n","    \"n_estimators\": 1000,\n","    \"colsample_bytree\": 0.5, \n","    \"colsample_bynode\": 0.5,\n","    \"verbose\": -1,\n","    \"random_state\": 42,\n","    'reg_alpha': 1,\n","    'reg_lambda': 1,\n","    \n","    \"device\": \"gpu\",\n","    \n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Set aggregations"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.682971Z","iopub.status.busy":"2024-03-25T16:01:41.682375Z","iopub.status.idle":"2024-03-25T16:01:41.802776Z","shell.execute_reply":"2024-03-25T16:01:41.802095Z","shell.execute_reply.started":"2024-03-25T16:01:41.682939Z"},"trusted":true},"outputs":[],"source":["# AGGREGATIONS\n","\n","# D: max, min, mean\n","# M: \n","# A : max, min, mean, sum\n","# L : max, min, mean, sum\n","\n","\n","APPLPREV1_AGG = {\n","\n","    'num_group1':['count'],\n","    'actualdpd_943P': ['min','max','mean','sum'],\n","    'annuity_853A': ['min','max','mean','sum'],\n","    'approvaldate_319D':['max','min','mean'],\n","    'byoccupationinc_3656910L': ['min','max','mean','sum'],\n","    'cancelreason_3545846M':['max'],\n","    'childnum_21L': ['min','max','mean','sum'],\n","    'creationdate_885D':['min','max','mean'],\n","    'credacc_actualbalance_314A': ['min','max','mean','sum'],\n","    'credacc_credlmt_575A': ['min','max','mean','sum'],\n","    'credacc_maxhisbal_375A': ['min','max','mean','sum'],\n","    'credacc_minhisbal_90A': ['min','max','mean','sum'],\n","    'credacc_status_367L': ['max'],\n","    'credacc_transactions_402L': ['min','max','mean','sum'],\n","    'credamount_590A': ['min','max','mean','sum'],\n","    'credtype_587L': ['max'],\n","    'currdebt_94A': ['min','max','mean','sum'],\n","    'dateactivated_425D':['min','max','mean'],\n","    'district_544M':['max'],\n","    'downpmt_134A': ['min','max','mean','sum'],\n","    'dtlastpmt_581D':['min','max','mean'],\n","    'dtlastpmtallstes_3545839D':['min','max','mean'],\n","    'education_1138M':['max'],\n","    'employedfrom_700D':['min','max','mean'],\n","    'familystate_726L': ['max'],\n","    'firstnonzeroinstldate_307D': ['min','max','mean'],\n","    'inittransactioncode_279L': ['max'],\n","    'isbidproduct_390L': ['min','max','mean','sum'],\n","    'isdebitcard_527L': ['min','max','mean','sum'],\n","    'mainoccupationinc_437A': ['min','max','mean','sum'],\n","    'maxdpdtolerance_577P': ['min','max','mean','sum'],\n","    'outstandingdebt_522A': ['min','max','mean','sum'],\n","    'pmtnum_8L': ['min','max','mean','sum'],\n","    'postype_4733339M':['max'],\n","    #'profession_152M':['max'],\n","    'rejectreason_755M':['max'],\n","    'rejectreasonclient_4145042M':['max'],\n","    'revolvingaccount_394A': ['min','max','mean','sum'],\n","    'status_219L': ['max'],\n","    'tenor_203L': ['min','max','mean','sum'],\n","    \n","}\n","APPLPREV2_AGG = {\n","    'num_group1':['count'],\n","    'num_group2':['count'],\n","    'conts_type_509L':['max'],\n","    #'cacccardblochreas_147M'\n","    'credacc_cards_status_52L':['max']\n","    \n","}\n","PERSON1_AGG={\n","    'num_group1':['count'],\n","    'birth_259D': ['max'],\n","    #'childnum_185L':['max','mean','min'],\n","    'contaddr_district_15M':['max'],\n","    'contaddr_matchlist_1032L':['max'],\n","    'contaddr_smempladdr_334L':['max'],\n","    'contaddr_zipcode_807M':['max'],\n","    'education_927M':['max'],\n","    'empl_employedfrom_271D':['max','mean','min'],\n","    'empl_employedtotal_800L':['max'],\n","    'empl_industry_691L':['max'],\n","    #'empladdr_district_926M'\n","    #'empladdr_zipcode_114M'\n","    'familystate_447L':['max','count'],\n","    #'gender_992L'\n","    'housetype_905L':['max'],\n","    #'housingtype_772L'\n","    'incometype_1044T':['max'],\n","    #'isreference_387L'\n","    'language1_981M':['max'],\n","    'mainoccupationinc_384A':['max','mean','min', 'count'],\n","    #'maritalst_703L'\n","    'personindex_1023L':['max','mean','min', 'count','sum'],\n","    'persontype_1072L':['max','mean','min', 'count','sum'],\n","    'persontype_792L':['max','mean','min', 'count','sum'],\n","    #'registaddr_district_1083M'\n","    #'registaddr_zipcode_184M'\n","    'relationshiptoclient_415T':['max','count'],\n","    'relationshiptoclient_642T':['max','count'],\n","    'remitter_829L':['max'],\n","    'role_1084L':['max','count'],\n","    #'role_993L'\n","    'safeguarantyflag_411L':['max'],\n","    'sex_738L':['max'],\n","    'type_25L':['max']\n","    \n","\n","    \n","    \n","    \n","}\n","PERSON2_AGG={\n","    'num_group1':['count'],\n","    'num_group2':['count'],\n","    #'addres_district_368M'\n","    #'addres_role_871L'\n","    #'addres_zip_823M'\n","    #'conts_role_79M'\n","    'empls_economicalst_849M':['max'],\n","    #'empls_employedfrom_796D'\n","    #'empls_employer_name_740M'\n","    #'relatedpersons_role_762T'\n","}\n","OTHER_AGG={\n","    'num_group1':['count'],\n","    'amtdebitincoming_4809443A':['max','mean','min', 'count','sum'],\n","    'amtdebitoutgoing_4809440A':['max','mean','min', 'count','sum'],\n","    #'amtdepositbalance_4809441A'\n","    #'amtdepositincoming_4809444A'\n","    #'amtdepositoutgoing_4809442A'\n","}\n","DEBITCARD_AGG={\n","    'num_group1':['count'],\n","    #'last180dayaveragebalance_704A'\n","    #'last180dayturnover_1134A'\n","    #'last30dayturnover_651A'\n","    'openingdate_857D':['min','max','mean']\n","}\n","TAX_REGISTRY_A_AGG={\n","    'num_group1':['count'],\n","    'amount_4527230A': ['max','mean','min','sum'],\n","    'name_4527232M':['max'],\n","    'recorddate_4527225D':['max','mean','min']\n","    \n","}\n","TAX_REGISTRY_B_AGG={\n","    'num_group1':['count'],\n","    'amount_4917619A':['min','mean','max','sum'],\n","    'deductiondate_4917603D':['max','mean','min'],\n","    'name_4917606M':['max'],\n","    \n","    \n","}\n","TAX_REGISTRY_C_AGG={\n","    'num_group1':['count'],\n","    'employername_160M':['max'],\n","    'pmtamount_36A':['min','mean','max','sum'],\n","    'processingdate_168D':['mean','min','max'],\n","\n","}\n","CREDIT_BUREAU_A_1_AGG={\n","    \n","    'num_group1':['count'],\n","    #'annualeffectiverate_199L'\n","    #'annualeffectiverate_63L'\n","    'classificationofcontr_13M':['max'],\n","    'classificationofcontr_400M':['max'],\n","    'contractst_545M':['max'],\n","    'contractst_964M':['max'],\n","    #'contractsum_5085717L'\n","    #'credlmt_230A'\n","    'credlmt_935A':['max'],\n","    'dateofcredend_289D':['mean','min','max'],\n","    'dateofcredend_353D':['mean','min','max'],\n","    'dateofcredstart_181D':['mean','min','max'],\n","    'dateofcredstart_739D':['mean','min','max'],\n","    'dateofrealrepmt_138D':['mean','min','max'],\n","    'debtoutstand_525A':['min','mean','max','sum'],\n","    'debtoverdue_47A':['min','mean','max','sum'],\n","    'description_351M':['max'],\n","    'dpdmax_139P':['min','mean','max','sum'],\n","    #'dpdmax_757P'\n","    #'dpdmaxdatemonth_442T':['max'],\n","    #'dpdmaxdatemonth_89T':['max'],\n","    #'dpdmaxdateyear_596T'\n","    #'dpdmaxdateyear_896T'\n","    'financialinstitution_382M':['max'],\n","    'financialinstitution_591M':['max'],\n","    'instlamount_768A':['min','mean','max','sum'],\n","    #'instlamount_852A'\n","    #'interestrate_508L'\n","    'lastupdate_1112D':['mean','min','max'],\n","    'lastupdate_388D':['mean','min','max'],\n","    'monthlyinstlamount_332A':['min','mean','max','sum'],\n","    #'monthlyinstlamount_674A'\n","    'nominalrate_281L':['mean','min','max'],\n","    #'nominalrate_498L'\n","    'numberofcontrsvalue_258L':['min','mean','max','sum'],\n","    'numberofcontrsvalue_358L':['min','mean','max','sum'],\n","    #'numberofinstls_229L':\n","    'numberofinstls_320L':['min','mean','max','sum'],\n","    #'numberofoutstandinstls_520L'\n","    'numberofoutstandinstls_59L':['min','mean','max','sum'],\n","    'numberofoverdueinstlmax_1039L':['min','mean','max','sum'],\n","    #'numberofoverdueinstlmax_1151L'\n","    #'numberofoverdueinstlmaxdat_148D'\n","    'numberofoverdueinstlmaxdat_641D':['mean','min','max'],\n","    \n","    \n","      \n","    'overdueamountmaxdatemonth_284T': ['min', 'mean', 'max'],\n","    'overdueamountmaxdatemonth_365T': ['min', 'mean', 'max'],\n","    'overdueamountmaxdateyear_2T': ['min', 'mean', 'max'],\n","    'overdueamountmaxdateyear_994T': ['min', 'mean', 'max'],\n","    'periodicityofpmts_1102L': ['min', 'mean', 'max'],\n","    'periodicityofpmts_837L': ['min', 'mean', 'max'],\n","    'prolongationcount_1120L': ['min', 'mean', 'max'],\n","    'prolongationcount_599L': ['min', 'mean', 'max'],\n","    'purposeofcred_426M': ['min', 'mean', 'max'],\n","    'purposeofcred_874M': ['min', 'mean', 'max'],\n","    'refreshdate_3813885D': ['min', 'mean', 'max'],\n","    'residualamount_488A': ['min', 'mean', 'max'],\n","    'residualamount_856A': ['min', 'mean', 'max'],\n","   \n","    'totalamount_6A': ['min', 'mean', 'max'],\n","    'totalamount_996A': ['min', 'mean', 'max'],\n","    'totaldebtoverduevalue_178A': ['min', 'mean', 'max'],\n","    'totaldebtoverduevalue_718A': ['min', 'mean', 'max'],\n","    'totaloutstanddebtvalue_39A': ['min', 'mean', 'max'],\n","    'totaloutstanddebtvalue_668A': ['min', 'mean', 'max']\n","   \n","}\n","CREDIT_BUREAU_B_1_AGG={\n","    'num_group1':['count'],\n","    \n","}\n","CREDIT_BUREAU_A_2_AGG={\n"," \n","   \n","    'pmts_dpd_1073P': ['min', 'mean', 'max'],\n","    'pmts_dpd_303P': ['min', 'mean', 'max'],\n","    'pmts_month_158T': ['min', 'mean', 'max'],\n","    'pmts_month_706T': ['min', 'mean', 'max'],\n","    'pmts_overdue_1140A': ['min', 'mean', 'max'],\n","    'pmts_overdue_1152A': ['min', 'mean', 'max'],\n","    'pmts_year_1139T': ['min', 'mean', 'max'],\n","    'pmts_year_507T': ['min', 'mean', 'max'],\n","    'subjectroles_name_541M': ['min', 'mean', 'max'],\n","    'subjectroles_name_838M': ['min', 'mean', 'max'],\n","    \n","    \n","    'num_group1':['count'],\n","    'num_group2':['count']\n","}\n","CREDIT_BUREAU_B_2_AGG={\n","    'num_group1':['count'],\n","    'num_group2':['count'],\n","    'pmts_date_1107D':['min', 'mean', 'max'],\n","    'pmts_dpdvalue_108P':['min','mean','max'],\n","    'pmts_pmtsoverdue_635A':['min','mean','max'],\n","}\n"]},{"cell_type":"markdown","metadata":{},"source":["# **MAIN FUNCTION**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.805272Z","iopub.status.busy":"2024-03-25T16:01:41.804952Z","iopub.status.idle":"2024-03-25T16:01:41.836276Z","shell.execute_reply":"2024-03-25T16:01:41.834820Z","shell.execute_reply.started":"2024-03-25T16:01:41.805237Z"},"trusted":true},"outputs":[],"source":["def main(debug= False):\n","    num_rows = 11111 if debug else None\n","    with timer(\"base\"):\n","        \n","        df = get_base(DATA_DIRECTORY, num_rows=num_rows)\n","\n","        print(\"base dataframe shape:\", df.shape)\n","   \n","        \n","        \n","    with timer(\"static\"):\n","       \n","        df_static = get_static(DATA_DIRECTORY, num_rows=num_rows)\n","        df_static = df_static.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_static, on='case_id', how='left', suffix='_static')\n","        print(\"static dataframe shape:\", df_static.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        \n","        del df_static\n","        gc.collect()\n","\n","    with timer(\"static_cb\"):\n","       \n","        df_static_cb = get_static_cb(DATA_DIRECTORY, num_rows=num_rows)\n","        df_static_cb = df_static_cb.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_static_cb, on='case_id', how='left', suffix='_static_cb')\n","        print(\"static cb dataframe shape:\", df_static_cb.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_static_cb\n","        gc.collect()\n","\n","    with timer(\"Previous applications depth 1 test\"):\n","       \n","        df_applprev1 = get_applprev1(DATA_DIRECTORY, num_rows=num_rows)\n","        df_applprev1 = df_applprev1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n","        print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_applprev1\n","        gc.collect()\n","\n","    with timer(\"Previous applications depth 2 test\"):\n","       \n","        df_applprev2 = get_applprev2(DATA_DIRECTORY, num_rows=num_rows)\n","        df_applprev2 = df_applprev2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_applprev2, on='case_id', how='left', suffix='_applprev2')\n","        print(\"Previous applications depth 2 test dataframe shape:\", df_applprev2.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_applprev2\n","        gc.collect()\n","\n","    with timer(\"Person depth 1 test\"):\n","       \n","        df_person1 = get_person1(DATA_DIRECTORY, num_rows=num_rows)\n","        df_person1 = df_person1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_person1, on='case_id', how='left', suffix='_person1')\n","        print(\"Person depth 1 test dataframe shape:\", df_person1.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_person1\n","        gc.collect()\n","\n","    with timer(\"Person depth 2 test\"):\n","     \n","        df_person2 = get_person2(DATA_DIRECTORY, num_rows=num_rows)\n","        df_person2 = df_person2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_person2, on='case_id', how='left', suffix='_person2')\n","        print(\"Person depth 2 test dataframe shape:\", df_person2.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_person2\n","        gc.collect()\n","\n","    with timer(\"Other test\"):\n","        \n","        df_other = get_other(DATA_DIRECTORY, num_rows=num_rows)\n","        df_other = df_other.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_other, on='case_id', how='left', suffix='_other')\n","        print(\"Other test dataframe shape:\", df_other.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_other\n","        gc.collect()\n","\n","    with timer(\"Debit card test\"):\n","      \n","        df_debitcard = get_debitcard(DATA_DIRECTORY, num_rows=num_rows)\n","        df_debitcard = df_debitcard.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_debitcard, on='case_id', how='left', suffix='_debitcard')\n","        print(\"Debit card test dataframe shape:\", df_debitcard.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_debitcard\n","        gc.collect()\n","\n","    with timer(\"Tax registry a test\"):\n","        \n","        df_tax_registry_a = get_tax_registry_a(DATA_DIRECTORY, num_rows=num_rows)\n","        df_tax_registry_a = df_tax_registry_a.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_tax_registry_a, on='case_id', how='left', suffix='_tax_registry_a')\n","        print(\"Tax registry a test dataframe shape:\", df_tax_registry_a.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_tax_registry_a\n","        gc.collect()\n","\n","    with timer(\"Tax registry b test\"):\n","       \n","        df_tax_registry_b = get_tax_registry_b(DATA_DIRECTORY, num_rows=num_rows)\n","        df_tax_registry_b = df_tax_registry_b.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_tax_registry_b, on='case_id', how='left', suffix='_tax_registry_b')\n","        print(\"Tax registry b test dataframe shape:\", df_tax_registry_b.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_tax_registry_b\n","        gc.collect()\n","\n","    with timer(\"Tax registry c test\"):\n","        \n","        df_tax_registry_c = get_tax_registry_c(DATA_DIRECTORY, num_rows=num_rows)\n","        df_tax_registry_c = df_tax_registry_c.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_tax_registry_c, on='case_id', how='left', suffix='_tax_registry_c')\n","        print(\"Tax registry c test dataframe shape:\", df_tax_registry_c.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_tax_registry_c\n","        gc.collect()\n","    \n","        \n","    \n","    with timer(\"Credit bureau a 1 test\"):\n","\n","        df_credit_bureau_a_1 = get_credit_bureau_a_1(DATA_DIRECTORY, num_rows=num_rows)\n","        df_credit_bureau_a_1 = df_credit_bureau_a_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_credit_bureau_a_1, on='case_id', how='left', suffix='_cb_a_1')\n","        print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_1.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_credit_bureau_a_1\n","        gc.collect()\n","    with timer(\"Credit bureau b 1 test\"):\n","       \n","        df_credit_bureau_b_1 = get_credit_bureau_b_1(DATA_DIRECTORY, num_rows=num_rows)\n","        df_credit_bureau_b_1 = df_credit_bureau_b_1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_credit_bureau_b_1, on='case_id', how='left', suffix='_cb_b_1')\n","        print(\"Credit bureau b 1 test dataframe shape:\", df_credit_bureau_b_1.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_credit_bureau_b_1\n","        gc.collect()\n","\n","\n","    \n","    \n","    with timer(\"Credit bureau a 2 test\"):\n","       \n","        df_credit_bureau_a_2 = get_credit_bureau_a_2(DATA_DIRECTORY, num_rows=num_rows)\n","        df_credit_bureau_a_2 = df_credit_bureau_a_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_credit_bureau_a_2, on='case_id', how='left', suffix='_cb_a_2')\n","        print(\"Credit bureau a 2 test dataframe shape:\", df_credit_bureau_a_2.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_credit_bureau_a_2\n","        gc.collect()\n","    \n","    with timer(\"Credit bureau b 2 test\"):\n","       \n","        df_credit_bureau_b_2 = get_credit_bureau_b_2(DATA_DIRECTORY, num_rows=num_rows)\n","        df_credit_bureau_b_2 = df_credit_bureau_b_2.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","        df = df.join(df_credit_bureau_b_2, on='case_id', how='left', suffix='_cb_b_2')\n","        print(\"Credit bureau b 2 test dataframe shape:\", df_credit_bureau_b_2.shape)\n","        print(\"DATAFRAME shape:\", df.shape)\n","        del df_credit_bureau_b_2\n","        gc.collect()\n","    \n","    with timer(\"Feature engineering / preprocessing\"): \n","       \n","        df=feature_engineering(df)\n","        df_pandas, cat_cols = to_pandas(df)\n","        del df;gc.collect()\n","        df=df_pandas\n","        df=reduce_mem_usage(df)\n","        print(\"DATAFRAME shape:\", df.shape)\n","   \n","    \n","    if(SELECTKBEST):\n","        with timer(\"SelectKBest feature research\"):\n","            \n","            selectkbestX(df)\n","            return\n","\n","    with timer(\"Model training\"):\n","       \n","        \n","        del_features = ['target', 'case_id','WEEK_NUM']\n","        predictors = list(filter(lambda v: v not in del_features, df.columns))\n","        model = kfold_lightgbm_sklearn(df, cat_cols)\n","       \n","        \n","\n","    \n","    \n","    with timer(\"Feature importance assesment\"):\n","        \n","        get_features_importances(predictors, model)\n","        \n","        \n","    \n","        \n","    with timer(\"Submission\"):\n","\n","            if generate_submission_file(df, model):\n","\n","                print(\"Submission file has been created.\")\n","  \n","    \n","    print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n","    \n","    return df, model\n","    \n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["# **UTILITY FUNCTIONS**"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.837716Z","iopub.status.busy":"2024-03-25T16:01:41.837456Z","iopub.status.idle":"2024-03-25T16:01:41.853363Z","shell.execute_reply":"2024-03-25T16:01:41.851497Z","shell.execute_reply.started":"2024-03-25T16:01:41.837695Z"},"trusted":true},"outputs":[],"source":["class Pipeline:\n","    @staticmethod\n","    \n","    \n","    # Sets datatypes accordingly\n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))            \n","\n","        return df\n","    \n","    \n","    # Changes the values of all date columns. The result will not be a date but number of days since date_decision.\n","    @staticmethod\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                \n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n","                df = df.with_columns(pl.col(col).dt.total_days())\n","                \n","        df = df.drop(\"date_decision\", \"MONTH\")\n","\n","        return df\n","    \n","    # It drops columns with a lot of NaN values.\n","    @staticmethod\n","    def filter_cols(df):\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\", \"collater_typofvalofguarant_298M\",\"classificationofcontr_400M\",\"contractst_964M\",\"financialinstitution_382M\",\"financialinstitution_591M\",\"numberofoverdueinstlmaxdat_641D\",\"prolongationcount_1120L\"]:  # Exclude the specified column\n","                isnull = df[col].is_null().mean()\n","\n","                if isnull > 0.95:\n","                    df = df.drop(col)\n","\n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\", \"collater_typofvalofguarant_298M\",\"classificationofcontr_400M\",\"contractst_964M\",\"financialinstitution_382M\",\"financialinstitution_591M\",\"numberofoverdueinstlmaxdat_641D\",\"prolongationcount_1120L\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","\n","        return df"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.855909Z","iopub.status.busy":"2024-03-25T16:01:41.855413Z","iopub.status.idle":"2024-03-25T16:01:41.869880Z","shell.execute_reply":"2024-03-25T16:01:41.868533Z","shell.execute_reply.started":"2024-03-25T16:01:41.855868Z"},"trusted":true},"outputs":[],"source":["def get_info(dataframe):\n","    \"\"\"\n","    View data types, shape, and calculate the percentage of NaN (missing) values in each column\n","    of a Polars DataFrame simultaneously.\n","    \n","    Parameters:\n","    dataframe (polars.DataFrame): The DataFrame to analyze.\n","    \n","    Returns:\n","    None\n","    \"\"\"\n","    # Print DataFrame shape\n","    print(\"DataFrame Shape:\", dataframe.shape)\n","    print(\"-\" * 60)\n","    \n","    # Print column information\n","    print(\"{:<50} {:<30} {:<20}\".format(\"Column Name\", \"Data Type\", \"NaN Percentage\"))\n","    print(\"-\" * 60)\n","    \n","    # Total number of rows in the DataFrame\n","    total_rows = len(dataframe)\n","    \n","    # Iterate over each column\n","    for column in dataframe.columns:\n","        # Get the data type of the column\n","        dtype = str(dataframe[column].dtype)\n","        \n","        # Count the number of NaN values in the column\n","        nan_count = dataframe[column].null_count()\n","        \n","        # Calculate the percentage of NaN values\n","        nan_percentage = (nan_count / total_rows) * 100\n","        \n","        # Print the information\n","        print(\"{:<50} {:<30} {:.2f}%\".format(column, dtype, nan_percentage))\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.872380Z","iopub.status.busy":"2024-03-25T16:01:41.871969Z","iopub.status.idle":"2024-03-25T16:01:41.883940Z","shell.execute_reply":"2024-03-25T16:01:41.882925Z","shell.execute_reply.started":"2024-03-25T16:01:41.872347Z"},"trusted":true},"outputs":[],"source":["def to_pandas(df_data, cat_cols=None):\n","    df_data = df_data.to_pandas()\n","    \n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    \n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    \n","    return df_data, cat_cols"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.885393Z","iopub.status.busy":"2024-03-25T16:01:41.885012Z","iopub.status.idle":"2024-03-25T16:01:41.896645Z","shell.execute_reply":"2024-03-25T16:01:41.895016Z","shell.execute_reply.started":"2024-03-25T16:01:41.885371Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","\n","    return df"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.899472Z","iopub.status.busy":"2024-03-25T16:01:41.898314Z","iopub.status.idle":"2024-03-25T16:01:41.916197Z","shell.execute_reply":"2024-03-25T16:01:41.914604Z","shell.execute_reply.started":"2024-03-25T16:01:41.899419Z"},"trusted":true},"outputs":[],"source":["@contextmanager\n","def timer(name):\n","    t0 = time.time()\n","    yield\n","    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.920245Z","iopub.status.busy":"2024-03-25T16:01:41.919883Z","iopub.status.idle":"2024-03-25T16:01:41.933209Z","shell.execute_reply":"2024-03-25T16:01:41.931836Z","shell.execute_reply.started":"2024-03-25T16:01:41.920218Z"},"trusted":true},"outputs":[],"source":["def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n","    \n","\n","    temp=base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n","        .sort_values(\"WEEK_NUM\") \\\n","        .groupby(\"WEEK_NUM\").mean()\n","   \n","    week_nums_to_drop = temp[(temp[\"target\"] == 0) | (temp[\"target\"] == 1)].index.tolist()\n","\n","    base_filtered = base[~base[\"WEEK_NUM\"].isin(week_nums_to_drop)]\n","\n","    # Apply the aggregator\n","    gini_in_time = base_filtered.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n","        .sort_values(\"WEEK_NUM\") \\\n","        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]] \\\n","        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n","\n","    \n","\n","    x = np.arange(len(gini_in_time))\n","    y = gini_in_time\n","    a, b = np.polyfit(x, y, 1)\n","    y_hat = a * x + b\n","    residuals = y - y_hat\n","    res_std = np.std(residuals)\n","    avg_gini = np.nanmean(gini_in_time)  # Use np.nanmean to handle NaN values\n","    \n","    if SHOW_REPORT:\n","        # Display the plot of x on y\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(x, y, 'o', label='Gini in Time')\n","        plt.plot(x, y_hat, '-', label='Fitted line (slope={:.2f}, intercept={:.2f})'.format(a, b))\n","        plt.xlabel('Week')\n","        plt.ylabel('Gini in Time')\n","        plt.title('Gini Stability Over Time')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","    \n","    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std"]},{"cell_type":"markdown","metadata":{},"source":["### Report function"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.935122Z","iopub.status.busy":"2024-03-25T16:01:41.934711Z","iopub.status.idle":"2024-03-25T16:01:41.956392Z","shell.execute_reply":"2024-03-25T16:01:41.954736Z","shell.execute_reply.started":"2024-03-25T16:01:41.935089Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\ndef make_report(num_rows, predictors, model):\\n    # 1. time\\n    current_time = datetime.now()\\n    # Print the current time\\n    print(\"Current Time:\", current_time)\\n    \\n    # 2. specification\\n    if not num_rows:\\n        print(\"The notebook was run in full mode.\")\\n    else:\\n        print(\"The notebook was run in debug mode. Number of rows: \" + str(num_rows))\\n    \\n    # 3. features\\n    feat_importances_df = model.get_features_importances_df(predictors)\\n    feat_importances_df[\\'gain\\'] = feat_importances_df[\\'gain\\'].round(0)\\n    print(feat_importances_df.shape)\\n    \\n    predictions = pd.Series(model.get_predictions())\\n   \\n    numerical_columns = data.select_dtypes(include=[\\'int\\', \\'float\\']).columns\\n\\n    # Compute correlations of each numerical column with \\'PREDICTIONS\\'\\n    correlations = {}\\n    \\n    # Compute correlations of each numerical column with \\'feat\\'\\n    for column in numerical_columns:\\n        correlations[column] = predictions.corr(data[column])\\n\\n    # Create a new DataFrame with \\'features\\' and \\'correlation\\' columns\\n    correlation_df = pd.DataFrame(list(correlations.items()), columns=[\\'features\\', \\'correlation\\'])\\n\\n    # Round the correlation numbers to three decimal places\\n    correlation_df[\\'correlation\\'] = correlation_df[\\'correlation\\'].round(3)\\n\\n    # Merge feat_importances_df and correlation_df on \\'feature\\'\\n    combined_df = pd.merge(feat_importances_df, correlation_df, left_on=\"feature\", right_on=\\'features\\', how=\\'left\\')\\n\\n    # Handle categorical features with no correlation\\n    combined_df[\\'correlation\\'] = combined_df[\\'correlation\\'].fillna(value=np.nan)\\n    \\n\\n    # Compute and add valid percentage for each feature\\n    valid_percentage = (data[0:-10].count() / len(data[0:-10]))\\n    valid_percentage = valid_percentage.round(3)\\n    combined_df[\\'valid_percentage\\'] = combined_df[\\'feature\\'].map(valid_percentage)\\n\\n    # Print the combined_df DataFrame\\n    print(combined_df.to_string(index=False))\\n    print()\\n    roc_score=roc_auc_score(data[\\'target\\'][0:-10],predictions)\\n    print(\"ROC score: \",roc_score)\\n\\n    # Compute false positive rate, true positive rate, and thresholds for ROC curve\\n    fpr, tpr, thresholds = roc_curve(data[\\'target\\'][0:-10], predictions)\\n\\n    # Plot ROC curve\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(fpr, tpr, color=\\'blue\\', lw=2, label=\\'ROC curve (AUC = %0.2f)\\' % roc_score)\\n    plt.plot([0, 1], [0, 1], color=\\'gray\\', linestyle=\\'--\\')\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel(\\'False Positive Rate\\')\\n    plt.ylabel(\\'True Positive Rate\\')\\n    plt.title(\\'Receiver Operating Characteristic (ROC) Curve\\')\\n    plt.legend(loc=\"lower right\")\\n    plt.grid(True)\\n    plt.show()\\n'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","def make_report(num_rows, predictors, model):\n","    # 1. time\n","    current_time = datetime.now()\n","    # Print the current time\n","    print(\"Current Time:\", current_time)\n","    \n","    # 2. specification\n","    if not num_rows:\n","        print(\"The notebook was run in full mode.\")\n","    else:\n","        print(\"The notebook was run in debug mode. Number of rows: \" + str(num_rows))\n","    \n","    # 3. features\n","    feat_importances_df = model.get_features_importances_df(predictors)\n","    feat_importances_df['gain'] = feat_importances_df['gain'].round(0)\n","    print(feat_importances_df.shape)\n","    \n","    predictions = pd.Series(model.get_predictions())\n","   \n","    numerical_columns = data.select_dtypes(include=['int', 'float']).columns\n","\n","    # Compute correlations of each numerical column with 'PREDICTIONS'\n","    correlations = {}\n","    \n","    # Compute correlations of each numerical column with 'feat'\n","    for column in numerical_columns:\n","        correlations[column] = predictions.corr(data[column])\n","\n","    # Create a new DataFrame with 'features' and 'correlation' columns\n","    correlation_df = pd.DataFrame(list(correlations.items()), columns=['features', 'correlation'])\n","\n","    # Round the correlation numbers to three decimal places\n","    correlation_df['correlation'] = correlation_df['correlation'].round(3)\n","\n","    # Merge feat_importances_df and correlation_df on 'feature'\n","    combined_df = pd.merge(feat_importances_df, correlation_df, left_on=\"feature\", right_on='features', how='left')\n","\n","    # Handle categorical features with no correlation\n","    combined_df['correlation'] = combined_df['correlation'].fillna(value=np.nan)\n","    \n","\n","    # Compute and add valid percentage for each feature\n","    valid_percentage = (data[0:-10].count() / len(data[0:-10]))\n","    valid_percentage = valid_percentage.round(3)\n","    combined_df['valid_percentage'] = combined_df['feature'].map(valid_percentage)\n","\n","    # Print the combined_df DataFrame\n","    print(combined_df.to_string(index=False))\n","    print()\n","    roc_score=roc_auc_score(data['target'][0:-10],predictions)\n","    print(\"ROC score: \",roc_score)\n","\n","    # Compute false positive rate, true positive rate, and thresholds for ROC curve\n","    fpr, tpr, thresholds = roc_curve(data['target'][0:-10], predictions)\n","\n","    # Plot ROC curve\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_score)\n","    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.grid(True)\n","    plt.show()\n","'''"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.958347Z","iopub.status.busy":"2024-03-25T16:01:41.957957Z","iopub.status.idle":"2024-03-25T16:01:41.975750Z","shell.execute_reply":"2024-03-25T16:01:41.974250Z","shell.execute_reply.started":"2024-03-25T16:01:41.958317Z"},"trusted":true},"outputs":[],"source":["def group(df_to_agg, prefix, aggregations, aggregate_by='case_id', datatype='polars'):\n","    # Create a dictionary mapping aggregation functions to their string representations\n","    \n","    if datatype=='polars':\n","        func_mapping = {\n","        'min': pl.min,\n","        'max': pl.max,\n","        'mean': pl.mean,\n","        'sum': pl.sum,\n","        'count': pl.count\n","        }\n","\n","    # Perform the aggregation\n","        agg_df = df_to_agg.group_by(aggregate_by).agg(**{\n","            f\"{func}_{col}\": func_mapping[func](col) for col, funcs in aggregations.items() for func in funcs\n","        })\n","        '''\n","        # Rename columns\n","        for col, funcs in aggregations.items():\n","            for func in funcs:\n","                old_name = f\"{col}_{func}\"\n","                new_name = f\"{prefix}{col}_{func.upper()}\"\n","                agg_df = agg_df.select(pl.col(old_name).alias(new_name))\n","        '''\n","        return agg_df\n","    \n","    if datatype=='pandas':\n","            # Create a dictionary mapping aggregation functions to their string representations\n","        func_mapping = {\n","            'min': 'min',\n","            'max': 'max',\n","            'mean': 'mean',\n","            'sum': 'sum',\n","            'count': 'count'\n","        }\n","\n","        # Perform the aggregation\n","        agg_df = df_to_agg.groupby(aggregate_by).agg(**{\n","            f\"{prefix}{col}_{func.upper()}\": (col, func_mapping[func]) for col, funcs in aggregations.items() for func in funcs\n","        }).reset_index()\n","        \n","        return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["# **SELECTKBEST METHOD**"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.978370Z","iopub.status.busy":"2024-03-25T16:01:41.978016Z","iopub.status.idle":"2024-03-25T16:01:41.996312Z","shell.execute_reply":"2024-03-25T16:01:41.994832Z","shell.execute_reply.started":"2024-03-25T16:01:41.978348Z"},"trusted":true},"outputs":[],"source":["def selectkbestX(data):\n","    #########################################################################################\n","    def preprocessingX(data):\n","    \n","        \n","\n","        def one_hot_encode(data):\n","            df=data.copy()\n","            del data;gc.collect()\n","            \n","            original_columns = list(df.columns)\n","            categories = [cat for cat in df.columns if df[cat].dtype == 'category']\n","            df = pd.get_dummies(df, columns= categories, dummy_na= True) #one_hot_encode the categorical features\n","            categorical_columns = [cat for cat in df.columns if cat not in original_columns]\n","            return df, categorical_columns\n","        \n","        \n","        df,categorical_columns=one_hot_encode(data)\n","        del data;gc.collect()\n","        \n","        for column in df.columns:\n","            # Calculate the mean value of the column excluding NaNs\n","            column_mean = df[column].mean()\n","            # Replace NaN values in the column with the mean value\n","            df[column].fillna(column_mean, inplace=True)\n","\n","       \n","\n","        return df\n","    #########################################################################################\n","    def selectkbest_base(X_train, y_train):\n","        \n","        # Define SelectKBest with desired parameters\n","        k = 500  # Number of top features to select\n","        S = SelectKBest(score_func=f_classif, k=k)\n","\n","        # Fit SelectKBest on training data and transform features\n","        X_train_k_best = S.fit_transform(X_train, y_train)\n","\n","        # Get scores assigned to each feature\n","        feature_scores = S.scores_\n","        \n","        # Create a DataFrame to store feature names and their scores\n","        feature_scores_df = pd.DataFrame({'Feature': X_train.columns, 'Score': feature_scores})\n","\n","        # Sort DataFrame by scores in descending order\n","        #feature_scores_df_sorted = feature_scores_df.sort_values(by='Score', ascending=False)\n","\n","        # Print the table of top features and their scores\n","      \n","        # Return DataFrame with feature names and their scores\n","        return feature_scores_df\n","    #########################################################################################\n","    \n","    df0=data.copy()\n","    del data;gc.collect()\n","    df1=preprocessingX(df0)\n","    del df0;gc.collect()\n","    \n","    \n","    \n","    \n","   \n","    N_CHUNKS=4\n","    \n","    df = df1[df1['target'].notnull()]\n","    del df1\n","    gc.collect()\n","    del_features = ['target', 'case_id']\n","    predictors = [col for col in df.columns if col not in del_features]\n","    \n","    feats_df = pd.DataFrame({'feature': predictors}, columns=['feature'])\n","    \n","    results=[]\n","    \n","    with tqdm(total=N_CHUNKS) as pbar:\n","        for i in range(N_CHUNKS):\n","\n","            sub_df = df[df.index % N_CHUNKS == i]\n","            df.drop(df.index[df.index % N_CHUNKS == i], inplace=True)\n","            X_train=sub_df[predictors]\n","            y_train=sub_df['target']\n","\n","\n","            result_df=selectkbest_base(X_train, y_train)\n","            \n","            del sub_df\n","            gc.collect()\n","\n","            results.append(result_df)\n","            pbar.update(1)\n","            \n","    del df; gc.collect()\n","    merged_df = results[0]\n","\n","# Merge the remaining dataframes horizontally on the 'Feature' column\n","    for df_index in range(1, len(results)):\n","        suffix = '_' + str(df_index)  # Add a suffix to distinguish overlapping column names\n","        merged_df = pd.merge(merged_df, results[df_index], on='Feature', suffixes=('', suffix))\n","\n","    merged_df.rename(columns={'Score': 'Score_0'}, inplace=True)\n","    merged_df['mean_score'] = 0\n","    \n","    for i in range(N_CHUNKS):\n","        merged_df['mean_score']+=merged_df[\"Score_\"+str(i)]\n","    \n","    \n","    final_df=merged_df[['Feature', 'mean_score']]\n","    final_df = final_df.sort_values(by='mean_score', ascending=False)\n","    pd.set_option('display.max_rows', None)  # Show all rows\n","# Display the DataFrame\n","    print(final_df)\n","   \n","\n","    final_df.to_csv(\"/kaggle/working/SelectKBest.csv\")\n","    \n","    return merged_df"]},{"cell_type":"markdown","metadata":{},"source":["#  **MODEL**"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:41.998938Z","iopub.status.busy":"2024-03-25T16:01:41.998296Z","iopub.status.idle":"2024-03-25T16:01:42.014405Z","shell.execute_reply":"2024-03-25T16:01:42.013416Z","shell.execute_reply.started":"2024-03-25T16:01:41.998912Z"},"trusted":true},"outputs":[],"source":["class VotingModel(BaseEstimator, RegressorMixin):\n","    def __init__(self, estimators):\n","        super().__init__()\n","        self.estimators = estimators\n","        \n","        \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def predict(self, X):\n","        y_preds = [estimator.predict(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","    \n","    def predict_proba(self, X):\n","        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n","        # Use tqdm to create a progress bar during the prediction\n","        with tqdm(total=len(self.estimators), desc=\"Predicting\", unit=\" models\") as pbar:\n","            for i, estimator in enumerate(self.estimators):\n","                y_preds[i] = estimator.predict_proba(X)\n","                pbar.update(1)  # Update the progress bar\n","        return np.mean(y_preds, axis=0)\n","\n","    \n","    def get_splits(self, aggregation_method=np.mean):\n","        \n","        feature_importances_list=[]\n","        for x in self.estimators:\n","            feature_importances_list.append(x.booster_.feature_importance(importance_type='split'))\n","            \n","        # Aggregate feature importances across all models\n","        if all(importances is not None for importances in feature_importances_list):\n","            combined_importances = aggregation_method(feature_importances_list, axis=0)\n","        else:\n","            combined_importances = None   \n","        return combined_importances\n","    \n","    \n","    def get_gains(self, aggregation_method=np.mean):\n","        \n","        feature_importances_list=[]\n","        for model in self.estimators:\n","            feature_importances_list.append(x.booster_.feature_importance(importance_type='gain'))\n","            \n","        # Aggregate feature importances across all models\n","        if all(importances is not None for importances in feature_importances_list):\n","            combined_importances = aggregation_method(feature_importances_list, axis=0)\n","        else:\n","            combined_importances = None\n","              \n","        return combined_importances\n","    \n","    def get_features_importances_df(self, predictors):\n","        \n","        \n","        importance_df = pd.DataFrame()\n","        eval_results = dict()\n","        for model in self.estimators:\n","            fold_importance = pd.DataFrame()\n","            fold_importance[\"feature\"] = predictors\n","            fold_importance[\"gain\"] = model.booster_.feature_importance(importance_type='gain')\n","            fold_importance[\"split\"] = model.booster_.feature_importance(importance_type='split')\n","            importance_df = pd.concat([importance_df, fold_importance], axis=0)\n","            importance_df= importance_df.groupby('feature').mean().reset_index()\n","        return importance_df\n","    \n","    \n","    def add_predictions(self, predictions):\n","        self.predictions=predictions\n","        \n","    def get_predictions(self):\n","        return self.predictions\n","        "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.016962Z","iopub.status.busy":"2024-03-25T16:01:42.015659Z","iopub.status.idle":"2024-03-25T16:01:42.035625Z","shell.execute_reply":"2024-03-25T16:01:42.034793Z","shell.execute_reply.started":"2024-03-25T16:01:42.016927Z"},"trusted":true},"outputs":[],"source":["def kfold_lightgbm_sklearn(data, categorical_feature = None):\n","    \n","    \n","    \n","   \n","    #time.sleep(30)\n","    start_time = time.time()\n","    \n","    df=data.copy()\n","    df.drop(df[df['target'].isnull()].index, inplace=True)\n","    #test=data.copy()\n","    #test.drop(test[test['target'].notnull()].index, inplace=True)\n","    del data; gc.collect()\n","    \n","    df=reduce_mem_usage(df)\n","    #test=reduce_mem_usage(test)\n","    \n","  \n","    #time.sleep(30)\n","    #print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n","    print(\"Train/valid shape: {}, \".format(df.shape))\n","    del_features = ['target', 'case_id', 'WEEK_NUM']\n","    predictors = list(filter(lambda v: v not in del_features, df.columns))\n","\n","    if not STRATIFIED_KFOLD:\n","        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n","    else:\n","        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n","    \n","        # Hold oof predictions, test predictions, feature importance and training/valid auc\n","    oof_preds = np.zeros(df.shape[0])\n","    \n","    importance_df = pd.DataFrame()\n","    eval_results = dict()\n","    \n","    fitted_models = []\n","    with tqdm(total=NUM_FOLDS) as pbar:\n","        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):\n","           \n","          \n","            train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]\n","            valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]\n","            \n","            \n","        \n","        \n","            #time.sleep(30)\n","            params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n","            clf = lgb.LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n","\n","\n","            if not categorical_feature:\n","                    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n","                            callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)]\n","                           )\n","            else:\n","                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n","                        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)],\n","                        feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n","\n","\n","            fitted_models.append(clf)\n","\n","            if EVALUATE_VALIDATION_SET:\n","                oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n","\n","\n","\n","                # Feature importance by GAIN and SPLIT\n","\n","            eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n","            eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n","\n","            elapsed_time = time.time() - start_time\n","            remaining_time = elapsed_time * (NUM_FOLDS - n_fold - 1) / (n_fold + 1)\n","            print('Fold %2d AUC : %.6f. Elapsed time: %.2f seconds. Remaining time: %.2f seconds.'\n","                  % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx]), elapsed_time, remaining_time))\n","            del clf, train_x, train_y, valid_x, valid_y\n","            gc.collect()\n","            pbar.update(1)\n","            \n","    print('Full AUC score %.6f' % roc_auc_score(df['target'], oof_preds))\n","    # Get the average feature importance between folds\n","    \n","    \n","    \n","    if len(df)>0:\n","        base=get_base(DATA_DIRECTORY, len(df))\n","        base, cat_cols = to_pandas(base)\n","        base=base[base['target'].notnull()]\n","        base['score']= oof_preds\n","        gini_score = gini_stability(base)\n","        print(\"Gini Score of the valid set:\", gini_score)\n","    \n","    \n","    \n","    \n","    # Save feature importance, test predictions and oof predictions as csv\n","    \n","        \n","        \n","  \n","        \n","        \n","    model = VotingModel(fitted_models)\n","    if GENERATE_SUBMISSION_FILES:\n","        \n","\n","\n","            # Generate oof csv\n","            oof = pd.DataFrame()\n","            oof['case_id'] = df['case_id'].copy()\n","            df['PREDICTIONS'] = oof_preds.copy()\n","            df['target'] = df['target'].copy()\n","            df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","    model.add_predictions(oof_preds.copy())\n","    del df; gc.collect()\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# **SUBMISSION**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.038587Z","iopub.status.busy":"2024-03-25T16:01:42.037130Z","iopub.status.idle":"2024-03-25T16:01:42.052916Z","shell.execute_reply":"2024-03-25T16:01:42.051453Z","shell.execute_reply.started":"2024-03-25T16:01:42.038533Z"},"trusted":true},"outputs":[],"source":["def generate_submission_file(data, model):\n","    test=data.copy()\n","    test.drop(test[test['target'].notnull()].index, inplace=True)\n","    del data;gc.collect()\n","\n","    '''\n","    length=len(test)\n","    y_pred = pd.Series([0.5] * length,index=test['case_id'])\n","    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","    df_subm = df_subm.set_index(\"case_id\")\n","    df_subm[\"score\"] = y_pred\n","    df_subm.to_csv(\"submission.csv\")\n","    '''\n","\n","    del_features = ['target', 'case_id','WEEK_NUM']\n","    predictors = list(filter(lambda v: v not in del_features, test.columns))\n","    y_pred = pd.Series(model.predict_proba(test[predictors])[:, 1], index=test['case_id']) \n","\n","    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","    df_subm = df_subm.set_index(\"case_id\")\n","    df_subm[\"score\"] = y_pred\n","\n","    df_subm.to_csv(\"submission.csv\")\n","    \n","    return True\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# **EVALUATE FEATURES IMPORTANCES**"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.055538Z","iopub.status.busy":"2024-03-25T16:01:42.055065Z","iopub.status.idle":"2024-03-25T16:01:42.065865Z","shell.execute_reply":"2024-03-25T16:01:42.064677Z","shell.execute_reply.started":"2024-03-25T16:01:42.055507Z"},"trusted":true},"outputs":[],"source":["def get_features_importances(predictors, model):\n","    importance_df = model.get_features_importances_df(predictors)\n","    mean_importance = importance_df.groupby('feature').mean().reset_index()\n","    mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n","    mean_importance.to_csv('feature_importance_gain{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","    mean_importance.sort_values(by= 'split', ascending=False, inplace=True)\n","    mean_importance.to_csv('feature_importance_split{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","    return True"]},{"cell_type":"markdown","metadata":{},"source":["# **FEATURE ENGINEERING FUNCTION**"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.067611Z","iopub.status.busy":"2024-03-25T16:01:42.067012Z","iopub.status.idle":"2024-03-25T16:01:42.103364Z","shell.execute_reply":"2024-03-25T16:01:42.101785Z","shell.execute_reply.started":"2024-03-25T16:01:42.067557Z"},"trusted":true},"outputs":[],"source":["def feature_engineering(df):\n","    \n","    \n","    \n","    df=df.pipe(Pipeline.handle_dates) \n","    #df=df.pipe(Pipeline.filter_cols)\n","    \n","    '''\n","     data['CREDIT_INCOME_PERCENT'] = data['AMT_CREDIT'] / data['AMT_INCOME_TOTAL']\n","                                    final credit amount on the previous application./Income of the client\n","    \n","    \n","    credamount_590A/byoccupationinc_3656910L\n","    '''\n","        \n","       \n","    \n","    columns_to_add = [\n","    # Ratios\n","    (pl.col(\"days30_165L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_30\"),\n","    (pl.col(\"days90_310L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_90\"),\n","    (pl.col(\"days120_123L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_120\"),\n","    (pl.col(\"days180_256L\") / pl.col(\"days360_512L\")).alias(\"ratio_queries_180\"),\n","    \n","    (pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"sum_collater\")).alias(\"ratio_collater_active\"),\n","    (pl.col(\"overdueamount_31A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_active\"),\n","    (pl.col(\"overdueamount_659A\") / pl.col(\"sum_overdue_amount\")).alias(\"ratio_overdue_amount_close\"),\n","    (pl.col(\"overdueamount_31A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_active\"),\n","    (pl.col(\"overdueamount_659A\") / pl.col(\"total_overdue_amount\")).alias(\"ratio_overdue_amount_close\"),\n","    (pl.col(\"totalamount\")).alias(\"sum_totalcredit_contract\"),\n","    (pl.col(\"totalamount_503A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_active\"),\n","    (pl.col(\"totalamount_6A\") / pl.col(\"sum_totalcredit_contract\")).alias(\"ratio_totalcredit_contract_close\"),\n","    (pl.col(\"totaldebtoverduevalue_178A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_active\"),\n","    (pl.col(\"totaldebtoverduevalue_718A\") / pl.col(\"totaldebt_9A\")).alias(\"ratio_overdue_debt_close\"),\n","    # sums\n","    (pl.col(\"numberofinstls_229L\") + pl.col(\"numberofinstls_320L\")).alias(\"sum_instalments\"),\n","    (pl.col(\"collater_typofvalofguarant_407M\") + pl.col(\"sum_collater\")).alias(\"ratio_collater_close\"),\n","    (pl.col(\"collater_typofvalofguarant_298M\") + pl.col(\"collater_typofvalofguarant_407M\")).alias(\"sum_collater\"),\n","    (pl.col(\"overdueamount_31A\") + pl.col(\"overdueamount_659A\")).alias(\"sum_overdue_amount\"),\n","    \n","    (pl.col(\"numberofinstls_320L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_active\"),\n","    (pl.col(\"numberofinstls_229L\") / pl.col(\"sum_instalments\")).alias(\"ratio_instalments_close\"),\n","    # Deltas\n","    (pl.col(\"annuity_780A\") - pl.col(\"annuitynextmonth_57A\")).alias(\"change_in_cur_to_next_annuity_amount\"),\n","    (pl.col(\"annuity_780A\") - pl.col(\"annuity_853A\")).alias(\"change_in_cur_to_prev_annuity_amount\"),\n","    (pl.col(\"byoccupationinc_3656910L\") - pl.col(\"byoccupationinc_3656910L\").shift()).alias(\"change_in_annual_income\"),\n","    (pl.col(\"currdebt_22A\") - pl.col(\"currdebt_94A\")).alias(\"change_in_debt_value\"),\n","    (pl.col(\"tenor_203L\") - pl.col(\"tenor_203L\").shift()).alias(\"change_in_instalments\"),\n","    (pl.col(\"familystate_726L\") - pl.col(\"familystate_726L\").shift()).alias(\"change_in_familystate\")\n","]\n","\n","# Add the calculated columns to the DataFrame\n","    for column in columns_to_add:\n","        df = df.with_columns([column])\n","        \n","        \n","    '''\n","    df = df.with_columns(\n","    'ratio_queries_30',\n","    df['days30_165L'] / df['days360_512L']\n",")\n","    df['ratio_queries_90'] = df['days90_310L'] / df['days360_512L']\n","    df['ratio_queries_120'] = df['days120_123L'] / df['days360_512L']\n","    df['ratio_queries_180'] = df['days180_256L'] / df['days360_512L']\n","    df['sum_collater'] = df['collater_typofvalofguarant_298M'] +    df['collater_typofvalofguarant_407M']\n","    df['ratio_collater_active'] = df['collater_typofvalofguarant_298M'] +    df['sum_collater']\n","    df['ratio_collater_close'] = df['collater_typofvalofguarant_407M'] +    df['sum_collater']\n","    df['sum_overdue_amount'] = df['overdueamount_31A'] +    df['overdueamount_659A']\n","    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['sum_overdue_amount']\n","    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['sum_overdue_amount']\n","    df['ratio_overdue_amount_active'] = df['overdueamount_31A'] /    df['total_overdue_amount']\n","    df['ratio_overdue_amount_close'] = df['overdueamount_659A'] /    df['total_overdue_amount']\n","    df['sum_totalcredit_contract'] = df['totalamount']\n","    df['ratio_totalcredit_contract_active'] = df['totalamount_503A'] /    df['sum_totalcredit_contract']\n","    df['ratio_totalcredit_contract_close'] = df['totalamount_6A'] /    df['sum_totalcredit_contract']\n","    df['ratio_overdue_debt_active'] = df['totaldebtoverduevalue_178A'] /    df['totaldebt_9A']\n","    df['ratio_overdue_debt_close'] = df['totaldebtoverduevalue_718A'] /    df['totaldebt_9A']\n","    df['sum_instalments'] = df['numberofinstls_229L'] +    df['numberofinstls_320L']\n","    df['ratio_instalments_active'] = df['numberofinstls_320L'] /    df['sum_instalments']\n","    df['ratio_instalments_close'] = df['numberofinstls_229L'] /    df['sum_instalments']\n","    '''\n","    df=df.pipe(Pipeline.filter_cols)\n","    \n","    columns_to_drop=[\n","     \n","   'min_pmts_year_1139T',              \n","'mean_pmts_year_1139T',                            \n","'max_pmts_year_1139T',                             \n","'min_pmts_year_507T' ,                               \n","'mean_pmts_year_507T',                              \n","'max_pmts_year_507T',\n","        \n","        'min_overdueamountmaxdateyear_2T'  ,               \n","'mean_overdueamountmaxdateyear_2T'   ,              \n","'max_overdueamountmaxdateyear_2T'  ,              \n","'min_overdueamountmaxdateyear_994T'  ,              \n","'mean_overdueamountmaxdateyear_994T' ,             \n","'max_overdueamountmaxdateyear_994T'\n","]\n","    columns_to_drop_existing = [col for col in columns_to_drop if col in df.columns]\n","\n","    df=df.drop(columns_to_drop_existing)\n","    \n","    \n","    \n","    features400=[\n","            #0\n","        'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n","        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n","        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n","        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n","        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n","        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n","        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n","        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n","        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n","        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n","        \n","        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n","        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n","        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n","        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n","        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n","        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n","        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n","        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n","        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n","        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n","        \n","         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n","        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n","        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n","        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n","        'pmtscount_423L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n","        'numinsttopaygr_769L','sum_credacc_actualbalance_314A','numinstunpaidmax_3546851L','max_empl_employedtotal_800L','price_1097A',\n","        'ratio_queries_30','max_postype_4733339M','mean_creationdate_885D','maxdebt4_972A',\n","        'mean_amount_4527230A','thirdquarter_1082L','sum_mainoccupationinc_437A','sum_pmtamount_36A','maxdpdinstldate_3546855D',\n","        'min_purposeofcred_874M','max_description_351M','description_5085714M','min_dtlastpmt_581D',\n","        'totaldebt_9A','max_contractst_964M','currdebt_22A','min_firstnonzeroinstldate_307D',\n","        \n","         'max_contractst_545M','max_purposeofcred_426M','max_pmts_dpd_303P',\n","        'max_classificationofcontr_13M','max_financialinstitution_591M','max_classificationofcontr_400M','max_financialinstitution_382M','count_num_group1',\n","        'eir_270L','interestrate_311L','min_purposeofcred_426M','maxdbddpdlast1m_3658939P',\n","        'sum_revolvingaccount_394A','max_firstnonzeroinstldate_307D','min_dateactivated_425D','firstdatedue_489D','min_approvaldate_319D',\n","        'min_refreshdate_3813885D','totalsettled_863A','count_persontype_1072L','max_credacc_cards_status_52L','min_creationdate_885D',\n","        'max_pmtamount_36A','min_pmtnum_8L','min_tenor_203L','count_num_group1_tax_registry_c','max_credacc_status_367L',\n","        'sumoutstandtotalest_4493215A','datelastunpaid_3546854D','sum_persontype_1072L','sumoutstandtotal_3546847A',\n","        'numinstls_657L','mean_dtlastpmtallstes_3545839D','max_familystate_447L','mean_pmtamount_36A',\n","        'max_purposeofcred_874M','maxdpdlast3m_392P','mean_credacc_actualbalance_314A','currdebtcredtyperange_828A',\n","        'sum_persontype_792L','lastapplicationdate_877D','max_creationdate_885D','max_outstandingdebt_522A','max_credacc_actualbalance_314A',\n","        \n","        'max_currdebt_94A','max_pmts_dpd_1073P','mean_credamount_590A','sum_personindex_1023L','min_credacc_actualbalance_314A',\n","        'sum_outstandingdebt_522A','min_amount_4527230A','mean_persontype_792L','mean_credacc_credlmt_575A','lastactivateddate_801D',\n","        'sum_currdebt_94A','count_personindex_1023L','count_relationshiptoclient_642T','max_dateactivated_425D','max_housetype_905L',\n","        'min_currdebt_94A','max_relationshiptoclient_415T','min_lastupdate_388D','min_processingdate_168D',\n","        'min_dateofrealrepmt_138D','mean_lastupdate_388D','max_persontype_1072L','max_persontype_792L','min_pmtamount_36A',\n","        'lastapprcommoditycat_1041M','pmtaverage_4527227A','annuity_780A','mean_dateofrealrepmt_138D',\n","        'sum_annuity_853A','mean_pmts_dpd_1073P','min_credacc_credlmt_575A','lastapprdate_640D','mean_credacc_minhisbal_90A',\n","        'min_dateofcredend_353D','mean_dtlastpmt_581D','max_credacc_minhisbal_90A','avgoutstandbalancel6m_4187114A','max_approvaldate_319D',\n","        'maxannuity_159A','min_credacc_minhisbal_90A','sum_amount_4917619A','cntincpaycont9m_3716944L','min_dateofcredstart_181D',\n","        'mean_refreshdate_3813885D','max_remitter_829L','pmtaverage_3A','avglnamtstart24m_4525187A','education_1103M',\n","       'mean_dpdmax_139P','mean_numberofoverdueinstlmax_1039L','min_recorddate_4527225D','min_annuity_853A',\n","        'max_dpdmax_139P','sum_dpdmax_139P','lastdelinqdate_224D','mean_persontype_1072L',\n","        'count_num_group1_cb_a_2','count_num_group2_cb_a_2','twobodfilling_608L','sum_numberofoverdueinstlmax_1039L','homephncnt_628L',\n","        'count_num_group1_tax_registry_b','datefirstoffer_1144D','max_numberofoverdueinstlmax_1039L','min_numberofoverdueinstlmax_1039L',\n","        'mean_downpmt_134A','max_empls_economicalst_849M','min_revolvingaccount_394A','responsedate_4527233D',\n","        'sum_isbidproduct_390L','max_mainoccupationinc_437A','count_familystate_447L','min_dateofcredstart_739D','max_amount_4917619A',\n","        'mean_dateofcredend_353D','min_dpdmax_139P','mean_revolvingaccount_394A','maininc_215A','lastrejectcredamount_222A',\n","        'max_processingdate_168D','min_totaldebtoverduevalue_178A','inittransactioncode_186L','max_deductiondate_4917603D',\n","        'min_deductiondate_4917603D','mean_processingdate_168D','contractssum_5085716L','mean_dateofcredstart_181D','applicationscnt_867L',\n","        'mean_amount_4917619A','max_revolvingaccount_394A','mean_isbidproduct_390L','mean_dateofcredstart_739D','min_pmts_dpd_1073P',\n","        \n","        'sum_credacc_credlmt_575A','min_pmts_dpd_303P','lastapprcredamount_781A','max_empl_industry_691L',\n","        'min_amount_4917619A','mean_annuity_853A',\n","       'max_overdueamountmaxdatemonth_365T','max_downpmt_134A','disbursementtype_67L',\n","        'min_overdueamountmaxdatemonth_284T','sum_numberofcontrsvalue_358L','count_num_group1_person2','sum_byoccupationinc_3656910L','mean_deductiondate_4917603D',\n","        'sellerplacescnt_216L','mean_overdueamountmaxdatemonth_365T','max_overdueamountmaxdatemonth_284T','mean_approvaldate_319D','credtype_322L',\n","        'max_numberofcontrsvalue_358L','mean_numberofcontrsvalue_358L','min_downpmt_134A','min_credacc_maxhisbal_375A','mean_isdebitcard_527L',\n","        'min_mainoccupationinc_384A','bankacctype_710L','mean_pmts_overdue_1152A','min_numberofcontrsvalue_358L','min_mainoccupationinc_437A',\n","        'min_residualamount_856A','mean_byoccupationinc_3656910L','downpmt_116A','isbidproduct_1095L','clientscnt12m_3712952L',\n","        'mean_credacc_maxhisbal_375A','max_nominalrate_281L','mean_dateactivated_425D','sum_downpmt_134A','mean_totaldebtoverduevalue_178A',\n","        \n","         'mean_numberofinstls_320L','max_numberofinstls_320L','max_isdebitcard_527L','mean_nominalrate_281L','dtlastpmtallstes_4499206D',\n","        'max_lastupdate_388D','responsedate_4917613D','sum_credacc_minhisbal_90A','max_byoccupationinc_3656910L',\n","        'min_credacc_transactions_402L','min_instlamount_768A','inittransactionamount_650A','max_totaldebtoverduevalue_178A','min_isdebitcard_527L',\n","        'clientscnt6m_3712949L','mean_credacc_transactions_402L','max_credacc_maxhisbal_375A','min_numberofinstls_320L','mean_totalamount_996A',\n","        'validfrom_1069D','count_num_group1_cb_a_1','mean_residualamount_856A','max_debtoverdue_47A',\n","        'max_dateofrealrepmt_138D','mean_totalamount_6A','min_pmts_overdue_1152A',\"max_cancelreason_3545846M\",\n","    ]\n","    \n","    features125=[\n","        \n","        'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n","        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n","        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n","        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n","        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n","        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n","        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n","        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n","        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n","        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n","        \n","        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n","        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n","        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n","        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n","        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n","        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n","        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n","        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n","        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n","        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n","         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n","        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n","        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n","        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n","        'pmtscount_423L','max_empl_industry_691L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n","        \n","    ]\n","    \n","    features200=[\n","         'max_role_1084L', 'max_language1_981M', 'max_sex_738L','max_incometype_1044T','max_education_927M',\n","        'max_type_25L','max_safeguarantyflag_411L','min_pmts_month_158T','min_pmts_month_706T','max_pmts_month_158T',\n","        'max_pmts_month_706T','mean_pmts_month_706T','mean_pmts_month_158T','pctinstlsallpaidlate1d_3546856L','pctinstlsallpaidlate4d_3546849L',\n","        'pctinstlsallpaidlate6d_3546844L','pctinstlsallpaidlat10d_839L','max_contaddr_smempladdr_334L','max_contaddr_matchlist_1032L','numinstlswithdpd10_728L',\n","        'pctinstlsallpaidearl3d_427L','lastrejectreason_759M','days120_123L','days180_256L','days90_310L',\n","        'lastrejectreasonclient_4145040M','lastcancelreason_561M','lastst_736L','numrejects9m_859L',\n","        'avgmaxdpdlast9m_3716943P','numberofqueries_373L','days360_512L','days30_165L','mobilephncnt_593L',\n","        'max_birth_259D','numcontrs3months_479L','dateofbirth_337D','numinstpaidearly3d_3546850L','sum_maxdpdtolerance_577P',\n","        'max_maxdpdtolerance_577P','numinstlallpaidearly3d_817L','maxdpdtolerance_374P','numinstlsallpaid_934L','daysoverduetolerancedd_3976961L',\n","        'max_employedfrom_700D','mean_maxdpdtolerance_577P','mean_employedfrom_700D','numinstpaidearly_338L','avgdpdtolclosure24_3658938P',\n","        \n","        'mean_pmtnum_8L','mean_tenor_203L','lastrejectdate_50D','numinstlswithoutdpd_562L','avgdbddpdlast24m_3658932P',\n","        'max_pmtnum_8L','max_tenor_203L','numinstpaidearly3dest_4493216L','numinstmatpaidtearly2d_4499204L','min_employedfrom_700D',\n","        'requesttype_4525192L','maxdpdfrom6mto36m_3546853P','maxdpdlast24m_143P','datelastinstal40dpd_247D','mindbddpdlast24m_3658935P',\n","        'amtinstpaidbefduel24m_4187115A','avgdbdtollast24m_4525197P','maxdpdlast12m_727P','maxdbddpdtollast12m_3658940P',\n","        'max_empl_employedfrom_271D','min_empl_employedfrom_271D','mean_empl_employedfrom_271D','min_maxdpdtolerance_577P','firstclxcampaign_1125D',\n","        'max_status_219L','sum_pmtnum_8L','sum_tenor_203L','monthsannuity_845L','pmtnum_254L',\n","        'max_education_1138M','maxdpdlast9m_1059P','sum_amount_4527230A','mindbdtollast24m_4525191P','min_subjectroles_name_541M',\n","        'birthdate_574D','max_subjectroles_name_838M','max_subjectroles_name_541M','min_subjectroles_name_838M','ratio_queries_120',\n","        'ratio_queries_90','mean_outstandingdebt_522A','cntpmts24_3658933L','ratio_queries_180','maxdbddpdtollast6m_4187119P',\n","        'mean_currdebt_94A','max_familystate_726L','numinstregularpaidest_4493210L','numincomingpmts_3546848L','numinstpaid_4499208L',\n","        \n","         'numinstregularpaid_973L','disbursedcredamount_1113A','secondquarter_766L','pmtssum_45A','min_dtlastpmtallstes_3545839D',\n","        'count_num_group1_tax_registry_a','max_inittransactioncode_279L','max_credtype_587L','max_conts_type_509L','min_isbidproduct_390L',\n","        'max_rejectreasonclient_4145042M','max_rejectreason_755M','max_isbidproduct_390L','max_amount_4527230A','mean_pmts_dpd_303P',\n","        'maxdpdlast6m_474P','mean_firstnonzeroinstldate_307D','max_relationshiptoclient_642T','sum_credamount_590A','credamount_770A',\n","        'pmtscount_423L','numinstunpaidmaxest_4493212L','numinsttopaygrest_4493213L','avgdbddpdlast3m_4187120P',\n","        'numinsttopaygr_769L','sum_credacc_actualbalance_314A','numinstunpaidmax_3546851L','max_empl_employedtotal_800L','price_1097A',\n","        'ratio_queries_30','max_postype_4733339M','mean_creationdate_885D','maxdebt4_972A',\n","        'mean_amount_4527230A','thirdquarter_1082L','sum_mainoccupationinc_437A','sum_pmtamount_36A','maxdpdinstldate_3546855D',\n","        'min_purposeofcred_874M','max_description_351M','description_5085714M','min_dtlastpmt_581D',\n","        'totaldebt_9A','max_contractst_964M','currdebt_22A','min_firstnonzeroinstldate_307D',\n","        \n","         'max_contractst_545M','max_purposeofcred_426M','max_pmts_dpd_303P',\n","        'max_classificationofcontr_13M','max_financialinstitution_591M','max_classificationofcontr_400M','max_financialinstitution_382M','count_num_group1',\n","        'eir_270L','interestrate_311L','min_purposeofcred_426M','maxdbddpdlast1m_3658939P',\n","        'sum_revolvingaccount_394A','max_firstnonzeroinstldate_307D','min_dateactivated_425D','firstdatedue_489D','min_approvaldate_319D',\n","        'min_refreshdate_3813885D','totalsettled_863A','count_persontype_1072L','max_credacc_cards_status_52L','min_creationdate_885D',\n","        'max_pmtamount_36A','min_pmtnum_8L','min_tenor_203L','count_num_group1_tax_registry_c','max_credacc_status_367L',\n","        'sumoutstandtotalest_4493215A','datelastunpaid_3546854D','sum_persontype_1072L','sumoutstandtotal_3546847A',\n","        'numinstls_657L','mean_dtlastpmtallstes_3545839D','max_familystate_447L','mean_pmtamount_36A',\n","        'max_purposeofcred_874M','maxdpdlast3m_392P','mean_credacc_actualbalance_314A','currdebtcredtyperange_828A',\n","        'sum_persontype_792L','lastapplicationdate_877D','max_creationdate_885D','max_outstandingdebt_522A','max_credacc_actualbalance_314A',\n","        \n","        'max_currdebt_94A','max_pmts_dpd_1073P','mean_credamount_590A','sum_personindex_1023L','min_credacc_actualbalance_314A',\n","        'sum_outstandingdebt_522A','min_amount_4527230A','mean_persontype_792L','mean_credacc_credlmt_575A','lastactivateddate_801D',\n","        'sum_currdebt_94A','count_personindex_1023L','count_relationshiptoclient_642T','max_dateactivated_425D','max_housetype_905L',\n","        'min_currdebt_94A','max_relationshiptoclient_415T','min_lastupdate_388D','min_processingdate_168D',\n","        'min_dateofrealrepmt_138D','mean_lastupdate_388D','max_persontype_1072L','max_persontype_792L','min_pmtamount_36A',\n","        'lastapprcommoditycat_1041M','pmtaverage_4527227A','annuity_780A','mean_dateofrealrepmt_138D'\n","    ]\n","    seen = set()\n","    duplicates = []\n","    for item in features400:\n","        if item in seen:\n","            duplicates.append(item)\n","        else:\n","            seen.add(item)\n","    print(\"duplicates:\")\n","    print(duplicates)\n","    print()\n","    \n","    print(\"Length of features400: \", len(features400))\n","    print(\"Length of features200: \", len(features200))\n","    print(\"Length of features125: \", len(features125))\n","    missing_columns = [col for col in features400 if col not in df.columns]\n","\n","    # Print missing columns, if any\n","    if missing_columns:\n","        print(\"The following columns are missing in the DataFrame:\")\n","        for col in missing_columns:\n","            print(col)\n","    else:\n","        print(\"All columns from features400 are present in the DataFrame.\")\n","    \n","    # Columns to preserve\n","    preserved_columns = ['target', 'case_id', 'WEEK_NUM',\"collater_typofvalofguarant_298M\",\"classificationofcontr_400M\",\"contractst_964M\",\"financialinstitution_382M\",\"financialinstitution_591M\",\"numberofoverdueinstlmaxdat_641D\",\"prolongationcount_1120L\"]\n","\n","    # Identify columns to drop excluding the preserved columns\n","    columns_to_drop = [col for col in df.columns if col not in preserved_columns + features400]\n","\n","    # Drop columns that are not in features_selected and not preserved\n","    \n","    #df=df.drop(columns=columns_to_drop)\n","    \n","    get_info(df)\n","    return df"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.104932Z","iopub.status.busy":"2024-03-25T16:01:42.104639Z","iopub.status.idle":"2024-03-25T16:01:42.123038Z","shell.execute_reply":"2024-03-25T16:01:42.121873Z","shell.execute_reply.started":"2024-03-25T16:01:42.104905Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\ndf = get_base(DATA_DIRECTORY)\\ndf_applprev1 = get_applprev1(DATA_DIRECTORY)\\ndf_applprev1 = df_applprev1.filter(pl.col(\\'case_id\\').is_in(df[\\'case_id\\'].unique()))\\ndf = df.join(df_applprev1, on=\\'case_id\\', how=\\'left\\', suffix=\\'_applprev1\\')\\nprint(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\\nprint(\"DATAFRAME shape:\", df.shape)\\ndel df_applprev1\\ngc.collect()\\n\\n\\ndf=feature_engineering(df)\\nprint(df[\\'credit_income_percent\\'])\\n'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","df = get_base(DATA_DIRECTORY)\n","df_applprev1 = get_applprev1(DATA_DIRECTORY)\n","df_applprev1 = df_applprev1.filter(pl.col('case_id').is_in(df['case_id'].unique()))\n","df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n","print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n","print(\"DATAFRAME shape:\", df.shape)\n","del df_applprev1\n","gc.collect()\n","\n","\n","df=feature_engineering(df)\n","print(df['credit_income_percent'])\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["# **GET FUNCTIONS**"]},{"cell_type":"markdown","metadata":{},"source":["### get_base()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.124552Z","iopub.status.busy":"2024-03-25T16:01:42.124288Z","iopub.status.idle":"2024-03-25T16:01:42.133538Z","shell.execute_reply":"2024-03-25T16:01:42.132590Z","shell.execute_reply.started":"2024-03-25T16:01:42.124530Z"},"trusted":true},"outputs":[],"source":["def get_base(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    train={}\n","    test={}\n","    \n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet'))\n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet')).limit(num_rows) \n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_base.parquet'))    \n","    length=len(test)\n","    nan_series=pl.Series([None] * length)\n","    test = test.select(pl.col(\"*\"), nan_series.alias(\"target\"))\n","    df=pl.concat([train, test])\n","    del test;del train;gc.collect()\n","    \n","    \n","    df = df.with_columns(pl.col('date_decision').cast(pl.Date))\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### get_static()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.135410Z","iopub.status.busy":"2024-03-25T16:01:42.134927Z","iopub.status.idle":"2024-03-25T16:01:42.149435Z","shell.execute_reply":"2024-03-25T16:01:42.148278Z","shell.execute_reply.started":"2024-03-25T16:01:42.135384Z"},"trusted":true},"outputs":[],"source":["def get_static(path, num_rows = None):\n","# Read the Parquet file using scan() method\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_static_0_*.parquet')):\n","        chunks.append(pl.read_parquet(path,low_memory=True).pipe(Pipeline.set_table_dtypes) )\n","    train = (pl.concat(chunks, how=\"vertical_relaxed\")).pipe(Pipeline.filter_cols)\n","    \n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(train))\n","        \n","        train=df1\n","        del df2\n","        gc.collect()\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_static_0_*.parquet')):\n","        chunks.append(pl.read_parquet(path,low_memory=True).pipe(Pipeline.set_table_dtypes) )\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    \n","    \n","    columns_to_keep = train.columns\n","\n","# Find columns in 'test' that are not in 'train'\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","\n","# Drop columns from 'test' that are not in 'train'\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    del test;del train;gc.collect()\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### get_static_cb()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.151269Z","iopub.status.busy":"2024-03-25T16:01:42.150943Z","iopub.status.idle":"2024-03-25T16:01:42.163770Z","shell.execute_reply":"2024-03-25T16:01:42.162436Z","shell.execute_reply.started":"2024-03-25T16:01:42.151239Z"},"trusted":true},"outputs":[],"source":["def get_static_cb(path, num_rows = None):\n","    \n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet'),low_memory=True).limit(num_rows).pipe(Pipeline.set_table_dtypes) \n","       \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_static_cb_0.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del test;del train;gc.collect()\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### get_applprev1(DATA_DIRECTORY, num_rows=num_rows)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.165472Z","iopub.status.busy":"2024-03-25T16:01:42.165109Z","iopub.status.idle":"2024-03-25T16:01:42.180443Z","shell.execute_reply":"2024-03-25T16:01:42.179495Z","shell.execute_reply.started":"2024-03-25T16:01:42.165444Z"},"trusted":true},"outputs":[],"source":["def get_applprev1(path, num_rows = None):\n","    \n","    \n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_applprev_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n","    train = pl.concat(chunks, how=\"vertical_relaxed\")#.pipe(Pipeline.filter_cols)\n","    \n","    \n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(train))\n","\n","        train=df1\n","        del df2   \n","        gc.collect()\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_applprev_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes))\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    del test;del train;gc.collect()\n","    agg_df = group(df, '', APPLPREV1_AGG)\n","    del df;gc.collect()\n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_applprev2(DATA_DIRECTORY, num_rows=num_rows)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.183053Z","iopub.status.busy":"2024-03-25T16:01:42.182515Z","iopub.status.idle":"2024-03-25T16:01:42.194194Z","shell.execute_reply":"2024-03-25T16:01:42.192539Z","shell.execute_reply.started":"2024-03-25T16:01:42.183015Z"},"trusted":true},"outputs":[],"source":["def get_applprev2(path, num_rows = None):\n","    train={}\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","     \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","       \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_applprev_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', APPLPREV2_AGG)\n","    del df ;gc.collect()\n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_person1"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.196222Z","iopub.status.busy":"2024-03-25T16:01:42.195935Z","iopub.status.idle":"2024-03-25T16:01:42.213107Z","shell.execute_reply":"2024-03-25T16:01:42.211811Z","shell.execute_reply.started":"2024-03-25T16:01:42.196200Z"},"trusted":true},"outputs":[],"source":["def get_person1(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","      \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_person_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', PERSON1_AGG)\n","    del df;gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_person2"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.214572Z","iopub.status.busy":"2024-03-25T16:01:42.214331Z","iopub.status.idle":"2024-03-25T16:01:42.226339Z","shell.execute_reply":"2024-03-25T16:01:42.225180Z","shell.execute_reply.started":"2024-03-25T16:01:42.214550Z"},"trusted":true},"outputs":[],"source":["def get_person2(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes)\n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_person_2.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', PERSON2_AGG)\n","    del df;gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### other"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.227585Z","iopub.status.busy":"2024-03-25T16:01:42.227319Z","iopub.status.idle":"2024-03-25T16:01:42.243196Z","shell.execute_reply":"2024-03-25T16:01:42.241982Z","shell.execute_reply.started":"2024-03-25T16:01:42.227561Z"},"trusted":true},"outputs":[],"source":["def get_other(path, num_rows = None):\n","     # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","         \n","    test = pl.read_parquet(os.path.join(path, 'test/test_other_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    \n","    \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', OTHER_AGG)\n","    del df;gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["## get_debitcard"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.248366Z","iopub.status.busy":"2024-03-25T16:01:42.248013Z","iopub.status.idle":"2024-03-25T16:01:42.256868Z","shell.execute_reply":"2024-03-25T16:01:42.255488Z","shell.execute_reply.started":"2024-03-25T16:01:42.248337Z"},"trusted":true},"outputs":[],"source":["def get_debitcard(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","     \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","      \n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_debitcard_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    \n","    \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', DEBITCARD_AGG)\n","    del df;gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_a"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.259122Z","iopub.status.busy":"2024-03-25T16:01:42.258792Z","iopub.status.idle":"2024-03-25T16:01:42.272965Z","shell.execute_reply":"2024-03-25T16:01:42.271513Z","shell.execute_reply.started":"2024-03-25T16:01:42.259099Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_a(path, num_rows = None):\n","    \n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","  \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_a_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', TAX_REGISTRY_A_AGG)    \n","    del df;gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_b"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.274837Z","iopub.status.busy":"2024-03-25T16:01:42.274464Z","iopub.status.idle":"2024-03-25T16:01:42.283713Z","shell.execute_reply":"2024-03-25T16:01:42.282735Z","shell.execute_reply.started":"2024-03-25T16:01:42.274796Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_b(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes)\n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_b_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', TAX_REGISTRY_B_AGG) \n","    del df;gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_c"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.284712Z","iopub.status.busy":"2024-03-25T16:01:42.284467Z","iopub.status.idle":"2024-03-25T16:01:42.300478Z","shell.execute_reply":"2024-03-25T16:01:42.299139Z","shell.execute_reply.started":"2024-03-25T16:01:42.284691Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_c(path, num_rows = None):\n","     # Read the Parquet file using scan() method\n","# Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","        \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_c_1.parquet'),low_memory=True).pipe(Pipeline.set_table_dtypes) \n","    \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test;gc.collect()\n","    agg_df = group(df, '', TAX_REGISTRY_C_AGG)    \n","    del df; gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_a_1"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.302281Z","iopub.status.busy":"2024-03-25T16:01:42.301878Z","iopub.status.idle":"2024-03-25T16:01:42.313981Z","shell.execute_reply":"2024-03-25T16:01:42.312564Z","shell.execute_reply.started":"2024-03-25T16:01:42.302247Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_a_1(path, num_rows = None):\n","    \n","    \n","    \n","    agg_chunks=[]\n","    \n","    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_1_*.parquet'):\n","        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n","        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n","        agg_chunks.append(agg_file_df)\n","        del file_df; gc.collect()\n","    \n","    \n","    train_agg_df=agg_chunks[0]\n","    for agg_chunk in agg_chunks[1:]:\n","        train_agg_df.vstack(agg_chunk)\n","    train_agg_df.rechunk()\n","        \n","        \n","    \n","    \n","    \n","    agg_chunks=[]\n","    \n","    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_1_*.parquet'):\n","        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n","        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_1_AGG, datatype='polars')\n","        agg_chunks.append(agg_file_df)\n","        del file_df; gc.collect()\n","        \n","        \n","    test_agg_df=agg_chunks[0]\n","    for agg_chunk in agg_chunks[1:]:\n","        test_agg_df.vstack(agg_chunk)\n","    test_agg_df.rechunk()\n","    \n","    \n","    \n","\n","    \n","    agg_df=train_agg_df\n","    agg_df.extend(test_agg_df)\n","    \n","\n","    \n","    \n","    print(\"agg df \", agg_df.shape)\n","   \n","    unique_count = agg_df['case_id'].n_unique()\n","\n","    print(\"Number of unique values in 'case_id' column:\", unique_count)\n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_b_1"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.315658Z","iopub.status.busy":"2024-03-25T16:01:42.315355Z","iopub.status.idle":"2024-03-25T16:01:42.330531Z","shell.execute_reply":"2024-03-25T16:01:42.329211Z","shell.execute_reply.started":"2024-03-25T16:01:42.315632Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_b_1(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n","   \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes) \n","    \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    del train; del test ; gc.collect()\n","    agg_df = group(df, '', CREDIT_BUREAU_B_1_AGG) \n","    \n","    \n","    del df; gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_a_2"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.332030Z","iopub.status.busy":"2024-03-25T16:01:42.331730Z","iopub.status.idle":"2024-03-25T16:01:42.347809Z","shell.execute_reply":"2024-03-25T16:01:42.346938Z","shell.execute_reply.started":"2024-03-25T16:01:42.332002Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_a_2(path, num_rows = None):\n","    \n","    \n","    \n","    agg_chunks=[]\n","    \n","    for path in glob(DATA_DIRECTORY + 'train/train_credit_bureau_a_2_*.parquet'):\n","        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n","        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n","        agg_chunks.append(agg_file_df)\n","        del file_df;gc.collect()\n","    \n","    train_agg_df=agg_chunks[0]\n","    for agg_chunk in agg_chunks[1:]:\n","        train_agg_df.vstack(agg_chunk)\n","    train_agg_df.rechunk()\n","    \n","    \n","    agg_chunks=[]\n","    \n","    for path in glob(DATA_DIRECTORY + 'test/test_credit_bureau_a_2_*.parquet'):\n","        file_df=pl.read_parquet(path, low_memory=True).pipe(Pipeline.set_table_dtypes)\n","        agg_file_df = group(file_df, '', CREDIT_BUREAU_A_2_AGG, datatype='polars')\n","        agg_chunks.append(agg_file_df)\n","        del file_df;gc.collect()\n","    \n","    test_agg_df=agg_chunks[0]\n","    for agg_chunk in agg_chunks[1:]:\n","        test_agg_df.vstack(agg_chunk)\n","    test_agg_df.rechunk()\n","    \n","    agg_df=train_agg_df\n","    agg_df.extend(test_agg_df)\n","    \n","    print(\"agg df \", agg_df.shape)\n","   \n","    unique_count = agg_df['case_id'].n_unique()\n","\n","    print(\"Number of unique values in 'case_id' column:\", unique_count)\n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_b_2"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.349214Z","iopub.status.busy":"2024-03-25T16:01:42.348947Z","iopub.status.idle":"2024-03-25T16:01:42.364172Z","shell.execute_reply":"2024-03-25T16:01:42.363244Z","shell.execute_reply.started":"2024-03-25T16:01:42.349191Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_b_2(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n","   \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n","\n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes) \n","    \n","    #train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    \n","    df=pl.concat([train, test])\n","    del train;del test; gc.collect()\n","    agg_df = group(df, '', CREDIT_BUREAU_B_2_AGG) \n","    \n","    del df; gc.collect()\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["# **EXECUTION**"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:01:42.366103Z","iopub.status.busy":"2024-03-25T16:01:42.365753Z","iopub.status.idle":"2024-03-25T16:03:56.892324Z","shell.execute_reply":"2024-03-25T16:03:56.890683Z","shell.execute_reply.started":"2024-03-25T16:01:42.366079Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["base dataframe shape: (11121, 5)\n","base - done in 0s\n","static dataframe shape: (11121, 156)\n","DATAFRAME shape: (11121, 160)\n","static - done in 2s\n","static cb dataframe shape: (10541, 23)\n","DATAFRAME shape: (11121, 182)\n","static_cb - done in 0s\n","Previous applications depth 1 test dataframe shape: (4531, 114)\n","DATAFRAME shape: (11121, 295)\n","Previous applications depth 1 test - done in 3s\n","Previous applications depth 2 test dataframe shape: (4530, 5)\n","DATAFRAME shape: (11121, 299)\n","Previous applications depth 2 test - done in 1s\n","Person depth 1 test dataframe shape: (11117, 47)\n","DATAFRAME shape: (11121, 345)\n","Person depth 1 test - done in 2s\n","Person depth 2 test dataframe shape: (10706, 4)\n","DATAFRAME shape: (11121, 348)\n","Person depth 2 test - done in 0s\n","Other test dataframe shape: (2, 12)\n","DATAFRAME shape: (11121, 359)\n","Other test - done in 0s\n","Debit card test dataframe shape: (152, 5)\n","DATAFRAME shape: (11121, 363)\n","Debit card test - done in 0s\n","Tax registry a test dataframe shape: (0, 10)\n","DATAFRAME shape: (11121, 372)\n","Tax registry a test - done in 1s\n","Tax registry b test dataframe shape: (2, 10)\n","DATAFRAME shape: (11121, 381)\n","Tax registry b test - done in 0s\n","Tax registry c test dataframe shape: (6592, 10)\n","DATAFRAME shape: (11121, 390)\n","Tax registry c test - done in 1s\n","agg df  (335276, 134)\n","Number of unique values in 'case_id' column: 335276\n","Credit bureau a 1 test dataframe shape: (7283, 134)\n","DATAFRAME shape: (11121, 523)\n","Credit bureau a 1 test - done in 15s\n","Credit bureau b 1 test dataframe shape: (15, 2)\n","DATAFRAME shape: (11121, 524)\n","Credit bureau b 1 test - done in 0s\n","agg df  (98304, 33)\n","Number of unique values in 'case_id' column: 98304\n","Credit bureau a 2 test dataframe shape: (4431, 33)\n","DATAFRAME shape: (11121, 556)\n","Credit bureau a 2 test - done in 31s\n","Credit bureau b 2 test dataframe shape: (15, 12)\n","DATAFRAME shape: (11121, 567)\n","Credit bureau b 2 test - done in 0s\n"]},{"ename":"ColumnNotFoundError","evalue":"collater_typofvalofguarant_298M\n\nError originated just after this operation:\nDF [\"case_id\", \"WEEK_NUM\", \"target\", \"actualdpdtolerance_344P\"]; PROJECT */569 COLUMNS; SELECTION: \"None\"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline total time\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[4], line 168\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(debug)\u001b[0m\n\u001b[0;32m    164\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature engineering / preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m--> 168\u001b[0m     df\u001b[38;5;241m=\u001b[39m\u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     df_pandas, cat_cols \u001b[38;5;241m=\u001b[39m to_pandas(df)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m df;gc\u001b[38;5;241m.\u001b[39mcollect()\n","Cell \u001b[1;32mIn[42], line 54\u001b[0m, in \u001b[0;36mfeature_engineering\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Add the calculated columns to the DataFrame\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns_to_add:\n\u001b[1;32m---> 54\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    df = df.with_columns(\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    'ratio_queries_30',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    df['ratio_instalments_close'] = df['numberofinstls_229L'] /    df['sum_instalments']\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mpipe(Pipeline\u001b[38;5;241m.\u001b[39mfilter_cols)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\polars\\dataframe\\frame.py:8289\u001b[0m, in \u001b[0;36mDataFrame.with_columns\u001b[1;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[0;32m   8143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_columns\u001b[39m(\n\u001b[0;32m   8144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8145\u001b[0m     \u001b[38;5;241m*\u001b[39mexprs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[0;32m   8146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs: IntoExpr,\n\u001b[0;32m   8147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   8148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8149\u001b[0m \u001b[38;5;124;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[0;32m   8150\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8287\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   8288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 8289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\polars\\lazyframe\\frame.py:1934\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\u001b[0m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m background:\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InProcessQuery(ldf\u001b[38;5;241m.\u001b[39mcollect_concurrently())\n\u001b[1;32m-> 1934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(ldf\u001b[38;5;241m.\u001b[39mcollect())\n","\u001b[1;31mColumnNotFoundError\u001b[0m: collater_typofvalofguarant_298M\n\nError originated just after this operation:\nDF [\"case_id\", \"WEEK_NUM\", \"target\", \"actualdpdtolerance_344P\"]; PROJECT */569 COLUMNS; SELECTION: \"None\""]}],"source":["\n","if __name__ == \"__main__\":\n","    pd.set_option('display.max_rows', 60)\n","    pd.set_option('display.max_columns', 100)\n","    with timer(\"Pipeline total time\"):\n","        main(debug= True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
