{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **LIBRARIES**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:54:58.991548Z","iopub.status.busy":"2024-03-19T17:54:58.991127Z","iopub.status.idle":"2024-03-19T17:55:00.971565Z","shell.execute_reply":"2024-03-19T17:55:00.970886Z","shell.execute_reply.started":"2024-03-19T17:54:58.991508Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import time\n","import numpy as np\n","import pandas as pd\n","from contextlib import contextmanager\n","import multiprocessing as mp\n","from functools import partial\n","from scipy.stats import kurtosis, iqr, skew\n","from lightgbm import LGBMClassifier\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","from glob import glob\n","from pathlib import Path\n","from datetime import datetime\n","import polars as pl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.metrics import roc_auc_score \n","from sklearn.metrics import roc_curve, auc\n","from tqdm.notebook import tqdm\n","import joblib\n","import warnings\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n"]},{"cell_type":"markdown","metadata":{},"source":["# **CONFIGURATION**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:00.973318Z","iopub.status.busy":"2024-03-19T17:55:00.972827Z","iopub.status.idle":"2024-03-19T17:55:00.979129Z","shell.execute_reply":"2024-03-19T17:55:00.978058Z","shell.execute_reply.started":"2024-03-19T17:55:00.973294Z"},"trusted":true},"outputs":[],"source":["# GENERAL CONFIGURATIONS\n","NUM_THREADS = 16\n","DATA_DIRECTORY = \"../parquet_files/\"\n","SUBMISSION_SUFIX = \"_model2_0\"\n","# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\n","GENERATE_SUBMISSION_FILES = True\n","EVALUATE_VALIDATION_SET = True\n","SHOW_REPORT = True\n","STRATIFIED_KFOLD = True\n","RANDOM_SEED = 737851\n","NUM_FOLDS = 10\n","EARLY_STOPPING = 100\n","ROOT            = Path(\"./\")\n","\n","LIGHTGBM_PARAMS = {\n","    'boosting_type': 'goss',\n","    'n_estimators': 1000,\n","    'learning_rate': 0.005134,\n","    'num_leaves': 54,\n","    'max_depth': 10,\n","    'subsample_for_bin': 240000,\n","    'reg_alpha': 0.436193,\n","    'reg_lambda': 0.479169,\n","    'colsample_bytree': 0.508716,\n","    'min_split_gain': 0.024766,\n","    'subsample': 1,\n","    'is_unbalance': False,\n","    'silent':-1,\n","    'verbose':-1,\n","    \n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Set aggregations (all)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:00.980189Z","iopub.status.busy":"2024-03-19T17:55:00.979982Z","iopub.status.idle":"2024-03-19T17:55:00.995183Z","shell.execute_reply":"2024-03-19T17:55:00.994554Z","shell.execute_reply.started":"2024-03-19T17:55:00.980170Z"},"trusted":true},"outputs":[],"source":["# AGGREGATIONS\n","\n","\n","# all features with split > 750\n","# all features with gain > 10000\n","\n","APPLPREV1_AGG = {\n","\n","    'cancelreason_3545846M' : ['min','mean','max'],\n","    'employedfrom_700D' : ['max','mean','min'],\n","    'dtlastpmt_581D' : ['max','min','mean'],\n","    'pmtnum_8L' : ['mean','min','max'],\n","    'tenor_203L' : ['mean','min','max'],\n","    'firstnonzeroinstldate_307D':['max','mean','min'],\n","    'mainoccupationinc_437A' : ['mean','min','max'],\n","    'annuity_853A': ['max','min'],\n","    'maxdpdtolerance_577P':['mean','max','min'],\n","    'dtlastpmtallstes_3545839D':['max','mean','min'],\n","    'credamount_590A':['min','mean','max'],\n","    'outstandingdebt_522A':['mean','max','min'],\n","    'dtlastpmt_581D':['min','mean','max'],\n","    'creationdate_885D':['max','min','mean'],\n","    'rejectreason_755M':['min'],\n","    'currdebt_94A':['max'],\n","    'approvaldate_319D':['max','min','mean'],\n","    'dateactivated_425D':['mean','min','max'],\n","    'familystate_726L':['max','min','mean'],\n","    'inittransactioncode_279L':['min'],\n","    'credtype_587L':['min'],\n","    'education_1138M':['min','max','mean']\n","    \n","}\n","APPLPREV2_AGG = {\n","    \n","}\n","PERSON1_AGG={\n","    'birth_259D': ['max'],\n","    'education_927M' : ['min'],\n","    'incometype_1044T':['max','min','mean'],\n","    'sex_738L':['max','min','mean'],\n","    'empl_employedfrom_271D':['max','mean','min'],\n","    'min_relationshiptoclient_642T'\n","    'relationshiptoclient_642T':['max','mean','min'],\n","    'familystate_447L':['max','min','mean'],\n","    'mainoccupationinc_384A':['max','mean'],\n","    'relationshiptoclient_415T':['max','min','mean'],\n","    'language1_981M':['min','max'],\n","    \n","    \n","}\n","PERSON2_AGG={}\n","OTHER_AGG={}\n","DEBITCARD_AGG={}\n","TAX_REGISTRY_A_AGG={\n","    'amount_4527230A': ['max','mean','min']\n","    \n","}\n","TAX_REGISTRY_B_AGG={\n","    'amount_4527230A': ['max','mean','min'],\n","    'amount_4917619A':['min','mean','max']\n","}\n","TAX_REGISTRY_C_AGG={\n","    'amount_4527230A': ['max','mean','min'],\n","    'pmtamount_36A':['min','mean','max'],\n","    'processingdate_168D':['mean','min','max']\n","}\n","CREDIT_BUREAU_B_1_AGG={}\n","CREDIT_BUREAU_B_2_AGG={}\n"]},{"cell_type":"markdown","metadata":{},"source":["### Set Aggregations (strict)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# AGGREGATIONS\n","\n","\n","# all features with split > 750\n","# all features with gain > 10000\n","\n","\n","# all features are arranged in the order from high importance to low \n","APPLPREV1_AGG = {\n","\n","    'maxdpdtolerance_577P':['mean','max','min'],        # rank 2 \n","    'rejectreason_755M':['min'],                        # rank 9 \n","    'cancelreason_3545846M' : ['min','mean','max'],     # rank 12\n","    'employedfrom_700D' : ['max','mean','min'],         # rank 34\n","    'firstnonzeroinstldate_307D':['max','mean','min'],  # rank 45, 112, 166\n","    'outstandingdebt_522A':['mean'],                    # rank 52\n","    'tenor_203L' : ['mean'],                            # rank 54\n","    'currdebt_94A':['mean'],                            # rank 64\n","    'annuity_853A': ['max','min'],                      # rank 95, 132, 163\n","    'dtlastpmt_581D' : ['max'],                         # rank 98 \n","    'pmtnum_8L' : ['mean','min','max'],                 # not found\n","    'mainoccupationinc_437A' : ['min','mean','max'],    # rank 116, 126, 179\n","    'dtlastpmtallstes_3545839D':['max','mean','min'],   # rank 117, 155, 156\n","    'credamount_590A':['min','mean','max'],             # rank 128, 149, 175\n","    'creationdate_885D':['max','min','mean'],           # rank 141, 162, 188 (min creationdate_885D is last of all)\n","    'approvaldate_319D':['max'],                        # rank 169\n","    'dateactivated_425D':['mean','min','max'],          # not found\n","    'familystate_726L':['max','min','mean'],            # not found\n","    'inittransactioncode_279L':['min'],                 # not found\n","    'credtype_587L':['min'],                            # not found\n","    'education_1138M':['min','max','mean']              # not found\n","    \n","}\n","\n","APPLPREV2_AGG = {\n","    'cacccardblochreas_147M': ['min', 'mean', 'max'],\n","    'conts_type_509L': ['min', 'mean', 'max'],\n","    'credacc_cards_status_52L': ['min', 'mean', 'max'],\n","    'num_group1': ['min', 'mean', 'max'],\n","    'num_group2': ['min', 'mean', 'max']\n","}\n","\n","PERSON1_AGG={\n","    'birth_259D': ['max'],                              # rank 3\n","    'incometype_1044T':['max','min'],                   # rank 17, 18 \n","    'empl_employedfrom_271D':['max','mean','min'],      # rank 32, 35, 37\n","    'relationshiptoclient_642T':['min'],                # rank 62, \n","    'relationshiptoclient_415T':['min'],                # rank 66\n","    'maininc_215A':['min','mean','max'],                # rank 112\n","    'mainoccupationinc_384A':['min','mean','max'],      # not found\n","    'education_927M' : ['min'],                         # not found\n","}\n","\n","# I could not found any feature with split > 750 or gain > 10000 for these tables\n","PERSON2_AGG={}\n","\n","OTHER_AGG={\n","    'amtdebitincoming_4809443A': ['min', 'mean', 'max'],\n","    'amtdebitoutgoing_4809440A': ['min', 'mean', 'max'],\n","    'amtdepositbalance_4809441A': ['min', 'mean', 'max'],\n","    'amtdepositincoming_4809444A': ['min', 'mean', 'max'],\n","    'amtdepositoutgoing_4809442A': ['min', 'mean', 'max'],\n","    'num_group1': ['min', 'mean', 'max']\n","}\n","\n","\n","DEBITCARD_AGG={\n","    'last180dayaveragebalance_704A': ['min', 'mean', 'max'],\n","    'last180dayturnover_1134A': ['min', 'mean', 'max'],\n","    'last30dayturnover_651A': ['min', 'mean', 'max'],\n","    'num_group1': ['min', 'mean', 'max'],\n","    'openingdate_857D': ['min', 'mean', 'max']\n","}\n","TAX_REGISTRY_A_AGG={\n","    'amount_4527230A': ['max','mean','min']\n","    \n","}\n","TAX_REGISTRY_B_AGG={\n","    \n","    'amount_4917619A':['min','mean','max']\n","}\n","TAX_REGISTRY_C_AGG={\n","    'pmtamount_36A':['min','mean','max'],\n","    'processingdate_168D':['mean','min','max']\n","}\n","\n","# \n","CREDIT_BUREAU_B_1_AGG={\n","    'collater_typofvalofguarant_298M': ['min', 'mean', 'max'],\n","    'collater_typofvalofguarant_407M': ['min', 'mean', 'max'],\n","    'collater_valueofguarantee_1124L': ['min', 'mean', 'max'],\n","    'collater_valueofguarantee_876L': ['min', 'mean', 'max'],\n","    'collaterals_typeofguarante_359M': ['min', 'mean', 'max'],\n","    'collaterals_typeofguarante_669M': ['min', 'mean', 'max'],\n","    'num_group1': ['min', 'mean', 'max'],\n","    'num_group2': ['min', 'mean', 'max'],\n","    'pmts_dpd_1073P': ['min', 'mean', 'max'],\n","    'pmts_dpd_303P': ['min', 'mean', 'max'],\n","    'pmts_month_158T': ['min', 'mean', 'max'],\n","    'pmts_month_706T': ['min', 'mean', 'max'],\n","    'pmts_overdue_1140A': ['min', 'mean', 'max'],\n","    'pmts_overdue_1152A': ['min', 'mean', 'max'],\n","    'pmts_year_1139T': ['min', 'mean', 'max'],\n","    'pmts_year_507T': ['min', 'mean', 'max'],\n","    'subjectroles_name_541M': ['min', 'mean', 'max'],\n","    'subjectroles_name_838M': ['min', 'mean', 'max']\n","}\n","\n","CREDIT_BUREAU_B_2_AGG={}\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# **MAIN FUNCTION**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:00.997462Z","iopub.status.busy":"2024-03-19T17:55:00.996968Z","iopub.status.idle":"2024-03-19T17:55:01.015022Z","shell.execute_reply":"2024-03-19T17:55:01.014225Z","shell.execute_reply.started":"2024-03-19T17:55:00.997425Z"},"trusted":true},"outputs":[],"source":["def main(debug= False):\n","    num_rows = 11111 if debug else None\n","    with timer(\"base\"):\n","        \n","        df = get_base(DATA_DIRECTORY, num_rows=num_rows)\n","        print(\"base dataframe shape:\", df.shape)\n","\n","    with timer(\"static\"):\n","       \n","        df_static = get_static(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_static, on='case_id', how='left', suffix='_static')\n","        print(\"static dataframe shape:\", df_static.shape)\n","        del df_static\n","        gc.collect()\n","\n","    with timer(\"static_cb\"):\n","       \n","        df_static_cb = get_static_cb(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_static_cb, on='case_id', how='left', suffix='_static_cb')\n","        print(\"static cb dataframe shape:\", df_static_cb.shape)\n","        del df_static_cb\n","        gc.collect()\n","\n","    with timer(\"Previous applications depth 1 test\"):\n","       \n","        df_applprev1 = get_applprev1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n","        print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n","        del df_applprev1\n","        gc.collect()\n","\n","    with timer(\"Previous applications depth 2 test\"):\n","       \n","        df_applprev2 = get_applprev2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_applprev2, on='case_id', how='left', suffix='_applprev2')\n","        print(\"Previous applications depth 2 test dataframe shape:\", df_applprev2.shape)\n","        del df_applprev2\n","        gc.collect()\n","\n","    with timer(\"Person depth 1 test\"):\n","       \n","        df_person1 = get_person1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_person1, on='case_id', how='left', suffix='_person1')\n","        print(\"Person depth 1 test dataframe shape:\", df_person1.shape)\n","        del df_person1\n","        gc.collect()\n","\n","    with timer(\"Person depth 2 test\"):\n","     \n","        df_person2 = get_person2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_person2, on='case_id', how='left', suffix='_person2')\n","        print(\"Person depth 2 test dataframe shape:\", df_person2.shape)\n","        del df_person2\n","        gc.collect()\n","\n","    with timer(\"Other test\"):\n","        \n","        df_other = get_other(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_other, on='case_id', how='left', suffix='_other')\n","        print(\"Other test dataframe shape:\", df_other.shape)\n","        del df_other\n","        gc.collect()\n","\n","    with timer(\"Debit card test\"):\n","      \n","        df_debitcard = get_debitcard(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_debitcard, on='case_id', how='left', suffix='_debitcard')\n","        print(\"Debit card test dataframe shape:\", df_debitcard.shape)\n","        del df_debitcard\n","        gc.collect()\n","\n","    with timer(\"Tax registry a test\"):\n","        \n","        df_tax_registry_a = get_tax_registry_a(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_tax_registry_a, on='case_id', how='left', suffix='_tax_registry_a')\n","        print(\"Tax registry a test dataframe shape:\", df_tax_registry_a.shape)\n","        del df_tax_registry_a\n","        gc.collect()\n","\n","    with timer(\"Tax registry b test\"):\n","       \n","        df_tax_registry_b = get_tax_registry_b(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_tax_registry_b, on='case_id', how='left', suffix='_tax_registry_b')\n","        print(\"Tax registry b test dataframe shape:\", df_tax_registry_b.shape)\n","        del df_tax_registry_b\n","        gc.collect()\n","\n","    with timer(\"Tax registry c test\"):\n","        \n","        df_tax_registry_c = get_tax_registry_c(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_tax_registry_c, on='case_id', how='left', suffix='_tax_registry_c')\n","        print(\"Tax registry c test dataframe shape:\", df_tax_registry_c.shape)\n","        del df_tax_registry_c\n","        gc.collect()\n","    '''\n","    with timer(\"Credit bureau a 1 test\"):\n","        df_credit_bureau_a_1 = get_credit_bureau_a_1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_a_1, on='case_id', how='left', suffix='_cb_a_1')\n","        print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_1.shape)\n","        del df_credit_bureau_a_1\n","        gc.collect()\n","        '''\n","\n","    with timer(\"Credit bureau b 1 test\"):\n","       \n","        df_credit_bureau_b_1 = get_credit_bureau_b_1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_b_1, on='case_id', how='left', suffix='_cb_b_1')\n","        print(\"Credit bureau b 1 test dataframe shape:\", df_credit_bureau_b_1.shape)\n","        del df_credit_bureau_b_1\n","        gc.collect()\n","\n","    '''\n","    with timer(\"Credit bureau a 2 test\"):\n","        df_credit_bureau_a_2 = get_credit_bureau_a_2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_a_2, on='case_id', how='left', suffix='_cb_a_2')\n","        print(\"Credit bureau a 2 test dataframe shape:\", df_credit_bureau_a_2.shape)\n","        # Free memory\n","        del df_credit_bureau_a_2\n","        gc.collect()\n","'''   \n","    with timer(\"Credit bureau b 2 test\"):\n","       \n","        df_credit_bureau_b_2 = get_credit_bureau_b_2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_b_2, on='case_id', how='left', suffix='_cb_b_2')\n","\n","    \n","    with timer(\"Feature engineering / preprocessing\"): \n","     \n","        df=feature_engineering(df)\n","       \n","   \n","    with timer(\"Model training\"):\n","      \n","        df, cat_cols = to_pandas(df)\n","        model = kfold_lightgbm_sklearn(df, cat_cols)\n","       \n","    with timer(\"Feature importance assesment\"):\n","      \n","        get_features_importances(df, model)\n","        if SHOW_REPORT:\n","            make_report(num_rows, df, model)\n","        \n","    with timer(\"Submission\"):\n","      \n","        if generate_submission_file(df, model):\n","            print(\"Submission file has been created.\")\n","        \n","    \n","  \n","    \n","    print(\"NOTEBOOK HAS BEEN SUCCESSFULLY EXECUTED !!!\")\n","    \n","    return df, model"]},{"cell_type":"markdown","metadata":{},"source":["# **UTILITY FUNCTIONS**"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.016673Z","iopub.status.busy":"2024-03-19T17:55:01.016383Z","iopub.status.idle":"2024-03-19T17:55:01.029567Z","shell.execute_reply":"2024-03-19T17:55:01.028671Z","shell.execute_reply.started":"2024-03-19T17:55:01.016648Z"},"trusted":true},"outputs":[],"source":["class Pipeline:\n","    @staticmethod\n","    \n","    \n","    # Sets datatypes accordingly\n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))            \n","\n","        return df\n","    \n","    \n","    # Changes the values of all date columns. The result will not be a date but number of days since date_decision.\n","    @staticmethod\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n","                df = df.with_columns(pl.col(col).dt.total_days())\n","                \n","        df = df.drop(\"date_decision\", \"MONTH\")\n","\n","        return df\n","    \n","    # It drops columns with a lot of NaN values.\n","    @staticmethod\n","    def filter_cols(df):\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","\n","                if isnull > 0.95:\n","                    df = df.drop(col)\n","\n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","\n","        return df"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.031281Z","iopub.status.busy":"2024-03-19T17:55:01.030917Z","iopub.status.idle":"2024-03-19T17:55:01.041181Z","shell.execute_reply":"2024-03-19T17:55:01.040363Z","shell.execute_reply.started":"2024-03-19T17:55:01.031253Z"},"trusted":true},"outputs":[],"source":["def get_info(dataframe):\n","    \"\"\"\n","    View data types, shape, and calculate the percentage of NaN (missing) values in each column\n","    of a Polars DataFrame simultaneously.\n","    \n","    Parameters:\n","    dataframe (polars.DataFrame): The DataFrame to analyze.\n","    \n","    Returns:\n","    None\n","    \"\"\"\n","    # Print DataFrame shape\n","    print(\"DataFrame Shape:\", dataframe.shape)\n","    print(\"-\" * 60)\n","    \n","    # Print column information\n","    print(\"{:<50} {:<30} {:<20}\".format(\"Column Name\", \"Data Type\", \"NaN Percentage\"))\n","    print(\"-\" * 60)\n","    \n","    # Total number of rows in the DataFrame\n","    total_rows = len(dataframe)\n","    \n","    # Iterate over each column\n","    for column in dataframe.columns:\n","        # Get the data type of the column\n","        dtype = str(dataframe[column].dtype)\n","        \n","        # Count the number of NaN values in the column\n","        nan_count = dataframe[column].null_count()\n","        \n","        # Calculate the percentage of NaN values\n","        nan_percentage = (nan_count / total_rows) * 100\n","        \n","        # Print the information\n","        print(\"{:<50} {:<30} {:.2f}%\".format(column, dtype, nan_percentage))\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.042866Z","iopub.status.busy":"2024-03-19T17:55:01.042416Z","iopub.status.idle":"2024-03-19T17:55:01.055320Z","shell.execute_reply":"2024-03-19T17:55:01.054234Z","shell.execute_reply.started":"2024-03-19T17:55:01.042838Z"},"trusted":true},"outputs":[],"source":["def to_pandas(df_data, cat_cols=None):\n","    df_data = df_data.to_pandas()\n","    \n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    \n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    \n","    return df_data, cat_cols"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.057142Z","iopub.status.busy":"2024-03-19T17:55:01.056833Z","iopub.status.idle":"2024-03-19T17:55:01.068136Z","shell.execute_reply":"2024-03-19T17:55:01.066952Z","shell.execute_reply.started":"2024-03-19T17:55:01.057116Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","\n","    return df"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.069781Z","iopub.status.busy":"2024-03-19T17:55:01.069486Z","iopub.status.idle":"2024-03-19T17:55:01.083135Z","shell.execute_reply":"2024-03-19T17:55:01.082174Z","shell.execute_reply.started":"2024-03-19T17:55:01.069751Z"},"trusted":true},"outputs":[],"source":["@contextmanager\n","def timer(name):\n","    t0 = time.time()\n","    yield\n","    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.087105Z","iopub.status.busy":"2024-03-19T17:55:01.086638Z","iopub.status.idle":"2024-03-19T17:55:01.095853Z","shell.execute_reply":"2024-03-19T17:55:01.094881Z","shell.execute_reply.started":"2024-03-19T17:55:01.087082Z"},"trusted":true},"outputs":[],"source":["def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n","    \n","\n","    temp=base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n","        .sort_values(\"WEEK_NUM\") \\\n","        .groupby(\"WEEK_NUM\").mean()\n","   \n","    week_nums_to_drop = temp[(temp[\"target\"] == 0) | (temp[\"target\"] == 1)].index.tolist()\n","\n","    base_filtered = base[~base[\"WEEK_NUM\"].isin(week_nums_to_drop)]\n","\n","    # Apply the aggregator\n","    gini_in_time = base_filtered.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]] \\\n","        .sort_values(\"WEEK_NUM\") \\\n","        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]] \\\n","        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n","\n","    \n","\n","    x = np.arange(len(gini_in_time))\n","    y = gini_in_time\n","    a, b = np.polyfit(x, y, 1)\n","    y_hat = a * x + b\n","    residuals = y - y_hat\n","    res_std = np.std(residuals)\n","    avg_gini = np.nanmean(gini_in_time)  # Use np.nanmean to handle NaN values\n","    \n","    if SHOW_REPORT:\n","        # Display the plot of x on y\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(x, y, 'o', label='Gini in Time')\n","        plt.plot(x, y_hat, '-', label='Fitted line (slope={:.2f}, intercept={:.2f})'.format(a, b))\n","        plt.xlabel('Week')\n","        plt.ylabel('Gini in Time')\n","        plt.title('Gini Stability Over Time')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","    \n","    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std"]},{"cell_type":"markdown","metadata":{},"source":["### Report function"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.097360Z","iopub.status.busy":"2024-03-19T17:55:01.097098Z","iopub.status.idle":"2024-03-19T17:55:01.111440Z","shell.execute_reply":"2024-03-19T17:55:01.110549Z","shell.execute_reply.started":"2024-03-19T17:55:01.097333Z"},"trusted":true},"outputs":[],"source":["def make_report(num_rows, data, model):\n","    # 1. time\n","    current_time = datetime.now()\n","    # Print the current time\n","    print(\"Current Time:\", current_time)\n","    \n","    # 2. specification\n","    if not num_rows:\n","        print(\"The notebook was run in full mode.\")\n","    else:\n","        print(\"The notebook was run in debug mode. Number of rows: \" + str(num_rows))\n","    \n","    # 3. features\n","    feat_importances_df = model.get_features_importances_df(data)\n","    feat_importances_df['gain'] = feat_importances_df['gain'].round(0)\n","    print(feat_importances_df.shape)\n","    feat = data['min_pmtamount_36A'][0:-10]\n","    predictions = pd.Series(model.get_predictions())\n","    correlation = feat.corr(predictions)\n","\n","    numerical_columns = data.select_dtypes(include=['int', 'float']).columns\n","\n","    # Compute correlations of each numerical column with 'PREDICTIONS'\n","    correlations = {}\n","\n","    # Compute correlations of each numerical column with 'feat'\n","    for column in numerical_columns:\n","        correlations[column] = feat.corr(data[column])\n","\n","    # Create a new DataFrame with 'features' and 'correlation' columns\n","    correlation_df = pd.DataFrame(list(correlations.items()), columns=['features', 'correlation'])\n","\n","    # Round the correlation numbers to three decimal places\n","    correlation_df['correlation'] = correlation_df['correlation'].round(3)\n","\n","    # Merge feat_importances_df and correlation_df on 'feature'\n","    combined_df = pd.merge(feat_importances_df, correlation_df, left_on=\"feature\", right_on='features', how='left')\n","\n","    # Handle categorical features with no correlation\n","    combined_df['correlation'] = combined_df['correlation'].fillna(value=np.nan)\n","    \n","\n","    # Compute and add valid percentage for each feature\n","    valid_percentage = (data[0:-10].count() / len(data[0:-10]))\n","    valid_percentage = valid_percentage.round(3)\n","    combined_df['valid_percentage'] = combined_df['feature'].map(valid_percentage)\n","\n","    # Print the combined_df DataFrame\n","    print(combined_df.to_string(index=False))\n","    print()\n","    roc_score=roc_auc_score(data['target'][0:-10],predictions)\n","    print(\"ROC score: \",roc_score)\n","\n","    # Compute false positive rate, true positive rate, and thresholds for ROC curve\n","    fpr, tpr, thresholds = roc_curve(data['target'][0:-10], predictions)\n","\n","    # Plot ROC curve\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_score)\n","    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.grid(True)\n","    plt.show()\n","    "]},{"cell_type":"markdown","metadata":{},"source":["#  **MODEL**"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.113008Z","iopub.status.busy":"2024-03-19T17:55:01.112748Z","iopub.status.idle":"2024-03-19T17:55:01.208551Z","shell.execute_reply":"2024-03-19T17:55:01.207472Z","shell.execute_reply.started":"2024-03-19T17:55:01.112987Z"},"trusted":true},"outputs":[],"source":["class VotingModel(BaseEstimator, RegressorMixin):\n","    def __init__(self, estimators):\n","        super().__init__()\n","        self.estimators = estimators\n","        \n","        \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def predict(self, X):\n","        y_preds = [estimator.predict(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","    \n","    def predict_proba(self, X):\n","        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n","        # Use tqdm to create a progress bar during the prediction\n","        with tqdm(total=len(self.estimators), desc=\"Predicting\", unit=\" models\") as pbar:\n","            for i, estimator in enumerate(self.estimators):\n","                y_preds[i] = estimator.predict_proba(X)\n","                pbar.update(1)  # Update the progress bar\n","        return np.mean(y_preds, axis=0)\n","\n","    \n","    def get_splits(self, aggregation_method=np.mean):\n","        \n","        feature_importances_list=[]\n","        for x in self.estimators:\n","            feature_importances_list.append(x.booster_.feature_importance(importance_type='split'))\n","            \n","        # Aggregate feature importances across all models\n","        if all(importances is not None for importances in feature_importances_list):\n","            combined_importances = aggregation_method(feature_importances_list, axis=0)\n","        else:\n","            combined_importances = None   \n","        return combined_importances\n","    \n","    \n","    def get_gains(self, aggregation_method=np.mean):\n","        \n","        feature_importances_list=[]\n","        for model in self.estimators:\n","            feature_importances_list.append(x.booster_.feature_importance(importance_type='gain'))\n","            \n","        # Aggregate feature importances across all models\n","        if all(importances is not None for importances in feature_importances_list):\n","            combined_importances = aggregation_method(feature_importances_list, axis=0)\n","        else:\n","            combined_importances = None\n","              \n","        return combined_importances\n","    \n","    def get_features_importances_df(self, df):\n","        del_features = ['target', 'case_id']\n","        predictors = list(filter(lambda v: v not in del_features, df.columns))\n","        importance_df = pd.DataFrame()\n","        eval_results = dict()\n","        for model in self.estimators:\n","            fold_importance = pd.DataFrame()\n","            fold_importance[\"feature\"] = predictors\n","            fold_importance[\"gain\"] = model.booster_.feature_importance(importance_type='gain')\n","            fold_importance[\"split\"] = model.booster_.feature_importance(importance_type='split')\n","            importance_df = pd.concat([importance_df, fold_importance], axis=0)\n","            importance_df= importance_df.groupby('feature').mean().reset_index()\n","        return importance_df\n","    \n","    \n","    def add_predictions(self, predictions):\n","        self.predictions=predictions\n","        \n","    def get_predictions(self):\n","        return self.predictions\n","        "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.210380Z","iopub.status.busy":"2024-03-19T17:55:01.210111Z","iopub.status.idle":"2024-03-19T17:55:01.228355Z","shell.execute_reply":"2024-03-19T17:55:01.227078Z","shell.execute_reply.started":"2024-03-19T17:55:01.210359Z"},"trusted":true},"outputs":[],"source":["def kfold_lightgbm_sklearn(data, categorical_feature = None):\n","    start_time = time.time()\n","    df = data[data['target'].notnull()]\n","    test = data[data['target'].isnull()]\n","    print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n","    del_features = ['target', 'case_id']\n","    predictors = list(filter(lambda v: v not in del_features, df.columns))\n","\n","    if not STRATIFIED_KFOLD:\n","        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n","    else:\n","        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n","    \n","        # Hold oof predictions, test predictions, feature importance and training/valid auc\n","    oof_preds = np.zeros(df.shape[0])\n","    \n","    importance_df = pd.DataFrame()\n","    eval_results = dict()\n","    \n","    fitted_models = []\n","    with tqdm(total=NUM_FOLDS) as pbar:\n","        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):\n","            train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]\n","            valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]\n","\n","            params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n","            clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n","\n","\n","            if not categorical_feature:\n","                    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n","                            eval_metric='auc' )\n","            else:\n","                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n","                        feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n","\n","\n","            fitted_models.append(clf)\n","\n","            if EVALUATE_VALIDATION_SET:\n","                oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n","\n","\n","\n","                # Feature importance by GAIN and SPLIT\n","\n","            eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n","            eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n","\n","            elapsed_time = time.time() - start_time\n","            remaining_time = elapsed_time * (NUM_FOLDS - n_fold - 1) / (n_fold + 1)\n","            print('Fold %2d AUC : %.6f. Elapsed time: %.2f seconds. Remaining time: %.2f seconds.'\n","                  % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx]), elapsed_time, remaining_time))\n","            del clf, train_x, train_y, valid_x, valid_y\n","            gc.collect()\n","            pbar.update(1)\n","            \n","    print('Full AUC score %.6f' % roc_auc_score(df['target'], oof_preds))\n","    # Get the average feature importance between folds\n","    \n","    \n","    \n","    if len(df)>0:\n","        base=get_base(DATA_DIRECTORY, len(df))\n","        base, cat_cols = to_pandas(base)\n","        base=base[base['target'].notnull()]\n","        base['score']= oof_preds\n","        gini_score = gini_stability(base)\n","        print(\"Gini Score of the valid set:\", gini_score)\n","    \n","    \n","    \n","    \n","    # Save feature importance, test predictions and oof predictions as csv\n","    if GENERATE_SUBMISSION_FILES:\n","\n","        # Generate oof csv\n","        oof = pd.DataFrame()\n","        oof['case_id'] = df['case_id'].copy()\n","        df['PREDICTIONS'] = oof_preds.copy()\n","        df['target'] = df['target'].copy()\n","        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","        \n","  \n","        \n","        \n","    model = VotingModel(fitted_models)\n","    model.add_predictions(oof_preds.copy())\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# **SUBMISSION**"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.230130Z","iopub.status.busy":"2024-03-19T17:55:01.229752Z","iopub.status.idle":"2024-03-19T17:55:01.241038Z","shell.execute_reply":"2024-03-19T17:55:01.240251Z","shell.execute_reply.started":"2024-03-19T17:55:01.230106Z"},"trusted":true},"outputs":[],"source":["def generate_submission_file(data, model):\n","   \n","    test = data[data['target'].isnull()]\n","    \n","    del_features = ['target', 'case_id']\n","    predictors = list(filter(lambda v: v not in del_features, data.columns))\n","    y_pred = pd.Series(model.predict_proba(test[predictors])[:, 1], index=test['case_id']) \n","   \n","    df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","    df_subm = df_subm.set_index(\"case_id\")\n","    df_subm[\"score\"] = y_pred\n","  \n","    df_subm.to_csv(\"submission.csv\")\n","    \n","    return True\n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["# **EVALUATE FEATURES IMPORTANCES**"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.242389Z","iopub.status.busy":"2024-03-19T17:55:01.242114Z","iopub.status.idle":"2024-03-19T17:55:01.251661Z","shell.execute_reply":"2024-03-19T17:55:01.250881Z","shell.execute_reply.started":"2024-03-19T17:55:01.242367Z"},"trusted":true},"outputs":[],"source":["def get_features_importances(data, model):\n","    importance_df = model.get_features_importances_df(data)\n","    mean_importance = importance_df.groupby('feature').mean().reset_index()\n","    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n","    mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","    return True"]},{"cell_type":"markdown","metadata":{},"source":["# **FEATURE ENGINEERING FUNCTION**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.252603Z","iopub.status.busy":"2024-03-19T17:55:01.252378Z","iopub.status.idle":"2024-03-19T17:55:01.263319Z","shell.execute_reply":"2024-03-19T17:55:01.262522Z","shell.execute_reply.started":"2024-03-19T17:55:01.252583Z"},"trusted":true},"outputs":[],"source":["def feature_engineering(df):\n","    df = df.pipe(Pipeline.handle_dates) \n","    df=df.pipe(Pipeline.filter_cols)\n","\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# **GET FUNCTIONS**"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.264117Z","iopub.status.busy":"2024-03-19T17:55:01.263926Z","iopub.status.idle":"2024-03-19T17:55:01.272107Z","shell.execute_reply":"2024-03-19T17:55:01.271320Z","shell.execute_reply.started":"2024-03-19T17:55:01.264099Z"},"trusted":true},"outputs":[],"source":["def group(df_to_agg, prefix, aggregations, aggregate_by='case_id'):\n","    # Create a dictionary mapping aggregation functions to their string representations\n","    func_mapping = {\n","    'min': pl.min,\n","    'max': pl.max,\n","    'mean': pl.mean,\n","    'sum': pl.sum,\n","    'count': pl.count\n","    }\n","    \n","# Perform the aggregation\n","    agg_df = df_to_agg.group_by(aggregate_by).agg(**{\n","        f\"{func}_{col}\": func_mapping[func](col) for col, funcs in aggregations.items() for func in funcs\n","    })\n","    '''\n","    # Rename columns\n","    for col, funcs in aggregations.items():\n","        for func in funcs:\n","            old_name = f\"{col}_{func}\"\n","            new_name = f\"{prefix}{col}_{func.upper()}\"\n","            agg_df = agg_df.select(pl.col(old_name).alias(new_name))\n","    '''\n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_base()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.273302Z","iopub.status.busy":"2024-03-19T17:55:01.272894Z","iopub.status.idle":"2024-03-19T17:55:01.280725Z","shell.execute_reply":"2024-03-19T17:55:01.279924Z","shell.execute_reply.started":"2024-03-19T17:55:01.273283Z"},"trusted":true},"outputs":[],"source":["def get_base(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    train={}\n","    test={}\n","    \n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet'))\n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet')).limit(num_rows) \n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_base.parquet'))    \n","    length=len(test)\n","    nan_series=pl.Series([None] * length)\n","    test = test.select(pl.col(\"*\"), nan_series.alias(\"target\"))\n","    df=pl.concat([train, test])\n","    df = df.with_columns(pl.col('date_decision').cast(pl.Date))\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### get_static()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.284057Z","iopub.status.busy":"2024-03-19T17:55:01.283845Z","iopub.status.idle":"2024-03-19T17:55:01.290624Z","shell.execute_reply":"2024-03-19T17:55:01.289668Z","shell.execute_reply.started":"2024-03-19T17:55:01.284039Z"},"trusted":true},"outputs":[],"source":["def get_static(path, num_rows = None):\n","# Read the Parquet file using scan() method\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_static_0_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    train = (pl.concat(chunks, how=\"vertical_relaxed\")).pipe(Pipeline.filter_cols)\n","    \n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(train))\n","        \n","        train=df1\n","        del df2\n","    \n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_static_0_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    \n","    \n","    columns_to_keep = train.columns\n","\n","# Find columns in 'test' that are not in 'train'\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","\n","# Drop columns from 'test' that are not in 'train'\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### get_static_cb()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.291918Z","iopub.status.busy":"2024-03-19T17:55:01.291690Z","iopub.status.idle":"2024-03-19T17:55:01.303067Z","shell.execute_reply":"2024-03-19T17:55:01.302338Z","shell.execute_reply.started":"2024-03-19T17:55:01.291899Z"},"trusted":true},"outputs":[],"source":["def get_static_cb(path, num_rows = None):\n","    \n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","       \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### get_applprev1(DATA_DIRECTORY, num_rows=num_rows)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.304776Z","iopub.status.busy":"2024-03-19T17:55:01.304050Z","iopub.status.idle":"2024-03-19T17:55:01.312679Z","shell.execute_reply":"2024-03-19T17:55:01.311887Z","shell.execute_reply.started":"2024-03-19T17:55:01.304746Z"},"trusted":true},"outputs":[],"source":["def get_applprev1(path, num_rows = None):\n","    \n","    \n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_applprev_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n","    \n","    \n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(train))\n","\n","        train=df1\n","        del df2   \n","    \n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_applprev_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', APPLPREV1_AGG)\n","    del df \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_applprev2(DATA_DIRECTORY, num_rows=num_rows)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.314440Z","iopub.status.busy":"2024-03-19T17:55:01.314115Z","iopub.status.idle":"2024-03-19T17:55:01.325533Z","shell.execute_reply":"2024-03-19T17:55:01.324646Z","shell.execute_reply.started":"2024-03-19T17:55:01.314405Z"},"trusted":true},"outputs":[],"source":["def get_applprev2(path, num_rows = None):\n","    train={}\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","     \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","       \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', APPLPREV2_AGG)\n","    del df \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_person1"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.327479Z","iopub.status.busy":"2024-03-19T17:55:01.326579Z","iopub.status.idle":"2024-03-19T17:55:01.334285Z","shell.execute_reply":"2024-03-19T17:55:01.333601Z","shell.execute_reply.started":"2024-03-19T17:55:01.327452Z"},"trusted":true},"outputs":[],"source":["def get_person1(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","      \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_person_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', PERSON1_AGG)\n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_person2"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.335748Z","iopub.status.busy":"2024-03-19T17:55:01.335152Z","iopub.status.idle":"2024-03-19T17:55:01.346912Z","shell.execute_reply":"2024-03-19T17:55:01.346112Z","shell.execute_reply.started":"2024-03-19T17:55:01.335727Z"},"trusted":true},"outputs":[],"source":["def get_person2(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_person_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', PERSON2_AGG)\n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### other"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.348456Z","iopub.status.busy":"2024-03-19T17:55:01.347924Z","iopub.status.idle":"2024-03-19T17:55:01.355986Z","shell.execute_reply":"2024-03-19T17:55:01.355333Z","shell.execute_reply.started":"2024-03-19T17:55:01.348412Z"},"trusted":true},"outputs":[],"source":["def get_other(path, num_rows = None):\n","     # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","         \n","    test = pl.read_parquet(os.path.join(path, 'test/test_other_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', OTHER_AGG)\n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["## get_debitcard"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.357872Z","iopub.status.busy":"2024-03-19T17:55:01.356946Z","iopub.status.idle":"2024-03-19T17:55:01.369243Z","shell.execute_reply":"2024-03-19T17:55:01.368693Z","shell.execute_reply.started":"2024-03-19T17:55:01.357842Z"},"trusted":true},"outputs":[],"source":["def get_debitcard(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","     \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","      \n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', DEBITCARD_AGG)\n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_a"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.370742Z","iopub.status.busy":"2024-03-19T17:55:01.370067Z","iopub.status.idle":"2024-03-19T17:55:01.380062Z","shell.execute_reply":"2024-03-19T17:55:01.379183Z","shell.execute_reply.started":"2024-03-19T17:55:01.370715Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_a(path, num_rows = None):\n","    \n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","  \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', TAX_REGISTRY_A_AGG)    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_b"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.384077Z","iopub.status.busy":"2024-03-19T17:55:01.383439Z","iopub.status.idle":"2024-03-19T17:55:01.392875Z","shell.execute_reply":"2024-03-19T17:55:01.392337Z","shell.execute_reply.started":"2024-03-19T17:55:01.384057Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_b(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', TAX_REGISTRY_B_AGG) \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_c"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.393896Z","iopub.status.busy":"2024-03-19T17:55:01.393545Z","iopub.status.idle":"2024-03-19T17:55:01.402074Z","shell.execute_reply":"2024-03-19T17:55:01.401507Z","shell.execute_reply.started":"2024-03-19T17:55:01.393877Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_c(path, num_rows = None):\n","     # Read the Parquet file using scan() method\n","# Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', TAX_REGISTRY_C_AGG)    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_a_1"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.403191Z","iopub.status.busy":"2024-03-19T17:55:01.402870Z","iopub.status.idle":"2024-03-19T17:55:01.414734Z","shell.execute_reply":"2024-03-19T17:55:01.414034Z","shell.execute_reply.started":"2024-03-19T17:55:01.403172Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_a_1(path, num_rows = None):\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_credit_bureau_a_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(train))\n","        \n","        train=df1\n","        del df2\n","    \n","    \n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_credit_bureau_a_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', CREDIT_BUREAU_A_1_AGG) \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_b_1"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.415878Z","iopub.status.busy":"2024-03-19T17:55:01.415538Z","iopub.status.idle":"2024-03-19T17:55:01.424633Z","shell.execute_reply":"2024-03-19T17:55:01.423683Z","shell.execute_reply.started":"2024-03-19T17:55:01.415858Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_b_1(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","   \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', CREDIT_BUREAU_B_1_AGG) \n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_a_2"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.425628Z","iopub.status.busy":"2024-03-19T17:55:01.425422Z","iopub.status.idle":"2024-03-19T17:55:01.439179Z","shell.execute_reply":"2024-03-19T17:55:01.438288Z","shell.execute_reply.started":"2024-03-19T17:55:01.425609Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_a_2(path, num_rows = None):\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_credit_bureau_a_2_*.parquet')):\n","        chunks.append(reduce_mem_usage(pl.read_parquet(path))) #.pipe(Pipeline.set_table_dtypes))\n","      \n","    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n","    \n","    '''\n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(df))\n","        \n","        train=df1\n","        del df2\n","    \n","    '''\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_credit_bureau_a_2_*.parquet')):\n","        chunks.append(reduce_mem_usage(pl.read_parquet(path))) #.pipe(Pipeline.set_table_dtypes))\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","  \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    return df\n","                      \n","                      \n","   "]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_b_2"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.440768Z","iopub.status.busy":"2024-03-19T17:55:01.440485Z","iopub.status.idle":"2024-03-19T17:55:01.448698Z","shell.execute_reply":"2024-03-19T17:55:01.447876Z","shell.execute_reply.started":"2024-03-19T17:55:01.440742Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_b_2(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","   \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","\n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    \n","    df=pl.concat([train, test])\n","    agg_df = group(df, '', CREDIT_BUREAU_B_2_AGG) \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["# **EXECUTION**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T17:55:01.450054Z","iopub.status.busy":"2024-03-19T17:55:01.449440Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["base dataframe shape: (11121, 5)\n","base - done in 0s\n","static dataframe shape: (11141, 156)\n","static - done in 6s\n","static cb dataframe shape: (11121, 23)\n","static_cb - done in 1s\n","Previous applications depth 1 test dataframe shape: (4109, 55)\n","Previous applications depth 1 test - done in 8s\n","Previous applications depth 2 test dataframe shape: (2759, 1)\n","Previous applications depth 2 test - done in 1s\n","Person depth 1 test dataframe shape: (3127, 24)\n","Person depth 1 test - done in 1s\n","Person depth 2 test dataframe shape: (9569, 1)\n","Person depth 2 test - done in 0s\n","Other test dataframe shape: (11120, 1)\n","Other test - done in 0s\n","Debit card test dataframe shape: (7481, 1)\n","Debit card test - done in 0s\n","Tax registry a test dataframe shape: (1688, 4)\n","Tax registry a test - done in 0s\n","Tax registry b test dataframe shape: (1541, 4)\n","Tax registry b test - done in 0s\n","Tax registry c test dataframe shape: (1825, 7)\n","Tax registry c test - done in 1s\n","Credit bureau b 1 test dataframe shape: (4515, 1)\n","Credit bureau b 1 test - done in 0s\n","Credit bureau b 2 test - done in 0s\n","Feature engineering / preprocessing - done in 0s\n","Train/valid shape: (11111, 136), test shape: (10, 136)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04dc16b15c0b469d8d2cb5d49035879d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","if __name__ == \"__main__\":\n","    pd.set_option('display.max_rows', 60)\n","    pd.set_option('display.max_columns', 100)\n","    with timer(\"Pipeline total time\"):\n","        main(debug= True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
