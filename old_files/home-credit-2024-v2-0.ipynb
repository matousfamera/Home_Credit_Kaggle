{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:20:58.936386Z","iopub.status.busy":"2024-03-16T23:20:58.936041Z","iopub.status.idle":"2024-03-16T23:21:05.161876Z","shell.execute_reply":"2024-03-16T23:21:05.160974Z","shell.execute_reply.started":"2024-03-16T23:20:58.936352Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import time\n","import numpy as np\n","import pandas as pd\n","from contextlib import contextmanager\n","import multiprocessing as mp\n","from functools import partial\n","from scipy.stats import kurtosis, iqr, skew\n","from lightgbm import LGBMClassifier\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","from glob import glob\n","from pathlib import Path\n","from datetime import datetime\n","import polars as pl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.metrics import roc_auc_score \n","from sklearn.metrics import roc_curve, auc\n","from tqdm.notebook import tqdm\n","import joblib\n","import lightgbm as lgb\n","import warnings\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.164125Z","iopub.status.busy":"2024-03-16T23:21:05.163586Z","iopub.status.idle":"2024-03-16T23:21:05.173111Z","shell.execute_reply":"2024-03-16T23:21:05.171822Z","shell.execute_reply.started":"2024-03-16T23:21:05.164094Z"},"trusted":true},"outputs":[],"source":["# GENERAL CONFIGURATIONS\n","NUM_THREADS = 4\n","# DATA_DIRECTORY = \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/\"\n","DATA_DIRECTORY = \"./parquet_files/\"\n","\n","SUBMISSION_SUFIX = \"_model2_0\"\n","# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\n","GENERATE_SUBMISSION_FILES = True\n","STRATIFIED_KFOLD = False\n","RANDOM_SEED = 737851\n","NUM_FOLDS = 10\n","EARLY_STOPPING = 100\n","ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n","\n","LIGHTGBM_PARAMS = {\n","    'boosting_type': 'goss',\n","    'n_estimators': 10000,\n","    'learning_rate': 0.005134,\n","    'num_leaves': 54,\n","    'max_depth': 10,\n","    'subsample_for_bin': 240000,\n","    'reg_alpha': 0.436193,\n","    'reg_lambda': 0.479169,\n","    'colsample_bytree': 0.508716,\n","    'min_split_gain': 0.024766,\n","    'subsample': 1,\n","    'is_unbalance': False,\n","    'silent':-1,\n","    'verbose':-1,\n","    \n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Main function"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.175792Z","iopub.status.busy":"2024-03-16T23:21:05.175139Z","iopub.status.idle":"2024-03-16T23:21:05.205918Z","shell.execute_reply":"2024-03-16T23:21:05.204744Z","shell.execute_reply.started":"2024-03-16T23:21:05.175737Z"},"trusted":true},"outputs":[],"source":["def main(debug= False):\n","    num_rows = 30000 if debug else None\n","    with timer(\"Test base\"):\n","        df = get_base(DATA_DIRECTORY, num_rows=num_rows)\n","        print(\"Test base dataframe shape:\", df.shape)\n","\n","    with timer(\"Test static\"):\n","        df_static = get_static(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_static, on='case_id', how='left', suffix='_static')\n","        print(\"Test static dataframe shape:\", df_static.shape)\n","        del df_static\n","        gc.collect()\n","\n","    with timer(\"Test static_cb\"):\n","        df_static_cb = get_static_cb(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_static_cb, on='case_id', how='left', suffix='_static_cb')\n","        print(\"Test static cb dataframe shape:\", df_static_cb.shape)\n","        del df_static_cb\n","        gc.collect()\n","\n","    with timer(\"Previous applications depth 1 test\"):\n","        df_applprev1 = get_applprev1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_applprev1, on='case_id', how='left', suffix='_applprev1')\n","        print(\"Previous applications depth 1 test dataframe shape:\", df_applprev1.shape)\n","        del df_applprev1\n","        gc.collect()\n","\n","    with timer(\"Previous applications depth 2 test\"):\n","        df_applprev2 = get_applprev2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_applprev2, on='case_id', how='left', suffix='_applprev2')\n","        print(\"Previous applications depth 2 test dataframe shape:\", df_applprev2.shape)\n","        del df_applprev2\n","        gc.collect()\n","\n","    with timer(\"Person depth 1 test\"):\n","        df_person1 = get_person1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_person1, on='case_id', how='left', suffix='_person1')\n","        print(\"Person depth 1 test dataframe shape:\", df_person1.shape)\n","        del df_person1\n","        gc.collect()\n","\n","    with timer(\"Person depth 2 test\"):\n","        df_person2 = get_person2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_person2, on='case_id', how='left', suffix='_person2')\n","        print(\"Person depth 2 test dataframe shape:\", df_person2.shape)\n","        del df_person2\n","        gc.collect()\n","\n","    with timer(\"Other test\"):\n","        df_other = get_other(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_other, on='case_id', how='left', suffix='_other')\n","        print(\"Other test dataframe shape:\", df_other.shape)\n","        del df_other\n","        gc.collect()\n","\n","    with timer(\"Debit card test\"):\n","        df_debitcard = get_debitcard(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_debitcard, on='case_id', how='left', suffix='_debitcard')\n","        print(\"Debit card test dataframe shape:\", df_debitcard.shape)\n","        del df_debitcard\n","        gc.collect()\n","\n","    with timer(\"Tax registry a test\"):\n","        df_tax_registry_a = get_tax_registry_a(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_tax_registry_a, on='case_id', how='left', suffix='_tax_registry_a')\n","        print(\"Tax registry a test dataframe shape:\", df_tax_registry_a.shape)\n","        del df_tax_registry_a\n","        gc.collect()\n","\n","    with timer(\"Tax registry b test\"):\n","        df_tax_registry_b = get_tax_registry_b(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_tax_registry_b, on='case_id', how='left', suffix='_tax_registry_b')\n","        print(\"Tax registry b test dataframe shape:\", df_tax_registry_b.shape)\n","        del df_tax_registry_b\n","        gc.collect()\n","\n","    with timer(\"Tax registry c test\"):\n","        df_tax_registry_c = get_tax_registry_c(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_tax_registry_c, on='case_id', how='left', suffix='_tax_registry_c')\n","        print(\"Tax registry c test dataframe shape:\", df_tax_registry_c.shape)\n","        del df_tax_registry_c\n","        gc.collect()\n","    '''\n","    with timer(\"Credit bureau a 1 test\"):\n","        df_credit_bureau_a_1 = get_credit_bureau_a_1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_a_1, on='case_id', how='left', suffix='_cb_a_1')\n","        print(\"Credit bureau a 1 test dataframe shape:\", df_credit_bureau_a_1.shape)\n","        del df_credit_bureau_a_1\n","        gc.collect()\n","        '''\n","\n","    with timer(\"Credit bureau b 1 test\"):\n","        df_credit_bureau_b_1 = get_credit_bureau_b_1(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_b_1, on='case_id', how='left', suffix='_cb_b_1')\n","        print(\"Credit bureau b 1 test dataframe shape:\", df_credit_bureau_b_1.shape)\n","        del df_credit_bureau_b_1\n","        gc.collect()\n","\n","    '''\n","    with timer(\"Credit bureau a 2 test\"):\n","        df_credit_bureau_a_2 = get_credit_bureau_a_2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_a_2, on='case_id', how='left', suffix='_cb_a_2')\n","        print(\"Credit bureau a 2 test dataframe shape:\", df_credit_bureau_a_2.shape)\n","\n","        # Free memory\n","        del df_credit_bureau_a_2\n","        gc.collect()\n","'''   \n","    with timer(\"Credit bureau b 2 test\"):\n","        df_credit_bureau_b_2 = get_credit_bureau_b_2(DATA_DIRECTORY, num_rows=num_rows)\n","        df = df.join(df_credit_bureau_b_2, on='case_id', how='left', suffix='_cb_b_2')\n","\n","    df = df.pipe(Pipeline.handle_dates) \n","        \n","\n","    #with timer(\"more preprocessing\"):\n","        #df = add_ratios_features(df)\n","        #df = add_ratios_features(df)\n","    print(df.shape)\n","    get_info(df)\n","    with timer(\"Run LightGBM\"):\n","        df, cat_cols = to_pandas(df)\n","        feat_importance = kfold_lightgbm_sklearn(df, cat_cols)\n","        print(feat_importance)\n","\n","   "]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.209622Z","iopub.status.busy":"2024-03-16T23:21:05.209069Z","iopub.status.idle":"2024-03-16T23:21:05.223753Z","shell.execute_reply":"2024-03-16T23:21:05.222815Z","shell.execute_reply.started":"2024-03-16T23:21:05.209565Z"},"trusted":true},"outputs":[],"source":["class Pipeline:\n","    @staticmethod\n","    \n","    \n","    # Sets datatypes accordingly\n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))            \n","\n","        return df\n","    \n","    \n","    # Changes the values of all date columns. The result will not be a date but number of days since date_decision.\n","    @staticmethod\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n","                df = df.with_columns(pl.col(col).dt.total_days())\n","                \n","        df = df.drop(\"date_decision\", \"MONTH\")\n","\n","        return df\n","    \n","    # It drops columns with a lot of NaN values.\n","    @staticmethod\n","    def filter_cols(df):\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","\n","                if isnull > 0.95:\n","                    df = df.drop(col)\n","\n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","\n","        return df"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.225260Z","iopub.status.busy":"2024-03-16T23:21:05.224926Z","iopub.status.idle":"2024-03-16T23:21:05.241659Z","shell.execute_reply":"2024-03-16T23:21:05.240614Z","shell.execute_reply.started":"2024-03-16T23:21:05.225236Z"},"trusted":true},"outputs":[],"source":["def to_pandas(df_data, cat_cols=None):\n","    df_data = df_data.to_pandas()\n","    \n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    \n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    \n","    return df_data, cat_cols"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.243960Z","iopub.status.busy":"2024-03-16T23:21:05.243176Z","iopub.status.idle":"2024-03-16T23:21:05.258859Z","shell.execute_reply":"2024-03-16T23:21:05.258011Z","shell.execute_reply.started":"2024-03-16T23:21:05.243934Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.260540Z","iopub.status.busy":"2024-03-16T23:21:05.260216Z","iopub.status.idle":"2024-03-16T23:21:05.279715Z","shell.execute_reply":"2024-03-16T23:21:05.278734Z","shell.execute_reply.started":"2024-03-16T23:21:05.260516Z"},"trusted":true},"outputs":[],"source":["def kfold_lightgbm_sklearn(data, categorical_feature = None):\n","    \n","    df = data[data['target'].notnull()]\n","    test = data[data['target'].isnull()]\n","    print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n","    del_features = ['target', 'case_id']\n","    predictors = list(filter(lambda v: v not in del_features, df.columns))\n","\n","    if not STRATIFIED_KFOLD:\n","        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n","    else:\n","        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n","    \n","        # Hold oof predictions, test predictions, feature importance and training/valid auc\n","    oof_preds = np.zeros(df.shape[0])\n","    sub_preds = np.zeros(test.shape[0])\n","    importance_df = pd.DataFrame()\n","    eval_results = dict()\n","    \n","    \n","    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):\n","        train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]\n","        valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]\n","        \n","        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n","        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n","\n","\n","        if not categorical_feature:\n","                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n","                        eval_metric='auc' )\n","        else:\n","            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric='auc',\n","                    feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n","\n","\n","        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n","        sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n","\n","            # Feature importance by GAIN and SPLIT\n","        fold_importance = pd.DataFrame()\n","        fold_importance[\"feature\"] = predictors\n","        fold_importance[\"gain\"] = clf.booster_.feature_importance(importance_type='gain')\n","        fold_importance[\"split\"] = clf.booster_.feature_importance(importance_type='split')\n","        importance_df = pd.concat([importance_df, fold_importance], axis=0)\n","        eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n","        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n","\n","        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n","        del clf, train_x, train_y, valid_x, valid_y\n","        gc.collect()\n","\n","    print('Full AUC score %.6f' % roc_auc_score(df['target'], oof_preds))\n","    test['target'] = sub_preds.copy()\n","    # Get the average feature importance between folds\n","    mean_importance = importance_df.groupby('feature').mean().reset_index()\n","    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n","    # Save feature importance, test predictions and oof predictions as csv\n","    if GENERATE_SUBMISSION_FILES:\n","\n","        # Generate oof csv\n","        oof = pd.DataFrame()\n","        oof['case_id'] = df['case_id'].copy()\n","        df['PREDICTIONS'] = oof_preds.copy()\n","        df['target'] = df['target'].copy()\n","        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","        # Save submission (test data) and feature importance\n","        df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","        df_subm = df_subm.set_index(\"case_id\")\n","        df_subm[\"score\"] = sub_pred\n","        df_subm.to_csv(\"submission.csv\")\n","        mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n","    return mean_importance"]},{"cell_type":"markdown","metadata":{},"source":["### Df info function"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.281113Z","iopub.status.busy":"2024-03-16T23:21:05.280842Z","iopub.status.idle":"2024-03-16T23:21:05.294734Z","shell.execute_reply":"2024-03-16T23:21:05.293751Z","shell.execute_reply.started":"2024-03-16T23:21:05.281090Z"},"trusted":true},"outputs":[],"source":["def get_info(dataframe):\n","    \"\"\"\n","    View data types, shape, and calculate the percentage of NaN (missing) values in each column\n","    of a Polars DataFrame simultaneously.\n","    \n","    Parameters:\n","    dataframe (polars.DataFrame): The DataFrame to analyze.\n","    \n","    Returns:\n","    None\n","    \"\"\"\n","    # Print DataFrame shape\n","    print(\"DataFrame Shape:\", dataframe.shape)\n","    print(\"-\" * 60)\n","    \n","    # Print column information\n","    print(\"{:<50} {:<30} {:<20}\".format(\"Column Name\", \"Data Type\", \"NaN Percentage\"))\n","    print(\"-\" * 60)\n","    \n","    # Total number of rows in the DataFrame\n","    total_rows = len(dataframe)\n","    \n","    # Iterate over each column\n","    for column in dataframe.columns:\n","        # Get the data type of the column\n","        dtype = str(dataframe[column].dtype)\n","        \n","        # Count the number of NaN values in the column\n","        nan_count = dataframe[column].null_count()\n","        \n","        # Calculate the percentage of NaN values\n","        nan_percentage = (nan_count / total_rows) * 100\n","        \n","        # Print the information\n","        print(\"{:<50} {:<30} {:.2f}%\".format(column, dtype, nan_percentage))\n"]},{"cell_type":"markdown","metadata":{},"source":["### get_base()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.296166Z","iopub.status.busy":"2024-03-16T23:21:05.295896Z","iopub.status.idle":"2024-03-16T23:21:05.309385Z","shell.execute_reply":"2024-03-16T23:21:05.308420Z","shell.execute_reply.started":"2024-03-16T23:21:05.296144Z"},"trusted":true},"outputs":[],"source":["def get_base(path, num_rows=None):\n","    \"\"\"\n","    Function to read base data.\n","\n","    Parameters:\n","        path (str): Path to the directory containing the data files.\n","        num_rows (int, optional): Number of rows to read from the training data. Default is None,\n","            meaning the entire training dataset is read.\n","\n","    Returns:\n","        DataFrame: Concatenated DataFrame containing base data from both train and test datasets.\n","    \"\"\"\n","    # Initialize empty dictionaries to store train and test data\n","    train = {}\n","    test = {}\n","    \n","    # If num_rows is not specified, read the entire training data\n","    if num_rows is None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet'))\n","    # If num_rows is specified, read only the specified number of rows from the training data\n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_base.parquet')).limit(num_rows) \n","        \n","    # Read the test data\n","    test = pl.read_parquet(os.path.join(path, 'test/test_base.parquet'))    \n","    \n","    # Create a series filled with None values to serve as the 'target' column for the test data\n","    length = len(test)\n","    nan_series = pl.Series([None] * length)\n","    \n","    # Add the 'target' column to the test data\n","    test = test.select(pl.col(\"*\"), nan_series.alias(\"target\"))\n","    \n","    # Concatenate train and test data\n","    df = pl.concat([train, test])\n","    \n","    # Convert 'date_decision' column to Date type\n","    df = df.with_column(pl.col('date_decision').cast(pl.Date))\n","    \n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:05.313125Z","iopub.status.busy":"2024-03-16T23:21:05.312865Z","iopub.status.idle":"2024-03-16T23:21:06.039013Z","shell.execute_reply":"2024-03-16T23:21:06.038027Z","shell.execute_reply.started":"2024-03-16T23:21:05.313103Z"},"trusted":true},"outputs":[],"source":["A=get_base(DATA_DIRECTORY)\n","get_info(A)\n","del A"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### get_static()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:06.040835Z","iopub.status.busy":"2024-03-16T23:21:06.040430Z","iopub.status.idle":"2024-03-16T23:21:06.050019Z","shell.execute_reply":"2024-03-16T23:21:06.049166Z","shell.execute_reply.started":"2024-03-16T23:21:06.040801Z"},"trusted":true},"outputs":[],"source":["def get_static(path, num_rows=None):\n","    \"\"\"\n","    Function to read static data.\n","\n","    Parameters:\n","        path (str): Path to the directory containing the data files.\n","        num_rows (int, optional): Number of rows to read from the training data. Default is None,\n","            meaning the entire training dataset is read.\n","\n","    Returns:\n","        DataFrame: Concatenated DataFrame containing static data from both train and test datasets.\n","    \"\"\"\n","    # Read training data\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY + str('train/train_static_0_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n","    \n","    # If num_rows is specified, limit the number of rows in the training data\n","    if num_rows is not None:\n","        df1 = train.slice(0, num_rows)\n","        df2 = train.slice(num_rows, len(train))\n","        train = df1\n","        del df2\n","    \n","    # Read test data\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY + str('test/test_static_0_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    \n","    # Filter columns of the training data\n","    columns_to_keep = train.columns\n","\n","    # Find columns in 'test' that are not in 'train'\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","\n","    # Drop columns from 'test' that are not in 'train'\n","    test = test.drop(columns_to_remove)\n","    \n","    # Concatenate train and test data\n","    df = pl.concat([train, test])\n","    \n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:06.051709Z","iopub.status.busy":"2024-03-16T23:21:06.051361Z","iopub.status.idle":"2024-03-16T23:21:12.506783Z","shell.execute_reply":"2024-03-16T23:21:12.505787Z","shell.execute_reply.started":"2024-03-16T23:21:06.051680Z"},"trusted":true},"outputs":[],"source":["A=get_static(DATA_DIRECTORY, 100)\n","get_info(A)\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_static_cb()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:12.508115Z","iopub.status.busy":"2024-03-16T23:21:12.507846Z","iopub.status.idle":"2024-03-16T23:21:12.515684Z","shell.execute_reply":"2024-03-16T23:21:12.514639Z","shell.execute_reply.started":"2024-03-16T23:21:12.508093Z"},"trusted":true},"outputs":[],"source":["def get_static_cb(path, num_rows=None):\n","    \"\"\"\n","    Function to read static credit bureau data.\n","\n","    Parameters:\n","        path (str): Path to the directory containing the data files.\n","        num_rows (int, optional): Number of rows to read from the training data. Default is None,\n","            meaning the entire training dataset is read.\n","\n","    Returns:\n","        DataFrame: Concatenated DataFrame containing static credit bureau data from both train and test datasets.\n","    \"\"\"\n","    # If num_rows is not specified, read the entire training data\n","    if num_rows is None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes)\n","    # If num_rows is specified, read only the specified number of rows from the training data\n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_static_cb_0.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","    \n","    # Read the test data\n","    test = pl.read_parquet(os.path.join(path, 'test/test_static_cb_0.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    # Filter columns of the training data\n","    train = train.pipe(Pipeline.filter_cols)\n","    \n","    # Ensure consistency of columns between train and test data\n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    # Concatenate train and test data\n","    df = pl.concat([train, test])\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:12.517048Z","iopub.status.busy":"2024-03-16T23:21:12.516771Z","iopub.status.idle":"2024-03-16T23:21:13.492776Z","shell.execute_reply":"2024-03-16T23:21:13.491808Z","shell.execute_reply.started":"2024-03-16T23:21:12.517024Z"},"trusted":true},"outputs":[],"source":["A=get_static_cb(DATA_DIRECTORY, 100)\n","get_info(A)\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_applprev1(DATA_DIRECTORY, num_rows=num_rows)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:13.494234Z","iopub.status.busy":"2024-03-16T23:21:13.493953Z","iopub.status.idle":"2024-03-16T23:21:13.504051Z","shell.execute_reply":"2024-03-16T23:21:13.502961Z","shell.execute_reply.started":"2024-03-16T23:21:13.494210Z"},"trusted":true},"outputs":[],"source":["def get_applprev1(path, num_rows = None):\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY + str('train/train_applprev_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n","\n","    if num_rows is not None:\n","        df1 = train.slice(0, num_rows)\n","        df2 = train.slice(num_rows, len(train))\n","        train = df1\n","        del df2\n","\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY + str('test/test_applprev_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    df = pl.concat([train, test])\n","\n","    cols = ['annuity_853A', 'currdebt_94A', 'mainoccupationinc_437A']\n","\n","    # Perform aggregations\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","    expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","\n","    agg_max_df = df.group_by(\"case_id\").agg(expr_max)\n","    agg_min_df = df.group_by(\"case_id\").agg(expr_min)\n","    agg_mean_df = df.group_by(\"case_id\").agg(expr_mean)\n","\n","    # Merge all aggregated dataframes\n","    agg_df = agg_max_df.join(agg_min_df, on=\"case_id\", how=\"inner\")\n","    agg_df = agg_df.join(agg_mean_df, on=\"case_id\", how=\"inner\")\n","\n","    del df\n","\n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:13.505405Z","iopub.status.busy":"2024-03-16T23:21:13.505140Z","iopub.status.idle":"2024-03-16T23:21:27.085589Z","shell.execute_reply":"2024-03-16T23:21:27.084605Z","shell.execute_reply.started":"2024-03-16T23:21:13.505383Z"},"trusted":true},"outputs":[],"source":["A=get_applprev1(DATA_DIRECTORY, 100)\n","print(A)\n","get_info(A)\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_applprev2(DATA_DIRECTORY, num_rows=num_rows)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:27.087024Z","iopub.status.busy":"2024-03-16T23:21:27.086700Z","iopub.status.idle":"2024-03-16T23:21:27.095702Z","shell.execute_reply":"2024-03-16T23:21:27.094618Z","shell.execute_reply.started":"2024-03-16T23:21:27.086999Z"},"trusted":true},"outputs":[],"source":["def get_applprev2(path, num_rows = None):\n","    train={}\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","     \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_applprev_2.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","       \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_applprev_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    cols=['num_group1','num_group2']\n","\n","    agg_df = df.groupby(\"case_id\").agg(*[pl.sum(col).alias(f'count_{col}') for col in cols])\n","\n","\n","# Rename the columns to include \"count_\" prefix\n","    agg_df.columns = ['case_id'] + [f'count_applprev2_{col}' for col in cols]\n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:27.097145Z","iopub.status.busy":"2024-03-16T23:21:27.096883Z","iopub.status.idle":"2024-03-16T23:21:28.504187Z","shell.execute_reply":"2024-03-16T23:21:28.503192Z","shell.execute_reply.started":"2024-03-16T23:21:27.097123Z"},"trusted":true},"outputs":[],"source":["A=get_applprev2(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head(100))\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_person1"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:28.505490Z","iopub.status.busy":"2024-03-16T23:21:28.505235Z","iopub.status.idle":"2024-03-16T23:21:28.513934Z","shell.execute_reply":"2024-03-16T23:21:28.512994Z","shell.execute_reply.started":"2024-03-16T23:21:28.505468Z"},"trusted":true},"outputs":[],"source":["def get_person1(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","      \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_person_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    \n","    \n","    \n","    cols = ['birth_259D',\n","        'mainoccupationinc_384A']\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    agg_df = df.group_by(\"case_id\").agg(expr_max)\n","    \n","    \n","    \n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:28.515399Z","iopub.status.busy":"2024-03-16T23:21:28.515136Z","iopub.status.idle":"2024-03-16T23:21:30.182623Z","shell.execute_reply":"2024-03-16T23:21:30.181637Z","shell.execute_reply.started":"2024-03-16T23:21:28.515376Z"},"trusted":true},"outputs":[],"source":["A=get_person1(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_person2"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.184389Z","iopub.status.busy":"2024-03-16T23:21:30.184066Z","iopub.status.idle":"2024-03-16T23:21:30.193925Z","shell.execute_reply":"2024-03-16T23:21:30.192913Z","shell.execute_reply.started":"2024-03-16T23:21:30.184351Z"},"trusted":true},"outputs":[],"source":["def get_person2(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_person_2.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_person_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    \n","    cols=['num_group1','num_group2']\n","\n","    agg_df = df.groupby(\"case_id\").agg(*[pl.sum(col).alias(f'count_{col}') for col in cols])\n","\n","\n","# Rename the columns to include \"count_\" prefix\n","    agg_df.columns = ['case_id'] + [f'count_person2_{col}' for col in cols]\n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.195293Z","iopub.status.busy":"2024-03-16T23:21:30.195048Z","iopub.status.idle":"2024-03-16T23:21:30.516615Z","shell.execute_reply":"2024-03-16T23:21:30.515486Z","shell.execute_reply.started":"2024-03-16T23:21:30.195272Z"},"trusted":true},"outputs":[],"source":["A=get_person2(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### other"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.518155Z","iopub.status.busy":"2024-03-16T23:21:30.517857Z","iopub.status.idle":"2024-03-16T23:21:30.527453Z","shell.execute_reply":"2024-03-16T23:21:30.526328Z","shell.execute_reply.started":"2024-03-16T23:21:30.518129Z"},"trusted":true},"outputs":[],"source":["def get_other(path, num_rows = None):\n","     # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_other_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","         \n","    test = pl.read_parquet(os.path.join(path, 'test/test_other_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    cols = ['amtdebitincoming_4809443A',\n","        'amtdepositincoming_4809444A']\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    agg_df = df.group_by(\"case_id\").agg(expr_max)\n","    \n","    \n","    \n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.529086Z","iopub.status.busy":"2024-03-16T23:21:30.528728Z","iopub.status.idle":"2024-03-16T23:21:30.567420Z","shell.execute_reply":"2024-03-16T23:21:30.566340Z","shell.execute_reply.started":"2024-03-16T23:21:30.529054Z"},"trusted":true},"outputs":[],"source":["A=get_other(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["## get_debitcard"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.569363Z","iopub.status.busy":"2024-03-16T23:21:30.568971Z","iopub.status.idle":"2024-03-16T23:21:30.578927Z","shell.execute_reply":"2024-03-16T23:21:30.577906Z","shell.execute_reply.started":"2024-03-16T23:21:30.569329Z"},"trusted":true},"outputs":[],"source":["def get_debitcard(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","     \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_debitcard_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","      \n","        \n","    test = pl.read_parquet(os.path.join(path, 'test/test_debitcard_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    cols=['num_group1']\n","\n","    agg_df = df.groupby(\"case_id\").agg(*[pl.sum(col).alias(f'count_{col}') for col in cols])\n","\n","\n","# Rename the columns to include \"count_\" prefix\n","    agg_df.columns = ['case_id'] + [f'count_debitcard_{col}' for col in cols]\n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.580956Z","iopub.status.busy":"2024-03-16T23:21:30.580449Z","iopub.status.idle":"2024-03-16T23:21:30.631698Z","shell.execute_reply":"2024-03-16T23:21:30.630632Z","shell.execute_reply.started":"2024-03-16T23:21:30.580913Z"},"trusted":true},"outputs":[],"source":["A=get_debitcard(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_a"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.633459Z","iopub.status.busy":"2024-03-16T23:21:30.633091Z","iopub.status.idle":"2024-03-16T23:21:30.643195Z","shell.execute_reply":"2024-03-16T23:21:30.642222Z","shell.execute_reply.started":"2024-03-16T23:21:30.633423Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_a(path, num_rows = None):\n","    \n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_a_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","  \n","    \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_a_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    \n","    \n","    \n","    cols = ['amount_4527230A']\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    agg_df = df.group_by(\"case_id\").agg(expr_max)\n","    \n","    \n","    \n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:30.649442Z","iopub.status.busy":"2024-03-16T23:21:30.648707Z","iopub.status.idle":"2024-03-16T23:21:31.028887Z","shell.execute_reply":"2024-03-16T23:21:31.027805Z","shell.execute_reply.started":"2024-03-16T23:21:30.649398Z"},"trusted":true},"outputs":[],"source":["A=get_tax_registry_a(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_b"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.030393Z","iopub.status.busy":"2024-03-16T23:21:31.030082Z","iopub.status.idle":"2024-03-16T23:21:31.039910Z","shell.execute_reply":"2024-03-16T23:21:31.038837Z","shell.execute_reply.started":"2024-03-16T23:21:31.030366Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_b(path, num_rows = None):\n","    # Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_b_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    \n","    \n","    \n","    cols = ['amount_4917619A']\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    agg_df = df.group_by(\"case_id\").agg(expr_max)\n","    \n","    \n","    \n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.041643Z","iopub.status.busy":"2024-03-16T23:21:31.041256Z","iopub.status.idle":"2024-03-16T23:21:31.202114Z","shell.execute_reply":"2024-03-16T23:21:31.201100Z","shell.execute_reply.started":"2024-03-16T23:21:31.041608Z"},"trusted":true},"outputs":[],"source":["A=get_tax_registry_b(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_tax_registry_c"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.203677Z","iopub.status.busy":"2024-03-16T23:21:31.203359Z","iopub.status.idle":"2024-03-16T23:21:31.212528Z","shell.execute_reply":"2024-03-16T23:21:31.211426Z","shell.execute_reply.started":"2024-03-16T23:21:31.203650Z"},"trusted":true},"outputs":[],"source":["def get_tax_registry_c(path, num_rows = None):\n","     # Read the Parquet file using scan() method\n","# Read the Parquet file using scan() method\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_tax_registry_c_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","        \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_tax_registry_c_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n","    \n","    \n","    \n","    cols = ['pmtamount_36A']\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    agg_df = df.group_by(\"case_id\").agg(expr_max)\n","    \n","    \n","    \n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.214201Z","iopub.status.busy":"2024-03-16T23:21:31.213819Z","iopub.status.idle":"2024-03-16T23:21:31.640152Z","shell.execute_reply":"2024-03-16T23:21:31.639098Z","shell.execute_reply.started":"2024-03-16T23:21:31.214167Z"},"trusted":true},"outputs":[],"source":["A=get_tax_registry_c(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_a_1"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.641764Z","iopub.status.busy":"2024-03-16T23:21:31.641422Z","iopub.status.idle":"2024-03-16T23:21:31.652253Z","shell.execute_reply":"2024-03-16T23:21:31.651243Z","shell.execute_reply.started":"2024-03-16T23:21:31.641716Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_a_1(path, num_rows = None):\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_credit_bureau_a_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(train))\n","        \n","        train=df1\n","        del df2\n","    \n","    \n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_credit_bureau_a_1_*.parquet')):\n","        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n","        test = pl.concat(chunks, how=\"vertical_relaxed\")\n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    \n","   \n","    \n","    cols = ['overdueamountmaxdatemonth_365T']\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    agg_df = df.group_by(\"case_id\").agg(expr_max)\n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_b_1"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.653905Z","iopub.status.busy":"2024-03-16T23:21:31.653537Z","iopub.status.idle":"2024-03-16T23:21:31.668814Z","shell.execute_reply":"2024-03-16T23:21:31.667765Z","shell.execute_reply.started":"2024-03-16T23:21:31.653872Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_b_1(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","        \n","        \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_1.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","   \n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_1.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","\n","    df=pl.concat([train, test])\n","    \n"," \n","    \n","    cols = ['amount_1115A','credlmt_3940954A']\n","    expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","    agg_df = df.group_by(\"case_id\").agg(expr_max)\n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.670397Z","iopub.status.busy":"2024-03-16T23:21:31.670084Z","iopub.status.idle":"2024-03-16T23:21:31.752613Z","shell.execute_reply":"2024-03-16T23:21:31.751646Z","shell.execute_reply.started":"2024-03-16T23:21:31.670372Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'DATA_DIRECTORY' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m A\u001b[38;5;241m=\u001b[39mget_credit_bureau_b_1(\u001b[43mDATA_DIRECTORY\u001b[49m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_info(A)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(A\u001b[38;5;241m.\u001b[39mhead())\n","\u001b[1;31mNameError\u001b[0m: name 'DATA_DIRECTORY' is not defined"]}],"source":["A=get_credit_bureau_b_1(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_a_2"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.754396Z","iopub.status.busy":"2024-03-16T23:21:31.754088Z","iopub.status.idle":"2024-03-16T23:21:31.767640Z","shell.execute_reply":"2024-03-16T23:21:31.766604Z","shell.execute_reply.started":"2024-03-16T23:21:31.754372Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\n    \\n    cols=[\\'num_group1\\', \\'num_group2\\']\\n\\n    agg_df = df.groupby(\"case_id\").agg(*[pl.sum(col).alias(f\\'count_{col}\\') for col in cols])\\n\\n\\n# Rename the columns to include \"count_\" prefix\\n    agg_df.columns = [\\'case_id\\'] + [f\\'count_{col}\\' for col in cols]\\n    \\n    \\n    del df\\n    \\n    return agg_df\\n    \\n    '"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["def get_credit_bureau_a_2(path, num_rows = None):\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('train/train_credit_bureau_a_2_*.parquet')):\n","        chunks.append(reduce_mem_usage(pl.read_parquet(path))) #.pipe(Pipeline.set_table_dtypes))\n","        print(path)\n","    train = pl.concat(chunks, how=\"vertical_relaxed\").pipe(Pipeline.filter_cols)\n","    \n","    '''\n","    if num_rows!= None:\n","        df1 = train.slice(0,num_rows)\n","        df2 = train.slice(num_rows,len(df))\n","        \n","        train=df1\n","        del df2\n","    \n","    '''\n","    chunks = []\n","    for path in glob(DATA_DIRECTORY+str('test/test_credit_bureau_a_2_*.parquet')):\n","        chunks.append(reduce_mem_usage(pl.read_parquet(path))) #.pipe(Pipeline.set_table_dtypes))\n","        test = pl.concat(chunks, how=\"vertical_relaxed\")\n","        print(path)\n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    df=pl.concat([train, test])\n","    return df\n","                      \n","                      \n","    '''\n","    if num_rows!= None:\n","        df1 = test.slice(0,num_rows)\n","        df2 = test.slice(num_rows,len(df))\n","        \n","        test=df1\n","        del df2\n","    \n","    \n","    \n","'''\n","'''\n","    \n","    cols=['num_group1', 'num_group2']\n","\n","    agg_df = df.groupby(\"case_id\").agg(*[pl.sum(col).alias(f'count_{col}') for col in cols])\n","\n","\n","# Rename the columns to include \"count_\" prefix\n","    agg_df.columns = ['case_id'] + [f'count_{col}' for col in cols]\n","    \n","    \n","    del df\n","    \n","    return agg_df\n","    \n","    '''"]},{"cell_type":"markdown","metadata":{},"source":["### get_credit_bureau_b_2"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.769401Z","iopub.status.busy":"2024-03-16T23:21:31.769036Z","iopub.status.idle":"2024-03-16T23:21:31.782186Z","shell.execute_reply":"2024-03-16T23:21:31.781278Z","shell.execute_reply.started":"2024-03-16T23:21:31.769368Z"},"trusted":true},"outputs":[],"source":["def get_credit_bureau_b_2(path, num_rows = None):\n","    if num_rows == None:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","   \n","    else:\n","        train = pl.read_parquet(os.path.join(path, 'train/train_credit_bureau_b_2.parquet')).limit(num_rows).pipe(Pipeline.set_table_dtypes)\n","\n","    \n","    test = pl.read_parquet(os.path.join(path, 'test/test_credit_bureau_b_2.parquet')).pipe(Pipeline.set_table_dtypes)\n","    \n","    train = train.pipe(Pipeline.filter_cols)\n","   \n","    columns_to_keep = train.columns\n","    columns_to_remove = [column for column in test.columns if column not in columns_to_keep]\n","    test = test.drop(columns_to_remove)\n","    \n","    df=pl.concat([train, test])\n","    \n","    cols=['num_group1', 'num_group2']\n","\n","    agg_df = df.groupby(\"case_id\").agg(*[pl.sum(col).alias(f'count_{col}') for col in cols])\n","\n","\n","# Rename the columns to include \"count_\" prefix\n","    agg_df.columns = ['case_id'] + [f'count_cbb2_{col}' for col in cols]\n","    \n","    \n","    del df\n","    \n","    return agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.784138Z","iopub.status.busy":"2024-03-16T23:21:31.783451Z","iopub.status.idle":"2024-03-16T23:21:31.918362Z","shell.execute_reply":"2024-03-16T23:21:31.917412Z","shell.execute_reply.started":"2024-03-16T23:21:31.784104Z"},"trusted":true},"outputs":[],"source":["A=get_credit_bureau_b_2(DATA_DIRECTORY, 1000)\n","get_info(A)\n","print(A.head())\n","del A"]},{"cell_type":"markdown","metadata":{},"source":["### Utility functions"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.920678Z","iopub.status.busy":"2024-03-16T23:21:31.920288Z","iopub.status.idle":"2024-03-16T23:21:31.926451Z","shell.execute_reply":"2024-03-16T23:21:31.925254Z","shell.execute_reply.started":"2024-03-16T23:21:31.920642Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'contextmanager' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimer\u001b[39m(name):\n\u001b[0;32m      3\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'contextmanager' is not defined"]}],"source":["@contextmanager\n","def timer(name):\n","    t0 = time.time()\n","    yield\n","    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Execution"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T23:21:31.927947Z","iopub.status.busy":"2024-03-16T23:21:31.927657Z"},"trusted":true},"outputs":[{"ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'with_column'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline total time\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(debug)\u001b[0m\n\u001b[0;32m      2\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest base\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mget_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIRECTORY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest base dataframe shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest static\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","Cell \u001b[1;32mIn[16], line 38\u001b[0m, in \u001b[0;36mget_base\u001b[1;34m(path, num_rows)\u001b[0m\n\u001b[0;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat([train, test])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Convert 'date_decision' column to Date type\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_column\u001b[49m(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_decision\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mDate))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n","\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'with_column'"]}],"source":["if __name__ == \"__main__\":\n","    pd.set_option('display.max_rows', 60)\n","    pd.set_option('display.max_columns', 100)\n","    with timer(\"Pipeline total time\"):\n","        main(debug= True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# write a simple function to test timer \n","def test_timer():\n","    with timer(\"test\"):\n","        time.sleep(1)\n","    assert True\n","test_timer()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
